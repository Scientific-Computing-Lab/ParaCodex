**Summary**
- Rewrote `kernel_gpu` so the outer loop uses `#pragma omp target teams distribute parallel for` and the inner level/leaf scans run with `#pragma omp simd`, caching the current node and updating `currKnode`/`offset` in one pass to avoid redundant loads (`src/kernel/kernel_cpu.c:75-117`).
- Refactored `kernel_cpu_2` to carry both start/end traversals with local node pointers, simd-accelerated scans, and in-loop updates of both `currKnode`/`lastKnode` plus their offsets; leaf record lookups now also use `#pragma omp simd` to keep the entire range-search kernel on fast vector lanes (`src/kernel/kernel_cpu_2.c:120-159`).
- Captured the profiling-driven plan, bottlenecks, and final outcomes in `optimization_plan.md`, including the new final performance summary and applied optimizations for reference (`optimization_plan.md:1-77`).
- The default `make clean` target is absent, so I ran `make -f Makefile.nvc clean` before rebuilding to honor the “always clean before build” rule.

**Testing**
- `make -f Makefile.nvc clean`
- `timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run > current_output.txt 2>&1`

**Next Steps**
1. Run the CLASS B Nsight Compute profile (using the provided command) to verify the new simd/teams structure reduces GPU kernel time and to re-measure host-to-device transfer dominance.
2. Consider exposing the `target data` region/span to cover multiple commands so the persistent tree representation stays on-device longer, then re-evaluate whether additional memory-management tuning is warranted.