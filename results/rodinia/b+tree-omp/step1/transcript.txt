- Offloaded `kernel_gpu` so all query loops live inside a single `#pragma omp target data` region that maps the dataset once and launches the outer `bid` loop with `#pragma omp target teams loop thread_limit(threadsPerBlock)`; the inner-level scans are now annotated with `#pragma omp loop` so the GPU-level `threadsPerBlock` threads iterate in parallel (`data/src/b+tree-omp/src/kernel/kernel_cpu.c:35-118`).
- Applied the same Strategy A pattern to `kernel_cpu_2`, mapping `knodes`, range endpoints, and all scratch/output buffers once per timed run before driving the traversal + leaf scans with `target teams loop` and nested `loop` pragmas (`data/src/b+tree-omp/src/kernel/kernel_cpu_2.c:92-155`).
- Captured the array inventory, device mapping strategy, and transfer budget in `data_plan.md` to guide implementation decisions, and added `<stdio.h>` to `kernel_cpu_2.c` so the existing `printf` calls are well declared (`data_plan.md:1-64`, `data/src/b+tree-omp/src/kernel/kernel_cpu_2.c:13-15`).
- `diff baseline_output.txt gpu_output.txt` highlights the new GPU timing behavior: `Tree transformation` shifted from 0.058429→0.058693 s, the `MCPU` stage now takes ~1 μs and the kernel stage ~0.14597 s, and the final runtimes moved from 0.004554/0.003738 (CPU/GPU) to 0.005307/0.002024 s (`baseline_output.txt:299-314`, `gpu_output.txt:10-25`).

- Tests: `OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc CC=nvc++ CLASS=C` (passes; `nvc++` still reports the pre-existing `main.c` unused-variable warnings plus `max_nthreads` being unused in both kernel sources, and the new `<stdio.h>` include clears the previous implicit `printf` warning).
- Tests: `OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run > gpu_output.txt 2>&1` (runs to completion with the new GPU output log above).
- Tests: `diff baseline_output.txt gpu_output.txt` (exit 1 because the GPU-driven kernel now reports longer kernel time but shorter GPU runtime; differences captured in the summary above).

- Next steps: 1) Run the prescribed Class B `nsys` profile once the manifest is ready to confirm that the teams/loop configuration saturates the RTX 4060 Ti, and 2) experiment with tuning `threadsPerBlock` vs. `thread_limit`/team counts if profiling shows low occupancy.

OpenAI Codex v0.72.0 (research preview)
--------
workdir: /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir
model: gpt-5.1-codex-mini
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: medium
reasoning summaries: auto
session id: 019b2172-9639-7503-a353-df44fda7f92c
--------
user
Directory: /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp

# GPU Offload with OpenMP

**Directory:** `/home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp/`  
**Files:** - kernel/kernel_cpu.c
- kernel/kernel_cpu_2.c  
**Reference:** `/home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp/analysis.md`

**Required:** 
- Use `OMP_TARGET_OFFLOAD=MANDATORY` for all runs
- DO NOT use `distribute parallel for`

## Workflow

### 0. Backup
Save backup of - kernel/kernel_cpu.c
- kernel/kernel_cpu_2.c.

### 1. Get Baseline (CLASS A/S)
```bash
cd /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp
make clean
timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run > baseline_output.txt 2>&1

DO NOT SKIP THIS STEP.
```

### 2. Choose Data Strategy
Walk through IN ORDER, stop at first match:

```
RULE 1: Type B (Sparse/CSR)?              → STRATEGY C
RULE 2: Type C1 (FFT/Butterfly)?          → STRATEGY C
RULE 3: Type C2 (Multigrid)?              → STRATEGY A
RULE 4: Outer A + inner E (per-thread RNG)?→ STRATEGY A
RULE 5: Multiple independent kernels?     → STRATEGY B
RULE 6: Otherwise                         → STRATEGY A
```

### 2.5. Create Data Management Plan
MANDATORY: Create data_plan.md before implementation
Analyze ALL arrays and functions in timed region:

```markdown

# Data Management Plan

## Arrays Inventory
List ALL arrays used in timed region:

| Array Name | Size | Type | Init | Access |
|------------|------|------|------|--------|
| [name] | [bytes] | working/scratch/const/index | host/device | R/W/RO |

**Types:** working (main data), scratch (temp), const (read-only), index (maps)

## Functions in Timed Region
| Function | Arrays Accessed | Frequency | Must Run On |
|----------|----------------|-----------|-------------|
| [name] | [list] | per-iteration/once | device/host |

## Data Movement Strategy

**Chosen Strategy:** [A/B/C]

**Device Allocations (once):**
```
Strategy C: d_[array]: [size] via omp_target_alloc
Strategy A: [arrays] in target data region
```

**Host→Device Transfers:**
- When: [before iterations/once at start]
- Arrays: [array1]→d_[array1] ([size] MB)
- Total H→D: ~[X] MB

**Device→Host Transfers:**
- When: [after iterations/once at end]
- Arrays: d_[array1]→[array1] ([size] MB)
- Total D→H: ~[Y] MB

**Transfers During Iterations:** [YES/NO]
- If YES: [which arrays and why]
- If NO: All data stays on device

## Critical Checks (for chosen strategy)

**Strategy A:**
- [ ] Functions inside target data use `present,alloc` wrapper?
- [ ] Scratch arrays use enter/exit data OR omp_target_alloc?

**Strategy C:**
- [ ] ALL functions in iteration loop use is_device_ptr?
- [ ] Scratch arrays allocated on device (not host)?
- [ ] No map() clauses (only is_device_ptr)?

**Common Mistakes:**
-  Some functions on device, others on host (causes copying)
-  Scratch as host arrays in Strategy C
-  Forgetting to offload ALL functions in loop

## Expected Transfer Volume
- Total: ~[X+Y] MB for entire execution
- **Red flag:** If actual >2x expected → data management wrong

**Summary:** [num] arrays ([num] scratch, [num] working), [num] functions, Strategy [A/B/C]. Expected: ~[X] MB H→D, ~[Y] MB D→H.
```

### 2.6. Implement Data Plan

**Use data_plan.md as implementation guide**

### Step 1: Setup Data Structures
From "Arrays Inventory" and "Data Movement Strategy":
- Declare device arrays/pointers as needed for chosen strategy
- Create allocation/initialization functions based on strategy:
  - **Strategy A:** Setup target data regions with map clauses from plan
  - **Strategy B:** Prepare depend clauses for async operations
  - **Strategy C:** Create omp_target_alloc calls using sizes from plan

### Step 2: Implement Transfers
From "H→D Transfers" and "D→H Transfers" sections:
- Implement each transfer listed with timing specified in plan
- Use method appropriate for strategy (map clauses, omp_target_memcpy, update, etc.)

### Step 3: Offload Functions
Use "Functions in Timed Region" table:
- For each function where "Must Run On" = device:
  - Add appropriate pragma for strategy
  - Include arrays from "Arrays Accessed" column
  - Follow strategy-specific patterns from Step 2

### Step 4: Main Program Flow
Follow "Data Movement Strategy" timing:
```
[setup from plan]
[H→D transfers at specified time]
[timed computation - call functions]
[D→H transfers at specified time]
[cleanup]
```

### Step 5: Verify Implementation
Check ALL items in "Critical Checks" section for YOUR strategy:
- [ ] Verify each checkpoint matches implementation
- [ ] Cross-reference "Functions in Timed Region" table
- [ ] Confirm transfer timing matches plan

**Common errors:** Mismatched array names, missing functions from table, wrong transfer timing

**Ready when:** All strategy-specific checks ✓ and compiles
---

## Strategy Details

### STRATEGY A: target data Region

**Map Clause Selection:**
| Scenario | Map Clause | Why |
|----------|------------|-----|
| Device-init arrays (zero(), fill()) | `alloc` | Avoid copying garbage |
| Host RNG init then sync | `alloc` + `update to` | Explicit sync after host init |
| Read + modify + write | `tofrom` | Bidirectional |
| Read-only | `to` | One-way |

**Functions Called Inside target data:**
Wrap with `present,alloc`/'to,tofrom', then use bare `target teams loop`:
```c
void compute(double *u, double *v, int n) {
  #pragma omp target data map(present,alloc:u[0:n],v[0:n])
  {
    #pragma omp target teams loop
    for (int i = 0; i < n; i++) { ... }
  }
}
```

**alloc + device init**
```c
#pragma omp target data map(alloc:arr1[0:N],arr2[0:N],arr3[0:N])
{
  device_zero(arr1, n);       // Initializes arr1 ON DEVICE
  device_fill(arr2, n, val);  // Initializes arr2 ON DEVICE
  
  // Host-side RNG init? Use target update to sync AFTER host fills array
  host_rng_init(arr3, n);
  #pragma omp target update to(arr3[0:N])
  
  for (iter = 0; iter < NITER; iter++) {
    #pragma omp target teams loop
    for (j = 0; j < N; j++) { ... }
  }
}
```

**Scratch Arrays (two options):**

- **Option 1: enter/exit data**
```c
double scratch[N];
#pragma omp target enter data map(alloc:scratch[0:n])
#pragma omp target data map(present,alloc:in[0:n])
{
  #pragma omp target teams loop
  for (...) { /* use scratch */ }
}
#pragma omp target exit data map(delete:scratch[0:n])
```

- **Option 2: omp_target_alloc**
```c
double *scratch = (double*)omp_target_alloc(n*sizeof(double), 0);
#pragma omp target data map(present,alloc:in[0:n])
{
  #pragma omp target teams loop is_device_ptr(scratch)
  for (...) { ... }
}
omp_target_free(scratch, 0);
```

**Mid-computation sync:**
```c
#pragma omp target update from(result)
host_compute(result);
#pragma omp target update to(indices)
```

### STRATEGY B: Asynchronous Offload
Use when: Overlapping compute/transfer possible
```c
#pragma omp target teams loop nowait depend(out:x[0])
for (i = 0; i < N; i++) { x[i] = init(i); }

#pragma omp target teams loop nowait depend(in:x[0]) depend(out:y[0])
for (i = 0; i < N; i++) { y[i] = compute(x[i]); }

#pragma omp taskwait
```

STRATEGY C: Global Device State (Iterative Solvers)
Only use omp_target_alloc + is_device_ptr
CRITICAL: This strategy is MANDATORY for iterative solvers
Pattern:

Shadow globals: static double *d_arr
Lifecycle: allocate_device(), copy_to_device(), free_device() in main
Reduction helpers ONLY: Small functions returning scalars (dot products) - e.g., device_dot()
INLINE ALL OTHER KERNELS: SpMV, vector updates, scaling operations MUST be inlined in the iteration loop

INLINING IS CRITICAL.

CORRECT Pattern for Iterative Solver:
cint tmp1, tmp2, tmp3;  // Function scope for register caching

// Setup once
allocate_device_arrays();
copy_to_device();

for (iter = 1; iter <= maxiter; iter++) {
  // INLINE SpMV - do NOT call device_spmv() function
  #pragma omp target teams loop is_device_ptr(d_rowptr, d_col, d_val, d_p, d_q)
  for (int i = 0; i < nrows; i++) {
    tmp1 = d_rowptr[i];
    tmp2 = d_rowptr[i+1];
    double sum = 0.0;
    #pragma omp loop reduction(+:sum)
    for (int k = tmp1; k < tmp2; k++) {
      tmp3 = d_col[k];
      sum += d_val[k] * d_p[tmp3];
    }
    d_q[i] = sum;
  }
  
  // Reduction helper OK - returns scalar
  double pq = device_dot(d_p, d_q, n);
  alpha = rho / pq;
  
  // INLINE vector updates - do NOT call device_update() function
  #pragma omp target teams loop is_device_ptr(d_x, d_r, d_p, d_q)
  for (int i = 0; i < n; i++) {
    d_x[i] += alpha * d_p[i];
    d_r[i] -= alpha * d_q[i];
  }
  
  rho_new = device_dot(d_r, d_r, n);
  beta = rho_new / rho;
  
  // INLINE update - do NOT call device_update_p() function
  #pragma omp target teams loop is_device_ptr(d_p, d_r)
  for (int i = 0; i < n; i++) {
    d_p[i] = d_r[i] + beta * d_p[i];
  }
  
  rho = rho_new;
}

WRONG Pattern (causes 10-100x slowdown):
for (iter = 1; iter <= maxiter; iter++) {
  device_spmv(d_q, d_p, n);     // Launch 1
  pq = device_dot(d_p, d_q, n); // Launch 2 (OK)
  device_axpy(d_x, ...);        // Launch 3
  device_axpy(d_r, ...);        // Launch 4
}

Operation | Helper? | Why |
|-----------|---------|--------|
| Dot/norm | YES | Returns scalar |
| SpMV in loop | NO | Every iteration |
| Vector update | NO | Every iteration |
| Init (once) | YES | Once before loop |
| Verification (once) | YES | Once after loop |
---

### 3. Map Globals & Functions
```c
#pragma omp declare target
double helper_func() { ... };
#pragma omp end declare target

#pragma omp declare target(global_var)
```
---

## 4. Parallelize loops

**Parallelization patterns:**

**Type A (Dense):**
```c
#pragma omp target teams loop collapse(2)
for (i = 0; i < N; i++)
  for (j = 0; j < M; j++) ...
```

**Type B (Sparse/CSR) - Nested Parallelism:**
```c
int tmp1, tmp2, tmp3;  // Function scope
#pragma omp target teams loop is_device_ptr(...)
for (int row = 0; row < nrows; row++) {
  tmp1 = rowptr[row];
  tmp2 = rowptr[row+1];
  double sum = 0.0;
  #pragma omp loop reduction(+:sum)  // Parallelize inner
  for (int k = tmp1; k < tmp2; k++) {
    tmp3 = colidx[k];
    sum += A[k] * x[tmp3];
  }
  y[row] = sum;
}
```

**Type C1 (FFT) - Serial Inner:**
```c
#pragma omp target teams is_device_ptr(...)
{
#pragma omp loop collapse(2)
  for (k = 0; k < K; k++) {
    for (j = 0; j < J; j++) {
      for (stage = 0; stage < S; stage++) { ... }  // No pragma - keep inner serial!
    }
  }
}
```

**Type C2 (Multigrid):** Wrap with `present,alloc`; each stencil call gets `target teams loop`.

**Type D (Histogram):** Add `#pragma omp atomic` on indirect writes.

**Type F (Reduction):** `reduction(+:sum)`

**Type G (Stencil):** `collapse(2)` on spatial dimensions.

**Type A+E (Outer parallel, inner RNG):** Add `declare target` on RNG function.

## 5. Compile and Test (CLASS A/S)
```bash
make clean
make -f Makefile.nvc CC=nvc++ CLASS=C
timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run > gpu_output.txt 2>&1
```

If timeout/segfault: Remove `#pragma omp loop` from Type C inner loops.

## 6. Verify Correctness
```bash
diff baseline_output.txt gpu_output.txt
```

## 8. Profile (CLASS B)
** NEVER RUN WITH CLASS C , ONLY RUN WITH CLASS B.**
```bash
make clean
<function _nsys_profile_cmd_str at 0x796a46292e80> > /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp/profile.log 2>&1
```

#**RULES** BRAKING A RULE = FAILURE.
- NO GIT COMMANDS.
- DO NOT READ/WRITE OUTSIDE THE WORKING DIRECTORY.
- DO NOT EDIT MAKEFILES.
- ALWAYS CLEAN BEFORE BUILD.


you might want to leave comments in the code to explain your changes.

the code might contain macros like GATE_CHECKSUM_* or GATE_STATS_*, you should not change them.

you need to check what hardware you are running on in `system_info.txt` and use the information to optimize your code.


mcp startup: no servers

thinking
**Reading system information**
exec
/bin/bash -lc pwd in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 6ms:
/home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir

thinking
**Reading system_info.txt**
exec
/bin/bash -lc 'cat system_info.txt' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 10ms:
=== SYSTEM BASELINE ===
Linux cmhq-ext-shared-codex 6.14.0-35-generic #35~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Oct 14 13:55:17 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 24.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.2 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
ldd (Ubuntu GLIBC 2.39-0ubuntu8.6) 2.39

=== FIRMWARE & MICROCODE ===
# dmidecode 3.5
Getting SMBIOS data from sysfs.

thinking
**Preparing kernel file backups**
exec
/bin/bash -lc ls in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 10ms:
analysis.md
backups
b+tree.c
common.h
input
kernel
log
main
main.c
main.h
main.o
Makefile
Makefile.nvc
penmp
run
src
util

thinking
**Preparing backup copies**
exec
/bin/bash -lc 'cp kernel/kernel_cpu.c backups/kernel_cpu.c.bak' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp exited 1 in 10ms:
cp: cannot stat 'kernel/kernel_cpu.c': No such file or directory

thinking
**Listing kernel directory**
exec
/bin/bash -lc 'ls kernel' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 10ms:
kernel_cpu_2.c.bak
kernel_cpu_2.h
kernel_cpu_2.o
kernel_cpu.c.bak
kernel_cpu.c.supervisor.bak
kernel_cpu.h
kernel_cpu.o

thinking
**Searching for kernel_cpu.c source**
exec
/bin/bash -lc "rg --files -g'*kernel_cpu*.c'" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 16ms:
src/kernel/kernel_cpu_2.c
src/kernel/kernel_cpu.c

thinking
**Checking kernel file locations**
exec
/bin/bash -lc 'ls src/kernel' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 10ms:
kernel_cpu_2.c
kernel_cpu_2.c.backup
kernel_cpu_2.c.backup_codex
kernel_cpu_2.c.bak
kernel_cpu_2.c.bak2
kernel_cpu_2.c.codex_backup
kernel_cpu_2.c.codex_backup2
kernel_cpu_2.c.codexbak
kernel_cpu_2.c.codex_preopt
kernel_cpu_2.c.mod
kernel_cpu_2.c.preopt
kernel_cpu_2.c.preopt_backup
kernel_cpu_2.c.userbak
kernel_cpu_2.h
kernel_cpu_2.o
kernel_cpu.c
kernel_cpu.c.backup
kernel_cpu.c.backup_codex
kernel_cpu.c.bak
kernel_cpu.c.bak2

thinking
**Backing up kernel source files**
exec
/bin/bash -lc 'cp src/kernel/kernel_cpu.c backups/src_kernel_cpu.c.bak' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 11ms:


thinking
**Backing up kernel_cpu_2.c**
exec
/bin/bash -lc 'cp src/kernel/kernel_cpu_2.c backups/src_kernel_cpu_2.c.bak' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 7ms:


thinking
**Preparing to review analysis instructions**
exec
/bin/bash -lc 'cat analysis.md' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 11ms:
# Loop Classification for B+Tree OMP Kernels

## Loop Nesting Structure
- kernel_gpu/kernel_cpu query loop (`data/src/b+tree-omp/src/kernel/kernel_cpu.c:82` and `:172`) — Type A
  ├── level traversal loop (`data/src/b+tree-omp/src/kernel/kernel_cpu.c:85` and `:175`) — Type E (stage dependency)
  │   └── per-node key scan (`data/src/b+tree-omp/src/kernel/kernel_cpu.c:88` and `:178`) — Type A
  └── leaf key check loop (`data/src/b+tree-omp/src/kernel/kernel_cpu.c:110` and `:200`) — Type A
- kernel_cpu_2 range query loop (`data/src/b+tree-omp/src/kernel/kernel_cpu_2.c:93`) — Type A
  ├── level traversal loop (`data/src/b+tree-omp/src/kernel/kernel_cpu_2.c:95`) — Type E (stage dependency on `currKnode/lastKnode`)
  │   └── dual node scan (`data/src/b+tree-omp/src/kernel/kernel_cpu_2.c:99`) — Type A
  ├── start-record scan (`data/src/b+tree-omp/src/kernel/kernel_cpu_2.c:127`) — Type A
  └── end-record scan (`data/src/b+tree-omp/src/kernel/kernel_cpu_2.c:137`) — Type A

## Loop Details

### Loop: kernel_gpu/kernel_cpu outer query driver (`data/src/b+tree-omp/src/kernel/kernel_cpu.c:82`, `:172`)
- **Iterations:** `count` (user-supplied bundle size), runs once per timed invocation in `main.c:2241-2275` (CPU) and `main.c:2261-2275` (GPU).
- **Type:** A – dense driver that launches the work for every query; loop-carried state lives per `bid`.
- **Parent loop:** none.
- **Contains:** level traversal loop, leaf key check loop.

thinking
**Preparing baseline environment and commands**
exec
/bin/bash -lc 'OMP_TARGET_OFFLOAD=MANDATORY make clean' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp exited 2 in 11ms:
make: *** No rule to make target 'clean'.  Stop.

thinking
**Investigating make clean failure**
exec
/bin/bash -lc 'OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc clean' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 11ms:
rm -rf main src/main.o src/util/num/num.o src/util/timer/timer.o src/kernel/kernel_cpu.o src/kernel/kernel_cpu_2.o

thinking
**Preparing baseline run command**
exec
/bin/bash -lc 'OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run > baseline_output.txt 2>&1' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 1.59s:


thinking
**Selecting data strategy and planning**
exec
/bin/bash -lc "sed -n '1,220p' src/kernel/kernel_cpu.c" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 11ms:
// #ifdef __cplusplus
// extern "C" {
// #endif

//========================================================================================================================================================================================================200
//	DEFINE/INCLUDE
//========================================================================================================================================================================================================200

//======================================================================================================================================================150
//	LIBRARIES
//======================================================================================================================================================150

#include <stdlib.h>									// (in directory known to compiler)			needed by malloc
#include <stdio.h>									// (in directory known to compiler)			needed by printf, stderr

//======================================================================================================================================================150
//	COMMON
//======================================================================================================================================================150

#include "../common.h"								// (in directory provided here)

thinking
**Identifying missing kernel CPU**
exec
/bin/bash -lc "sed -n '1,260p' src/kernel/kernel_cpu_2.c" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 10ms:
// #ifdef __cplusplus
// extern "C" {
// #endif

//========================================================================================================================================================================================================200
//	DEFINE/INCLUDE
//========================================================================================================================================================================================================200

//======================================================================================================================================================150
//	LIBRARIES
//======================================================================================================================================================150

#include <stdlib.h>									// (in directory known to compiler)

//======================================================================================================================================================150
//	COMMON
//======================================================================================================================================================150

#include "../common.h"								// (in directory provided here)


thinking
**Preparing to analyze GPU offloading**
exec
/bin/bash -lc "sed -n '2140,2500p' src/main.c" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 10ms:
			// ----------------------------------------40

			case 'v':
			{
				while (getchar() != (int)'\n');
				verbose_output = !verbose_output;
				break;
			}

			// ----------------------------------------40
			// quit
			// ----------------------------------------40

			case 'q':
			{
				while (getchar() != (int)'\n');
				return EXIT_SUCCESS;
			}

			// ----------------------------------------40


thinking
**Creating detailed GPU offload data plan**

I’m organizing a comprehensive data management plan for offloading the timed code regions to GPU with OpenMP target directives. I’m analyzing array usages, sizes, and mapping types across kernel_cpu, kernel_gpu, and kernel_cpu_2 functions to define const vs working arrays and the proper map clauses for Strategy A target data regions. I’ll structure the plan in markdown, specifying approximate sizes with symbolic counts and clarifying how arrays move between host and device during execution. I’m also considering loop restructuring with #pragma omp target teams loop and restrictions like avoiding distribute parallel for, plus handling peculiarities like unused malloc calls inside kernels by guarding or removing them. The plan will guide the next implementation step to offload kernel_gpu and kernel_cpu_2 loops efficiently.
thinking
**Finalizing GPU kernel restructuring and data mapping plan**

I’m refining the plan to move unused malloc calls out of the GPU target region to run safely on host and avoid device malloc issues, adding corresponding free calls to prevent memory leaks. I’m also defining the proper OpenMP pragmas for kernel_gpu and kernel_cpu_2 with `target teams loop` and nested `loop` directives, respecting thread limits and avoiding banned constructs like `distribute parallel for`. I’ll ensure all arrays are properly mapped with `map(to:)` or `map(tofrom:)`, and structure a detailed markdown data plan including array sizes, types, init, and access patterns, plus data movement strategy with approximate transfer volumes expressed symbolically. This will give a clear blueprint before starting the code translation to GPU offload.
exec
/bin/bash -lc 'rg -n "typedef struct" common.h' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 11ms:
52:typedef struct list_item list_item_t;
54:typedef struct list_t {
74:typedef struct record {
106:typedef struct node {
116:typedef struct knode {
exec
/bin/bash -lc "sed -n '70,140p' common.h" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 11ms:
* Users can rewrite this part of the code
* to change the type and content
* of the value field.
*/
typedef struct record {
	int value;
} record;

/* Type representing a node in the B+ tree.
* This type is general enough to serve for both
* the leaf and the internal node.
* The heart of the node is the array
* of keys and the array of corresponding
* pointers.  The relation between keys
* and pointers differs between leaves and
* internal nodes.  In a leaf, the index
* of each key equals the index of its corresponding
* pointer, with a maximum of order - 1 key-pointer
* pairs.  The last pointer points to the
* leaf to the right (or NULL in the case

thinking
**Investigating DEFAULT_ORDER definition**
exec
/bin/bash -lc 'rg -n "DEFAULT_ORDER" -n' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 15ms:
main.c:133:int order = DEFAULT_ORDER;
main.c:1993:	order = DEFAULT_ORDER;
common.h:34:#define DEFAULT_ORDER 508
common.h:118:	int indices [DEFAULT_ORDER + 1];
common.h:119:	int  keys [DEFAULT_ORDER + 1];
analysis.md:136:  - `knodes` (`knode` struct, see `common.h`): a contiguous dynamic array of nodes where each node contains fixed-size `keys`/`indices` arrays (`DEFAULT_ORDER + 1`). Allocated from the shared `mem` region before calling the kernels; uses static struct layout with inline arrays.
src/common.h:34:#define DEFAULT_ORDER 508
src/common.h:118:	int indices [DEFAULT_ORDER + 1];
src/common.h:119:	int  keys [DEFAULT_ORDER + 1];
src/main.c:133:int order = DEFAULT_ORDER;
src/main.c:1993:	order = DEFAULT_ORDER;

thinking
**Preparing detailed data management plan**

I’m working out approximate sizes and transfer volumes for arrays like `records`, `knodes`, and various scratch arrays to include in a thorough `data_plan.md` file. This plan will document arrays, functions, transfer strategies (notably no transfers during iterations), device allocations using OpenMP `target data` maps, critical checks, and an overall summary of expected data movement and memory usage. I’ll draft the content following the provided template and then apply the patch to add this file.
file update
A /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data_plan.md
# Data Management Plan

## Arrays Inventory
| Array Name | Size | Type | Init | Access |
|------------|------|------|------|--------|
| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |

## Functions in Timed Region
| Function | Arrays Accessed | Frequency | Must Run On |
|----------|------------------|-----------|-------------|
| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |

## Data Movement Strategy

**Chosen Strategy:** A (target data region + map clauses)

### Device Allocations (once per kernel invocation)
- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.

### Host→Device Transfers
- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).

### Device→Host Transfers
- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).

### Transfers During Iterations
- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.

### Critical Checks (for Strategy A)
- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.

### Common Mistakes
- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.

## Expected Transfer Volume
- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.

## Summary
- Arrays: 12 total (6 working/scratch, 6 const).
- Functions: 3 (two device, one host comparison).
- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).
apply_patch(auto_approved=true) exited 0 in 3ms:
Success. Updated the following files:
A data_plan.md
file update:
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).


thinking
**Preparing OpenMP target loop refactor plan**
file update:
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).


thinking
**Correcting patch path**
file update:
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).


thinking
**Preparing careful patch application**
file update:
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "sed -n '1,210p' data/src/b+tree-omp/src/kernel/kernel_cpu.c" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 9ms:
// #ifdef __cplusplus
// extern "C" {
// #endif

//========================================================================================================================================================================================================200
//	DEFINE/INCLUDE
//========================================================================================================================================================================================================200

//======================================================================================================================================================150
//	LIBRARIES
//======================================================================================================================================================150

#include <stdlib.h>									// (in directory known to compiler)			needed by malloc
#include <stdio.h>									// (in directory known to compiler)			needed by printf, stderr

//======================================================================================================================================================150
//	COMMON
//======================================================================================================================================================150

#include "../common.h"								// (in directory provided here)

thinking
**Preparing precise function patch**
file update:
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).


thinking
**Crafting granular patch for code replacement**
file update:
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

file update
M /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -79,4 +79,6 @@
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
@@ -87,2 +89,3 @@
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
@@ -109,2 +112,3 @@
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
@@ -209,3 +213,2 @@
 }
-
 
apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
M data/src/b+tree-omp/src/kernel/kernel_cpu.c
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..eb32c1d2de95a8f90a8f53b26a184794f72b1cb8
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -77,14 +77,17 @@
 	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +110,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -207,5 +211,4 @@
 
 	}
 }
-
 
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).


thinking
**Enhancing kernel_cpu_2 with OpenMP pragmas and target data**

I'm preparing to wrap the main loops in `kernel_cpu_2` with a `#pragma omp target data` region, placing it just after the timing call `time1 = get_time()`. The patch will replace the existing `for(bid=0; bid<count; bid++)` block to include `target teams loop` and `#pragma omp loop` directives for inner loops, carefully mapping arrays like `knodes`, `start`, and `end` with correct data clauses. This should optimize parallel execution while preserving existing logic and array usage.
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..eb32c1d2de95a8f90a8f53b26a184794f72b1cb8
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -77,14 +77,17 @@
 	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +110,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -207,5 +211,4 @@
 
 	}
 }
-
 
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

file update
M /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -92,51 +92,60 @@
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
@@ -144,3 +153,2 @@
 		}
-
 	}
apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..eb32c1d2de95a8f90a8f53b26a184794f72b1cb8
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -77,14 +77,17 @@
 	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +110,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -207,5 +211,4 @@
 
 	}
 }
-
 
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).


thinking
**Reviewing OpenMP pragma usage and variable cleanup**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..eb32c1d2de95a8f90a8f53b26a184794f72b1cb8
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -77,14 +77,17 @@
 	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +110,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -207,5 +211,4 @@
 
 	}
 }
-
 
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "sed -n '40,160p' data/src/b+tree-omp/src/kernel/kernel_cpu.c" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 8ms:
			long knodes_elem,
			long records_elem,

			int order,
			long maxheight,
			int count,

			long *currKnode,
			long *offset,
			int *keys,
			record *ans)
{


	//======================================================================================================================================================150
	//	MCPU SETUP
	//======================================================================================================================================================150

	int max_nthreads;
	// printf("max # of threads = %d\n", max_nthreads);

thinking
**Removing unused malloc and variables**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..eb32c1d2de95a8f90a8f53b26a184794f72b1cb8
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -77,14 +77,17 @@
 	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +110,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -207,5 +211,4 @@
 
 	}
 }
-
 
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

file update
M /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -74,6 +74,2 @@
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
@@ -213,2 +209 @@
 }
-
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M data/src/b+tree-omp/src/kernel/kernel_cpu.c
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..63f5e7296eeb725788e51167c8ed531956a25c62
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -207,5 +207,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).


thinking
**Preparing to remove duplicate lines in kernel_cpu**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..63f5e7296eeb725788e51167c8ed531956a25c62
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -207,5 +207,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

file update
M /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -165,6 +165,2 @@
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M data/src/b+tree-omp/src/kernel/kernel_cpu.c
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).


thinking
**Checking for leftover unused variables**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "sed -n '30,110p' data/src/b+tree-omp/src/kernel/kernel_cpu.c" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 7ms:

//========================================================================================================================================================================================================200
//	KERNEL_CPU FUNCTION
//========================================================================================================================================================================================================200

void 
kernel_gpu(	int cores_arg,

			record *records,
			knode *knodes,
			long knodes_elem,
			long records_elem,

			int order,
			long maxheight,
			int count,

			long *currKnode,
			long *offset,
			int *keys,

thinking
**Verifying code formatting and function integrity**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "sed -n '150,280p' data/src/b+tree-omp/src/kernel/kernel_cpu.c" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 6ms:
	// printf("set # of threads = %d\n", cores_arg);

	int threadsPerBlock;
	threadsPerBlock = order < 1024 ? order : 1024;


	//======================================================================================================================================================150
	//	PROCESS INTERACTIONS
	//======================================================================================================================================================150

	// private thread IDs
	int thid;
	int bid;
	int i;


	// process number of querries

	for(bid = 0; bid < count; bid++){


thinking
**Verifying pragma usage in loops**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "sed -n '100,220p' data/src/b+tree-omp/src/kernel/kernel_cpu_2.c" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 6ms:
			for(i = 0; i < maxheight; i++){

				// process all leaves at each level
				#pragma omp loop
				for(thid = 0; thid < threadsPerBlock; thid++){

					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
						// this conditional statement is inserted to avoid crush due to but in original code
						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
							offset[bid] = knodes[currKnode[bid]].indices[thid];
						}
					}
					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
						// this conditional statement is inserted to avoid crush due to but in original code
						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];

thinking
**Verifying pragma insertion location**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "sed -n '60,120p' data/src/b+tree-omp/src/kernel/kernel_cpu_2.c" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 5ms:

	// timer
	long long time0;
	long long time1;
	long long time2;

	// common variables
	int i;

	time0 = get_time();

	//======================================================================================================================================================150
	//	MCPU SETUP
	//======================================================================================================================================================150

	int max_nthreads;
	// printf("max # of threads = %d\n", max_nthreads);
	// printf("set # of threads = %d\n", cores_arg);

	int threadsPerBlock;

thinking
**Preparing build and run commands**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc clean' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 10ms:
rm -rf main src/main.o src/util/num/num.o src/util/timer/timer.o src/kernel/kernel_cpu.o src/kernel/kernel_cpu_2.o

thinking
**Running build command with environment variables**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc CC=nvc++ CLASS=C' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 1.27s:
nvc++  -std=c99 -Wall -I/home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/gate_sdk -O3 -Minfo -mp=gpu -gpu=cc89 -c src/main.c -o src/main.o
"src/main.c", line 1854: warning: variable "j" was declared but never referenced [declared_but_not_referenced]
    int i,j,fail;
          ^

Remark: individual warnings can be suppressed with "--diag_suppress <warning-name>"

"src/main.c", line 2182: warning: variable "records_mem" was declared but never referenced [declared_but_not_referenced]
  				long records_mem = (long)rootLoc;
  				     ^

"src/main.c", line 2188: warning: variable "knodes_mem" was declared but never referenced [declared_but_not_referenced]
  				long knodes_mem = (long)(mem_used) - (long)rootLoc;
  				     ^

"src/main.c", line 2371: warning: variable "knodes_mem" was declared but never referenced [declared_but_not_referenced]
  				long knodes_mem = (long)(mem_used) - (long)rootLoc;
  				     ^

"src/main.c", line 1970: warning: variable "sPointer" was declared but never referenced [declared_but_not_referenced]

thinking
**Preparing to run timed GPU command**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run > gpu_output.txt 2>&1' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 591ms:


thinking
**Running output comparison**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'diff baseline_output.txt gpu_output.txt' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp exited 1 in 9ms:
1,289d0
< nvc++  -std=c99 -Wall -I/home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/gate_sdk -O3 -Minfo -mp=gpu -gpu=cc89 -c src/main.c -o src/main.o
< "src/main.c", line 1854: warning: variable "j" was declared but never referenced [declared_but_not_referenced]
<     int i,j,fail;
<           ^
< 
< Remark: individual warnings can be suppressed with "--diag_suppress <warning-name>"
< 
< "src/main.c", line 2182: warning: variable "records_mem" was declared but never referenced [declared_but_not_referenced]
<   				long records_mem = (long)rootLoc;
<   				     ^
< 
< "src/main.c", line 2188: warning: variable "knodes_mem" was declared but never referenced [declared_but_not_referenced]
<   				long knodes_mem = (long)(mem_used) - (long)rootLoc;
<   				     ^
< 
< "src/main.c", line 2371: warning: variable "knodes_mem" was declared but never referenced [declared_but_not_referenced]
<   				long knodes_mem = (long)(mem_used) - (long)rootLoc;
<   				     ^
< 

thinking
**Reviewing diff and git status**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'git status -sb' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 19ms:
## main...origin/main
 M Makefile.nvc
 M src/kernel/kernel_cpu.c
 M src/kernel/kernel_cpu_2.c
 M ../backprop-omp/Makefile.nvc
 M ../backprop-omp/src/backprop.c
 M ../backprop-omp/src/backprop_kernel.c
 M ../backprop-omp/src/facetrain.c
 M ../bfs-omp/Makefile.nvc
 M ../bfs-omp/src/bfs.c
 M ../hotspot-omp/Makefile.nvc
 D ../hotspot-omp/src/hotspot_openmp.cpp
 M ../lud-omp/Makefile.nvc
 M ../lud-omp/src/lud.c
 M ../lud-omp/src/lud_omp.c
 M ../lud-omp/src/tools/gen_input.c
 M ../nw-omp/Makefile.nvc
 M ../nw-omp/src/needle.cpp
 M ../srad-omp/Makefile.nvc
 M ../srad-omp/src/main.c

thinking
**Summarizing modifications and test outcomes**

I'll focus the final response on the key files we modified like `data_plan.md` and `src/kernel` files, mentioning created output logs `baseline_output.txt` and `gpu_output.txt` which show runtime differences (notably CPU vs GPU timings). I'll include compile warnings seen with nvc++ and how we used `OMP_TARGET_OFFLOAD=MANDATORY`. Also, I'll reference specific lines for important changes like `#pragma omp target data` in kernel files, note the diff results indicating slight runtime variations, and summarize test commands run along with their results and any warnings.
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "nl -ba data/src/b+tree-omp/src/kernel/kernel_cpu.c | sed -n '30,150p'" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 9ms:
    30	
    31	//========================================================================================================================================================================================================200
    32	//	KERNEL_CPU FUNCTION
    33	//========================================================================================================================================================================================================200
    34	
    35	void 
    36	kernel_gpu(	int cores_arg,
    37	
    38				record *records,
    39				knode *knodes,
    40				long knodes_elem,
    41				long records_elem,
    42	
    43				int order,
    44				long maxheight,
    45				int count,
    46	
    47				long *currKnode,
    48				long *offset,
    49				int *keys,

thinking
**Extracting line numbers for kernel_cpu_2 changes**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "nl -ba data/src/b+tree-omp/src/kernel/kernel_cpu_2.c | sed -n '60,220p'" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 8ms:
    60	
    61		// timer
    62		long long time0;
    63		long long time1;
    64		long long time2;
    65	
    66		// common variables
    67		int i;
    68	
    69		time0 = get_time();
    70	
    71		//======================================================================================================================================================150
    72		//	MCPU SETUP
    73		//======================================================================================================================================================150
    74	
    75		int max_nthreads;
    76		// printf("max # of threads = %d\n", max_nthreads);
    77		// printf("set # of threads = %d\n", cores_arg);
    78	
    79		int threadsPerBlock;

thinking
**Preparing file line references**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'nl -ba baseline_output.txt | tail -n 20' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 8ms:
   297	
   298	Getting input from file core...
   299	Transforming data to a GPU suitable structure...
   300	Tree transformation took 0.058429
   301	Waiting for command
   302	> 
   303	******command: j count=6000, rSize=6000 
   304	Time spent in different stages of CPU/MCPU KERNEL:
   305	 0.000000000000 s,  0.000000000000 % : MCPU: SET DEVICE
   306	 0.004970999900 s, 100.000000000000 % : CPU/MCPU: KERNEL
   307	Total time:
   308	0.004970999900 s
   309	
   310	 ******command: k count=10000 
   311	
   312	
   313	CPU Runtime: 0.004554s
   314	GPU Runtime: 0.003738s
   315	Non-Matching CPU-GPU Outputs Beyond Error Threshold of 0.05 Percent: 0
   316	

thinking
**Retrieving last lines of gpu_output.txt**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'nl -ba gpu_output.txt | tail -n 20' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 8ms:
     8	
     9	Getting input from file core...
    10	Transforming data to a GPU suitable structure...
    11	Tree transformation took 0.057621
    12	Waiting for command
    13	> 
    14	******command: j count=6000, rSize=6000 
    15	Time spent in different stages of CPU/MCPU KERNEL:
    16	 0.000000000000 s,  0.000000000000 % : MCPU: SET DEVICE
    17	 0.152568995953 s, 100.000000000000 % : CPU/MCPU: KERNEL
    18	Total time:
    19	0.152568995953 s
    20	
    21	 ******command: k count=10000 
    22	
    23	
    24	CPU Runtime: 0.005287s
    25	GPU Runtime: 0.002006s
    26	Non-Matching CPU-GPU Outputs Beyond Error Threshold of 0.05 Percent: 0
    27	

thinking
**Adding stdio.h include to fix warnings**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..65c55e1a6f262c41b19c720db625e38198fde989
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -90,59 +90,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

file update
M /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -13,2 +13,3 @@
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).


thinking
**Preparing for rebuild and testing**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc clean' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 10ms:
rm -rf main src/main.o src/util/num/num.o src/util/timer/timer.o src/kernel/kernel_cpu.o src/kernel/kernel_cpu_2.o

thinking
**Starting build process**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc CC=nvc++ CLASS=C' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 1.28s:
nvc++  -std=c99 -Wall -I/home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/gate_sdk -O3 -Minfo -mp=gpu -gpu=cc89 -c src/main.c -o src/main.o
"src/main.c", line 1854: warning: variable "j" was declared but never referenced [declared_but_not_referenced]
    int i,j,fail;
          ^

Remark: individual warnings can be suppressed with "--diag_suppress <warning-name>"

"src/main.c", line 2182: warning: variable "records_mem" was declared but never referenced [declared_but_not_referenced]
  				long records_mem = (long)rootLoc;
  				     ^

"src/main.c", line 2188: warning: variable "knodes_mem" was declared but never referenced [declared_but_not_referenced]
  				long knodes_mem = (long)(mem_used) - (long)rootLoc;
  				     ^

"src/main.c", line 2371: warning: variable "knodes_mem" was declared but never referenced [declared_but_not_referenced]
  				long knodes_mem = (long)(mem_used) - (long)rootLoc;
  				     ^

"src/main.c", line 1970: warning: variable "sPointer" was declared but never referenced [declared_but_not_referenced]

thinking
**Rerunning command with timeout**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run > gpu_output.txt 2>&1' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 578ms:


thinking
**Preparing to run diff command**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'diff baseline_output.txt gpu_output.txt' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp exited 1 in 8ms:
1,289d0
< nvc++  -std=c99 -Wall -I/home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/gate_sdk -O3 -Minfo -mp=gpu -gpu=cc89 -c src/main.c -o src/main.o
< "src/main.c", line 1854: warning: variable "j" was declared but never referenced [declared_but_not_referenced]
<     int i,j,fail;
<           ^
< 
< Remark: individual warnings can be suppressed with "--diag_suppress <warning-name>"
< 
< "src/main.c", line 2182: warning: variable "records_mem" was declared but never referenced [declared_but_not_referenced]
<   				long records_mem = (long)rootLoc;
<   				     ^
< 
< "src/main.c", line 2188: warning: variable "knodes_mem" was declared but never referenced [declared_but_not_referenced]
<   				long knodes_mem = (long)(mem_used) - (long)rootLoc;
<   				     ^
< 
< "src/main.c", line 2371: warning: variable "knodes_mem" was declared but never referenced [declared_but_not_referenced]
<   				long knodes_mem = (long)(mem_used) - (long)rootLoc;
<   				     ^
< 

thinking
**Summarizing runtime and compile log differences**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'nl -ba gpu_output.txt | tail -n 20' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 8ms:
     8	
     9	Getting input from file core...
    10	Transforming data to a GPU suitable structure...
    11	Tree transformation took 0.058693
    12	Waiting for command
    13	> 
    14	******command: j count=6000, rSize=6000 
    15	Time spent in different stages of CPU/MCPU KERNEL:
    16	 0.000001000000 s,  0.000685067556 % : MCPU: SET DEVICE
    17	 0.145970001817 s, 99.999313354492 % : CPU/MCPU: KERNEL
    18	Total time:
    19	0.145971000195 s
    20	
    21	 ******command: k count=10000 
    22	
    23	
    24	CPU Runtime: 0.005307s
    25	GPU Runtime: 0.002024s
    26	Non-Matching CPU-GPU Outputs Beyond Error Threshold of 0.05 Percent: 0
    27	

thinking
**Outlining final response structure**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "nl -ba data/src/b+tree-omp/src/kernel/kernel_cpu_2.c | sed -n '1,40p'" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 10ms:
     1	// #ifdef __cplusplus
     2	// extern "C" {
     3	// #endif
     4	
     5	//========================================================================================================================================================================================================200
     6	//	DEFINE/INCLUDE
     7	//========================================================================================================================================================================================================200
     8	
     9	//======================================================================================================================================================150
    10	//	LIBRARIES
    11	//======================================================================================================================================================150
    12	
    13	#include <stdlib.h>									// (in directory known to compiler)
    14	#include <stdio.h>									// needed by printf
    15	
    16	//======================================================================================================================================================150
    17	//	COMMON
    18	//======================================================================================================================================================150
    19	
    20	#include "../common.h"								// (in directory provided here)

thinking
**Preparing to reference data_plan lines**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "nl -ba data_plan.md | sed -n '1,80p'" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir/data/src/b+tree-omp succeeded in 8ms:
nl: data_plan.md: No such file or directory

thinking
**Verifying file creation location**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc 'ls data_plan.md' in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 9ms:
data_plan.md

thinking
**Preparing to extract lines from data_plan.md**
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

exec
/bin/bash -lc "nl -ba data_plan.md | sed -n '1,120p'" in /home/codex_user/lian/codex_baseline/serial_omp_rodinia_workdir succeeded in 8ms:
     1	# Data Management Plan
     2	
     3	## Arrays Inventory
     4	| Array Name | Size | Type | Init | Access |
     5	|------------|------|------|------|--------|
     6	| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
     7	| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
     8	| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
     9	| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
    10	| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
    11	| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
    12	| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
    13	| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
    14	| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
    15	| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
    16	| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
    17	| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
    18	
    19	## Functions in Timed Region
    20	| Function | Arrays Accessed | Frequency | Must Run On |

thinking
**Summarizing code changes and tests**

I’m outlining code modifications to kernel_gpu and kernel_cpu_2 with OpenMP target data directives, noting the addition of `<stdio.h>`. I’ll reference the new `data_plan.md` document and detail the diffs between baseline and GPU outputs, highlighting runtime differences and compile warnings about unused variables and vectorization. I’ll also list the test commands run, their success with warnings, and suggest natural next steps for profiling and tuning.
codex
- Offloaded `kernel_gpu` so all query loops live inside a single `#pragma omp target data` region that maps the dataset once and launches the outer `bid` loop with `#pragma omp target teams loop thread_limit(threadsPerBlock)`; the inner-level scans are now annotated with `#pragma omp loop` so the GPU-level `threadsPerBlock` threads iterate in parallel (`data/src/b+tree-omp/src/kernel/kernel_cpu.c:35-118`).
- Applied the same Strategy A pattern to `kernel_cpu_2`, mapping `knodes`, range endpoints, and all scratch/output buffers once per timed run before driving the traversal + leaf scans with `target teams loop` and nested `loop` pragmas (`data/src/b+tree-omp/src/kernel/kernel_cpu_2.c:92-155`).
- Captured the array inventory, device mapping strategy, and transfer budget in `data_plan.md` to guide implementation decisions, and added `<stdio.h>` to `kernel_cpu_2.c` so the existing `printf` calls are well declared (`data_plan.md:1-64`, `data/src/b+tree-omp/src/kernel/kernel_cpu_2.c:13-15`).
- `diff baseline_output.txt gpu_output.txt` highlights the new GPU timing behavior: `Tree transformation` shifted from 0.058429→0.058693 s, the `MCPU` stage now takes ~1 μs and the kernel stage ~0.14597 s, and the final runtimes moved from 0.004554/0.003738 (CPU/GPU) to 0.005307/0.002024 s (`baseline_output.txt:299-314`, `gpu_output.txt:10-25`).

- Tests: `OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc CC=nvc++ CLASS=C` (passes; `nvc++` still reports the pre-existing `main.c` unused-variable warnings plus `max_nthreads` being unused in both kernel sources, and the new `<stdio.h>` include clears the previous implicit `printf` warning).
- Tests: `OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run > gpu_output.txt 2>&1` (runs to completion with the new GPU output log above).
- Tests: `diff baseline_output.txt gpu_output.txt` (exit 1 because the GPU-driven kernel now reports longer kernel time but shorter GPU runtime; differences captured in the summary above).

- Next steps: 1) Run the prescribed Class B `nsys` profile once the manifest is ready to confirm that the teams/loop configuration saturates the RTX 4060 Ti, and 2) experiment with tuning `threadsPerBlock` vs. `thread_limit`/team counts if profiling shows low occupancy.
file update:
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
index dafe1abcf66c09a16841eb0777a90113b9bce5bb..ea0bd7ba8d1764b817650ff62ede86d5168b11de
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu.c
@@ -72,19 +72,18 @@
 	int i;
 
 
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
-
 	// process number of querries
-
+	#pragma omp target data map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])
 	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
 		for(bid = 0; bid < count; bid++){
 
 			// process levels of the tree
 			for(i = 0; i < maxheight; i++){
 
 				// process all leaves at each level
+				#pragma omp loop
 				for(thid = 0; thid < threadsPerBlock; thid++){
 
 					// if value is between the two keys
@@ -107,6 +106,7 @@
 			//At this point, we have a candidate leaf node which may contain
 			//the target record.  Check each key to hopefully find the record
 			// process all leaves at each level
+			#pragma omp loop
 			for(thid = 0; thid < threadsPerBlock; thid++){
 
 				if(knodes[currKnode[bid]].keys[thid] == keys[bid]){
@@ -162,10 +162,6 @@
 	int bid;
 	int i;
 
-
-	int x = 100;
-	int *A;
-	A = (int *) malloc (sizeof(int)*x);
 
 	// process number of querries
 
@@ -207,5 +203,3 @@
 
 	}
 }
-
-
diff --git a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
index d5eda48b5f025db955b9ae615e012674d6244a80..55211ffdb12c38153b49bfc368fd091e782d4720
--- a/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
+++ b/serial_omp_rodinia_workdir/data/src/b+tree-omp/src/kernel/kernel_cpu_2.c
@@ -11,6 +11,7 @@
 //======================================================================================================================================================150
 
 #include <stdlib.h>									// (in directory known to compiler)
+#include <stdio.h>									// needed by printf
 
 //======================================================================================================================================================150
 //	COMMON
@@ -90,59 +91,67 @@
 	int bid;
 
 	// process number of querries
-	for(bid = 0; bid < count; bid++){
+	#pragma omp target data map(to: knodes[0:knodes_elem], start[0:count], end[0:count]) \
+	                        map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])
+	{
+		#pragma omp target teams loop thread_limit(threadsPerBlock)
+		for(bid = 0; bid < count; bid++){
 
-		// process levels of the tree
-		for(i = 0; i < maxheight; i++){
+			// process levels of the tree
+			for(i = 0; i < maxheight; i++){
 
-			// process all leaves at each level
-			for(thid = 0; thid < threadsPerBlock; thid++){
+				// process all leaves at each level
+				#pragma omp loop
+				for(thid = 0; thid < threadsPerBlock; thid++){
 
-				if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
-						offset[bid] = knodes[currKnode[bid]].indices[thid];
+					if((knodes[currKnode[bid]].keys[thid] <= start[bid]) && (knodes[currKnode[bid]].keys[thid+1] > start[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[currKnode[bid]].indices[thid] < knodes_elem){
+							offset[bid] = knodes[currKnode[bid]].indices[thid];
+						}
 					}
-				}
-				if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
-					// this conditional statement is inserted to avoid crush due to but in original code
-					// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
-					// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
-					if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
-						offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+					if((knodes[lastKnode[bid]].keys[thid] <= end[bid]) && (knodes[lastKnode[bid]].keys[thid+1] > end[bid])){
+						// this conditional statement is inserted to avoid crush due to but in original code
+						// "offset_2[bid]" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault
+						// more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address
+						if(knodes[lastKnode[bid]].indices[thid] < knodes_elem){
+							offset_2[bid] = knodes[lastKnode[bid]].indices[thid];
+						}
 					}
+
 				}
 
+				// set for next tree level
+				currKnode[bid] = offset[bid];
+				lastKnode[bid] = offset_2[bid];
+
 			}
 
-			// set for next tree level
-			currKnode[bid] = offset[bid];
-			lastKnode[bid] = offset_2[bid];
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		}
-
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the starting record
+				if(knodes[currKnode[bid]].keys[thid] == start[bid]){
+					recstart[bid] = knodes[currKnode[bid]].indices[thid];
+				}
 
-			// Find the index of the starting record
-			if(knodes[currKnode[bid]].keys[thid] == start[bid]){
-				recstart[bid] = knodes[currKnode[bid]].indices[thid];
 			}
 
-		}
+			// process leaves
+			#pragma omp loop
+			for(thid = 0; thid < threadsPerBlock; thid++){
 
-		// process leaves
-		for(thid = 0; thid < threadsPerBlock; thid++){
+				// Find the index of the ending record
+				if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
+					reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
+				}
 
-			// Find the index of the ending record
-			if(knodes[lastKnode[bid]].keys[thid] == end[bid]){
-				reclength[bid] = knodes[lastKnode[bid]].indices[thid] - recstart[bid]+1;
 			}
 
 		}
-
 	}
 
 	time2 = get_time();
diff --git a/serial_omp_rodinia_workdir/data_plan.md b/serial_omp_rodinia_workdir/data_plan.md
index 82c53f58e9b9adcb0667648e9b5b3e101b6cdf58..5fa6be6d441f742e55b201693340c0651de7d234
--- a/serial_omp_rodinia_workdir/data_plan.md
+++ b/serial_omp_rodinia_workdir/data_plan.md
@@ -3,64 +3,62 @@
 ## Arrays Inventory
 | Array Name | Size | Type | Init | Access |
 |------------|------|------|------|--------|
-| `records` | `records_elem * sizeof(record)` | working | host | RO |
-| `knodes` | `knodes_elem * sizeof(knode)` | working | host | RO |
-| `currKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `keys` | `count * sizeof(int)` | const | host (rand) | RO |
-| `ans` | `count * sizeof(record)` | working | host (−1) | RW |
-| `lastKnode` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `offset_2` | `count * sizeof(long)` | working | host (zeroed) | RW |
-| `start` | `count * sizeof(int)` | const | host (rand range) | RO |
-| `end` | `count * sizeof(int)` | const | host (derived from start) | RO |
-| `recstart` | `count * sizeof(int)` | working | host (zeroed) | RW |
-| `reclength` | `count * sizeof(int)` | working | host (zeroed) | RW |
+| `records` | `records_elem * sizeof(record)` | const | host (loaded before timed region) | R |
+| `knodes` | `knodes_elem * sizeof(knode)` | const | host (populated before kernel) | R |
+| `currKnode` | `count * sizeof(long)` | working | host (memset 0) | R/W |
+| `offset` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `keys` | `count * sizeof(int)` | const | host (rand-generated) | R |
+| `ans` | `count * sizeof(record)` | working | host (initialized to -1) | R/W |
+| `lastKnode` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `offset_2` | `count * sizeof(long)` | scratch | host (memset 0) | R/W |
+| `start` | `count * sizeof(int)` | const | host (generated from user input) | R |
+| `end` | `count * sizeof(int)` | const | host (derived from `start` + `rSize`) | R |
+| `recstart` | `count * sizeof(int)` | working | host (zeros) | R/W |
+| `reclength` | `count * sizeof(int)` | working | host (zeros) | R/W |
 
 ## Functions in Timed Region
 | Function | Arrays Accessed | Frequency | Must Run On |
 |----------|------------------|-----------|-------------|
-| `kernel_gpu` | `records`, `knodes`, `currKnode`, `offset`, `keys`, `ans` | once per `k` command | device |
-| `kernel_cpu_2` | `knodes`, `currKnode`, `offset`, `lastKnode`, `offset_2`, `start`, `end`, `recstart`, `reclength` | once per `j` command | device |
+| `kernel_gpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command | device |
+| `kernel_cpu` | `records, knodes, currKnode, offset, keys, ans` | per `k` command (for CPU comparison) | host |
+| `kernel_cpu_2` | `knodes, currKnode, offset, lastKnode, offset_2, start, end, recstart, reclength` | per `j` command | device |
 
 ## Data Movement Strategy
 
-**Chosen Strategy:** A (default since none of the ordering rules above apply)
-
-### Device Allocations (once)
-Each kernel enters a `#pragma omp target data` region, mapping the host buffers before launching the per-query loop on the GPU. The outer `#pragma omp target teams loop` drives the `bid` iterations while the inner level/leaf scans stay on the device.
+**Chosen Strategy:** A (target data region + map clauses)
 
-- `kernel_gpu`: `map(to: records[0:records_elem], knodes[0:knodes_elem], keys[0:count])`
-- `kernel_gpu`: `map(tofrom: currKnode[0:count], offset[0:count], ans[0:count])`
-- `kernel_cpu_2`: `map(to: knodes[0:knodes_elem], start[0:count], end[0:count])`
-- `kernel_cpu_2`: `map(tofrom: currKnode[0:count], offset[0:count], lastKnode[0:count], offset_2[0:count], recstart[0:count], reclength[0:count])`
+### Device Allocations (once per kernel invocation)
+- `records`, `knodes`, `keys`, `start`, `end`: `map(to: ...)` inside `#pragma omp target data`.
+- Scratch/output buffers (`currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength`): `map(tofrom: ...)` so host initialization and device results stay coherent.
 
 ### Host→Device Transfers
-- When: upon entering each kernel's `target data` region (before loop nest)
-- Arrays: `records` (~`records_elem*4` bytes), `knodes` (~`knodes_elem*sizeof(knode)`), `keys` (`count*4` bytes), plus zeroed state buffers copied via `map(tofrom)` (`currKnode`, `offset`, etc.)
-- Total H→D: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` + `count*(2*sizeof(long)+sizeof(int)+sizeof(record))` for `kernel_gpu`, and + `count*(4*sizeof(long)+2*sizeof(int))` for `kernel_cpu_2`
+- **When:** Immediately after entering the `#pragma omp target data` region that wraps each timed kernel call.
+- **Arrays:** `records`, `knodes`, `keys`, `start`, `end`, plus the `tofrom` buffers listed above.
+- **Total H→D:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode) + count*(4*sizeof(long) + 7*sizeof(long/int) + sizeof(record))` (~ dataset size + scratch arrays).
 
 ### Device→Host Transfers
-- When: exiting each kernel's `target data` region
-- Arrays: `currKnode`, `offset`, `ans` (for `kernel_gpu`), and `currKnode`, `offset`, `lastKnode`, `offset_2`, `recstart`, `reclength` (for `kernel_cpu_2`)
-- Total D→H: same as the active `map(tofrom)` buffers
+- **When:** Upon exiting the `#pragma omp target data` region (after kernel execution completes).
+- **Arrays:** `currKnode`, `offset`, `ans`, `lastKnode`, `offset_2`, `recstart`, `reclength` (capturing updated state for post-processing or verification).
+- **Total D→H:** `count*(4*sizeof(long) + 3*sizeof(long) + 3*sizeof(int) + sizeof(record))` (~scratch + output).
 
-### Transfers During Iterations: NO
-- All arrays stay resident for the duration of the region; there are no intermediate host-device copies while the nested traverse loops run.
+### Transfers During Iterations
+- **NO:** Once data is resident via the target data region at the start of the kernel, all processing stays on-device until the region exits.
 
-## Critical Checks (for chosen strategy)
+### Critical Checks (for Strategy A)
+- [x] Functions inside the target data region are launched with `#pragma omp target teams loop` so mapped arrays use the `present` data.
+- [x] Scratch/output arrays are captured via `map(tofrom: ...)` so no explicit `enter/exit data` or `omp_target_alloc` orchestration is required.
+- [x] All arrays are consistently mapped once per invocation; no stray host pointers are dereferenced on-device.
 
-**Strategy A:**
-- [x] Each kernel enters `#pragma omp target data` so the mapped buffers are `present` for the outer `#pragma omp target teams loop` that iterates over queries.
-- [x] The sequential per-level scans stay inside the target region so no secondary scratch allocations or host/device transfers are needed beyond the mapped arrays.
-
-**Common Mistakes:**
-- Each kernel must call its traversal loops directly inside the `target data` region so CUDA/LLVM sees the `present` arrays.
-- No scratch buffers are left mapped only on the host.
-- We will inline all level-traversal loops inside the `target teams loop` so no extra offload-ready helper is called.
+### Common Mistakes
+- Mixing host-only helper functions with GPU loops (we keep `kernel_cpu` on host and only offload `kernel_gpu/kernel_cpu_2`).
+- Forgetting to transfer the updates to `recstart`/`reclength` back to the host after `kernel_cpu_2`.
+- Using more than the available `threadsPerBlock` when launching `#pragma omp loop` over `thid`.
 
 ## Expected Transfer Volume
-- Total: ~`records_elem*sizeof(record)` + `knodes_elem*sizeof(knode)` H→D plus `count*(a few KB)`.
-- D→H: only the small query-state arrays (`currKnode`/`offset`/`ans`/`lastKnode`/`recstart`/`reclength`), roughly `count*O(32)` bytes per kernel.
-- **Red flag:** Cross-check before profiling; per-command transfers remain proportional to `count`, not the tree size.
+- **Total:** `records_elem*sizeof(record) + knodes_elem*sizeof(knode)` (~primary data set) + `count*`(`sizeof(record)` + `4*sizeof(long)` + `5*sizeof(int)`) for scratch/output.
+- **Red flag:** If the actual transfer volume exceeds ~2× this (e.g., due to repeated copies each level) the target data region is not being respected.
 
-**Summary:** 12 arrays (10 working, 2 const), 2 functions in the timed region, Strategy A. Expected host→device payload dominated by `knodes`/`records`, device→host payload equals combined state arrays.
+## Summary
+- Arrays: 12 total (6 working/scratch, 6 const).
+- Functions: 3 (two device, one host comparison).
+- Strategy: A. Expected ~dataset size H→D and ~scratch size D→H each timed invocation (plus small setup/verification cost).

tokens used
424,008