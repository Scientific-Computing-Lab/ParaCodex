Offloaded both B+ tree kernels onto the GPU by wrapping the query loops inside `#pragma omp target teams` regions (with `#pragma omp loop`), so each query traverses the tree on the device while all required arrays are mapped in and back out of the GPU memory.

- `src/kernel/kernel_cpu.c:35-90` – Replaced the old placeholder GPU routine with a `target teams` region that computes `threadsPerBlock`/`numTeams`, maps `records`, `knodes`, `keys`, `currKnode`, `offset`, and `ans`, and runs each query’s level/key scans through the device-enabled loop nest; this uses the requested teams+loop offload pattern without `teams distribute parallel for`.
- `src/kernel/kernel_cpu_2.c:25-129` – Added the missing `<stdio.h>` include, removed the unused `int i`, and rewrote the range-search kernel so it also launches via `target teams`/`#pragma omp loop`, computes `numTeams`, maps the start/end arrays plus the various node/offset buffers, and preserves the timing reports for the CPU/MCPU stages.

- **Testing:** `make -f Makefile.nvc clean` then `make -f Makefile.nvc` (build succeeds but `nvc++` still emits existing warnings about unused variables in `src/main.c` and `src/kernel/kernel_cpu.c`, e.g., `max_nthreads`/`A`); `./main core 2 file input/mil.txt command input/command.txt` (transform stage ~0.059 s, the bundled range-query kernel took ~0.144 s, followed by the bundled point-kernel timings that show CPU 0.005 s vs. GPU 0.002 s and matching outputs).

Next step: collect Nsight (nsys) profiles to check occupancy/latency of the new kernels and consider refactoring the inner-level scans so that more work can run per team/thread once profiling highlights the true bottlenecks.