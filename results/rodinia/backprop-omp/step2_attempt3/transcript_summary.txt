**Summary**
- Replaced the unsupported `#pragma omp loop` with a `#pragma omp simd reduction` so the forward kernel can vectorize the dot product while staying under the existing `target teams distribute` launch (`src/backprop.c:302`).
- The adjust-weights kernel now caches `d_delta[j]`/`d_ly[k]` and wraps the innermost updates in `#pragma omp simd`, keeping `device_new_dw` in registers before writing `d_w`/`d_oldw` back to global memory (`src/backprop.c:452`).
- Rebuilt `optimization_plan.md` using the `cuda_gpu_kern_sum` and `cuda_gpu_mem_time_sum` data to document the current bottlenecks, final summary, and remaining Action 4A work (`profile.log:5`, `profile.log:12`; `optimization_plan.md:1`).

**Testing**
- `timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run` (succeeds; the latest `current_output.txt` shows GPU runtimes of 1.461 ms, 0.007 ms, 0.006 ms, and 0.077 ms for the four kernel launches).

**Next Steps**
1. Hoist the flattened host/device scratch buffers (e.g., `conn_gpu`, `w_gpu`, `oldw_gpu`, `l2_gpu`) into a persistent `omp_target_alloc`/`target data` region so the 12 H2D + 6 D2H transfers per iteration disappear (Action 4A).