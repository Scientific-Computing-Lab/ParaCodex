**GPU Offload**
- Updated the point-query kernel so that the OpenMP runtime picks team/thread counts rather than hard-coding `num_teams`/`thread_limit`, and the `target data` region now maps only the tree/record buffers plus read-only keys while keeping the per-query arrays on-device (`src/kernel/kernel_cpu.c:62`, `src/kernel/kernel_cpu.c:75`). Knowing the host is paired with an Ada Lovelace RTX 4060 Ti (`system_info.txt:253`) and that `_OPENMP 202011` (OpenMP 5.1) is available, this keeps the data resident and lets the device expose each query as a team over the `#pragma omp target teams loop`.
- Applied the same modern style to the range kernel: `threadsPerBlock` is computed uniformly (`src/kernel/kernel_cpu_2.c:79`), the single `target data` region maps the read-only range inputs, and the loop now uses `#pragma omp target teams loop` before the per-level scan (`src/kernel/kernel_cpu_2.c:93`), which preserves the original algorithm while avoiding redundant data movement or manual team sizing.

**Correctness**
- `diff baseline_output.txt gpu_output.txt` shows only timing jitter (`baseline_output.txt:11` vs `gpu_output.txt:11` for the tree transformation time and `baseline_output.txt:24` vs `gpu_output.txt:24` for the CPU runtime); the actual search results stay identical (`baseline_output.txt:26` and `profile.log:84` both report “Non-Matching CPU-GPU Outputs … 0”) so the numerics remain correct.

**Profiling**
- Ran `FORCE_OMP_GPU=1 OMP_TARGET_OFFLOAD=MANDATORY nsys ...` and captured the record in `profile.log`; it generated `nsys_profile.nsys-rep` (see `profile.log:133`) but noted there was no CUDA trace/memory data to report (`profile.log:91`), so the `[CUDA memcpy Host-to-Device]` breakdown is absent even though the run is recorded.

Next steps:
1. Open the generated `nsys_profile.nsys-rep` to inspect kernel occupancy/transfer behavior and decide how to tune team/thread counts or kernel launch parameters in Step 3 for tighter data-movement control.