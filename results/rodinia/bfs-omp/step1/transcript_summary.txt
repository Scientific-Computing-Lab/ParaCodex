- Hooked the GPU loop into an OpenMP target-data region so the graph metadata (`h_graph_nodes`/`h_graph_edges`) and the working depth array stay resident on the device while the frontier scan and update kernels run entirely on the accelerator (see `data/src/bfs-omp/src/bfs.c:150-205`). Added `<omp.h>` and explanatory comments, and cleaned up the GPU mask/visited/cost buffers afterwards (`data/src/bfs-omp/src/bfs.c:260-269`).

- Captured the entire Strategy A data-flow plan in `data/src/bfs-omp/data_plan.md`, detailing array roles, allocations, host-to-device transfers, and reuse of the target-data region during the timed BFS stage.

- Tests: `cd data/src/bfs-omp && OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc CC=nvc++ CLASS=C clean` then `OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run > gpu_output.txt 2>&1` (baseline run was captured earlier in `baseline_output.txt` following the same instructions).

- Diff: `cd data/src/bfs-omp && diff baseline_output.txt gpu_output.txt` now only reports shifted compiler diagnostics (loop/vectorization reports and the `num_omp_threads` warning line number) plus the new GPU runtime (~0.155s versus the original ~0.048s); no functional mismatch was observed.

Next steps:
1. Profile the new GPU kernels under a CLASS B build with `nsys` (per the workflow) to identify whether the scan or update pass dominates and to verify the offload isnâ€™t bandwidth-bound.
2. Exercise the modified binary on larger graphs or additional datasets to confirm `compareResults` still passes and to gather data for further occupancy/tuning improvements.