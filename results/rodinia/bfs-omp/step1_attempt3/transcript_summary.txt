**Changes**
- Rebuilt `BFSGraph` to use helper stages so the GPU buffers stay resident: `allocate_device_arrays`/`copy_to_device` stage the host masks/results, `run_gpu_bfs` does the two inline `target teams loop`s, and `copy_from_device`/`release_device_arrays` clean up afterward; this also lets us honor the command-line thread count with `omp_set_num_threads` (see `src/bfs.c:83-255` and `src/bfs.c:262-364`).
- Added `init_device_context` so we choose the real GPU device (fallback from the host device number) before any `omp_target_alloc/memcpy`, which keeps all of the Strategy C requirements (in-place `is_device_ptr` loops, scratch buffers on the device) satisfied (`src/bfs.c:262-364`).
- Refreshed `data_plan.md` to enumerate both the host staging arrays (`h_graph_*_gpu`) and the device buffers (`d_*`), to explain the one-time transfers (~48 MB H→D, 4 MB D→H), and to document the new helper-based workflow (`data/src/bfs-omp/data_plan.md:1-44`).

**Tests**
- `timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run` (baseline run that still compiles only for the host and therefore aborts under `OMP_TARGET_OFFLOAD=MANDATORY`): `baseline_output.txt:1-2` shows `make run` aborted with a core dump.
- `OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C DEVICE=gpu SM=cc86 run` (GPU build/run with the Ada Lovelace compute capability); see `data/src/bfs-omp/gpu_output.txt:1-44` for the compiler info plus `GPU Runtime: 0.001678s`, `CPU Runtime: 0.048717s`, and zero mismatches.

Next steps:
1. Regenerate the CLASS C baseline output with the same GPU-aware build so you have a valid `baseline_output.txt` to compare (then run the recommended `diff baseline_output.txt gpu_output.txt`).
2. Once the stabilized GPU run is confirmed, trigger the CLASS B profile command from the instructions to collect `nsys`/GPU hotspot data.