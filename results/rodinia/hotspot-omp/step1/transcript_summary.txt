The GPU branch now keeps `power/temp/result` resident in a single `#pragma omp target data` region and launches both the stencil sweep and the staging copy with `#pragma omp target teams loop collapse(2)`, so the entire timestep works on-device while still feeding the host-side buffers that `compareResults` needs.

- **Code (`data/src/hotspot-omp/src/hotspot_openmp.cpp:37-259`)**: added `<omp.h>` and layered the two nested loops in `single_iteration_gpu` with `target teams loop collapse(2)` plus `map(present: …)` so each sweep and the subsequent copy happen entirely on the GPU; the GPU branch of `compute_tran_temp` now wraps its iterations in a `target data map(...)` region with a final `target update from(result[0:grid_size])` to bring the staged output home, keeping transfers to a single H→D batch for `power/temp` and a single D→H update of `result`. Comments highlight the new kernel launches and the device-resident buffers, and a backup of the original source is preserved at `data/src/hotspot-omp/src/hotspot_openmp.cpp.backup`.
- **Data plan (`data/src/hotspot-omp/data_plan.md`)**: documents every array used in the timed region, lists the CPU/GPU functions, and records the Strategy A mapping so we know which buffers move once per run and that no incremental transfers occur.
- **Testing**: `OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc CC=nvc++ CLASS=C`, `OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc CC=nvc++ CLASS=C run > gpu_output.txt 2>&1`, and `diff baseline_output.txt gpu_output.txt` (the diff is expected because the new `Minfo` log now lists the target teams/collapse loops and the GPU runtime is ~0.150 s instead of the original ~0.0013 s; CPU runtime stayed around 0.0014 s).
- **Verification**: `baseline_output.txt` (original serial build/run) is still available for regression comparison, and the final GPU run writes to `gpu_output.txt` for ongoing checks.

1. Profile the GPU build under CLASS B (e.g., with `nsys` as outlined in the instructions) to see whether launch/occupancy issues dominate before tackling further optimizations.
2. Try larger grids or tiled batches to amortize the kernel-launch overhead and to check whether the device-resident data strategy yields a speedup once the workload grows.