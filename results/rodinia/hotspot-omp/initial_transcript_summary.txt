**GPU Offload**
- Replaced `single_iteration_gpu` with two `#pragma omp target teams loop collapse(2)` kernels so the delta computation and result-to-temp copy both execute entirely on the GPU, keeping the original stencil logic and writing `result` and `temp` via `result[index] = curr_temp + delta` (`data/src/hotspot-omp/src/hotspot_openmp.cpp:36-117`).
- Wrapped the GPU simulation loop in a persistent `#pragma omp target data` region so `power`, `temp`, and `result` stay resident between iterations; the host now dispatches either the CPU or GPU loop branch (CPU path unchanged) and still emits the verbose iteration counter (`data/src/hotspot-omp/src/hotspot_openmp.cpp:223-247`).

**Build & Test**
- `cd data/src/hotspot-omp && make -f Makefile.nvc clean && make -f Makefile.nvc` (nvc++ emitted its usual warnings about the unused `t` variable and deprecated `char*` literal usage).
- `./main 512 512 2 4 input/temp_512 input/power_512` â†’ CPU runtime 0.001405s, GPU runtime 0.151409s, zero mismatched cells; the output matches the baseline.

Next steps:
1. Profile the GPU kernel with `nsys` or a similar tool so thread limits/collapsed dimensions can be tuned for larger problem sizes.
2. Drive the validated build through the Gate harness (`gate_sdk/scripts/gate_harness.py`) or any regression suite that compares against `golden_labels` to ensure long-running stability.