Optimized the LUD GPU path in `data/src/lud-omp/src` by keeping the matrix resident through the whole offload region and issuing two `teams loop` kernels per stage, and added explicit host declarations to avoid build-time implicit-declaration warnings.

- **GPU offload** `data/src/lud-omp/src/lud_omp.c:27`: wrap the sweep in `#pragma omp target data` so the matrix stays mapped to the device, launch the upper- and lower-triangular updates with `teams loop thread_limit(256)` (one kernel per sweep), and compute the pivot on-device to keep `a` updates on the GPU between kernels.
- **Host declarations** `data/src/lud-omp/src/lud.c:40`: declare `lud_omp_cpu`/`lud_omp_gpu` before `main` so the clean build no longer has implicit-declaration warnings while leaving the rest of the driver logic intact.
- **Testing**: `make -f Makefile.nvc clean && make -f Makefile.nvc CC=nvc++ CLASS=B` (only the longstanding `common/common.c` warning about a missing `return` remains), then `./main -s 256 -v` with the default matrix generator to verify correctness and gate stats.

Next steps:
1. Profile `./main -s 1024 -v` with `nsys`/`nvprof` to verify whether the two kernels dominate runtime on the Ada Lovelace GPU and to tune `thread_limit` or team sizes accordingly.
2. Explore tiling or shared-memory buffering inside the `k` loop so more work can be done inside each `teams loop` kernel before considering fusion of the two triangular updates.