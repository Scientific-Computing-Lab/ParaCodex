The driver now targets the GPU with an OpenMP `target data` + `target teams`/`distribute parallel for collapse(2)` stencil that keeps the original argument checks and validation while caching row bases and using device-alias pointers so the stencil mirrors the CUDA layout (see `data/src/microXORh-omp/main.cpp:44`, `main.cpp:50`, and `main.cpp:55`).

- Added a `localbin/nvc++` wrapper that rewrites `-arch=sm_*` into the `-gpu=cc*` form that `nvc++` accepts, so the existing Makefile can stay untouched (`localbin/nvc++:1`).
- Captured the baseline metrics, micro-optimization goals, and the final 0.25 s runtime/≈1.4× speedup in `optimization_plan.md` (`optimization_plan.md:3` for the plan and `optimization_plan.md:30` for the final summary).
- Tests: `timeout 300 env PATH="…/localbin:$PATH" EXTRA_CFLAGS="-mp=gpu" OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run` (output now in `profile.log:1`), rerun to refresh `profile.log`, and `/usr/bin/time -f "%e" env OMP_TARGET_OFFLOAD=MANDATORY ./microXORh.exe 1024 32` to record the 0.25 s runtime.

Next steps:
1. Profile the `target teams` region with `nsys` (or a vendor profiler) under the same env so you can collect per-kernel timings and confirm GPU utilization.
2. Sweep `blockEdge`/`thread_limit` and larger `N` values to verify the tiling still matches the CUDA grid for different problem sizes and to look for any remaining load imbalance.