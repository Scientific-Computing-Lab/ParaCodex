- Replaced the CUDA kernel and host orchestration in `data/src/nano-XOR-omp/nanoXOR.cpp:1` with a Strategy-A OpenMP offload: both grids live in a `#pragma omp target data` region and the device work is expressed through `#pragma omp target teams loop collapse(2)` on the Ada Lovelace RTX 4060, while all `cuda` calls and `cleanup` are removed and validation/`GATE_CHECKSUM` remain untouched.
- Added the required `data/src/nano-XOR-omp/data_plan.md:1` detailing the CUDA memory use, migration rationale, and the chosen OMP mapping plus recorded the requested backup as `data/src/nano-XOR-omp/nanoXOR.cpp.bak:1` (there is no `main.cu` in this workspace to back up).
- Tests: `make -f Makefile.nvc clean`, `make -f Makefile.nvc`, `OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc run > gpu_output.txt 2>&1` (run output recorded in `data/src/nano-XOR-omp/gpu_output.txt:1`, showing validation passed and the `GATE:SUM` lines).
- Notes: `diff data/src/nano-XOR-omp/baseline_output.txt data/src/nano-XOR-omp/gpu_output.txt` only differs in the compiler banner and checksum value (the random inputs change each run so the `GATE:SUM` entries no longer match).

Next steps:
1. Seed or preset the input grid so that repeated runs reproduce the same checksum and `baseline_output.txt` diff can pass deterministically.
2. Run the recommended profiling sequence (nsys with `OMP_TARGET_OFFLOAD=MANDATORY`) once the checksum is stabilized to inspect GPU kernel behavior.