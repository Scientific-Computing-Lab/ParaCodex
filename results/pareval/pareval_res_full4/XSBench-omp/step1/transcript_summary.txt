Converted the XSBench CUDA kernels into OpenMP offload versions, documented the new data management plan, and kept the requested `Simulation.cpp.bak` snapshot so you can compare before/after behaviour.

- Added `data/src/XSBench-omp/data_plan.md` detailing the CUDA → OpenMP Strategy C mapping, array transfers, kernel inventory, and key migration issues before touching the code.
- Reworked device setup and teardown inside `data/src/XSBench-omp/GridInit.cpp:1-84` to use `omp_target_alloc`/`omp_target_memcpy` with helper templates instead of `cudaMalloc/cudaMemcpy`.
- Simplified `data/src/XSBench-omp/XSbench_header.cuh:73-110` to expose plain C prototypes (no CUDA/Thrust includes) so the new kernel helpers can compile cleanly.
- Rebuilt `data/src/XSBench-omp/Simulation.cpp:10-870` with OpenMP helpers that allocate sample buffers, copy/resync arrays, execute `#pragma omp target teams loop` kernels, and perform the host-side sorts/counts; the baseline and optimizations now move `SimulationData` to the device, invoke the converted kernels, and copy the verification data back via `copy_from_device`.
- Updated `data/src/XSBench-omp/io.cpp:121-179` (and added `<omp.h>`) so the CLI prints “Programming Model: OpenMP Offload” and the default target ID instead of CUDA device info.
- Captured the run log in `data/src/XSBench-omp/gpu_output.txt` (baseline comparison diffed below), and left `Simulation.cpp.bak` as the requested backup.

- **Tests:** `make -f Makefile.nvc` and `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > gpu_output.txt 2>&1` (both succeed; run logs in `data/src/XSBench-omp/gpu_output.txt`).
- **Diff vs baseline:** `diff baseline_output.txt gpu_output.txt` shows the new output says “Programming Model: OpenMP Offload” with `Runtime=0.541s`, `Lookups/s=184,687`, and profile line `281.243086,130.403042,0.617981,1,1`, whereas the CUDA baseline recorded the CUDA build log, a faster runtime (0.232s), and different lookups/s (431,119). These differences are expected because the new harness uses OpenMP target offload semantics.

Natural next steps:
1. Re-run the other optimization kernels (1–6) to ensure their verification hashes still match and to capture their runtimes.
2. Measure whether scheduling clauses (e.g., `schedule(dynamic,64)`) or chunking can recover some of the lost throughput shown in `gpu_output.txt`.