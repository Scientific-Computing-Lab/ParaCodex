Converted the XSBench CUDA kernels into OpenMP offload versions, documented the new data management plan, and kept the requested `Simulation.cpp.bak` snapshot so you can compare before/after behaviour.

- Added `data/src/XSBench-omp/data_plan.md` detailing the CUDA → OpenMP Strategy C mapping, array transfers, kernel inventory, and key migration issues before touching the code.
- Reworked device setup and teardown inside `data/src/XSBench-omp/GridInit.cpp:1-84` to use `omp_target_alloc`/`omp_target_memcpy` with helper templates instead of `cudaMalloc/cudaMemcpy`.
- Simplified `data/src/XSBench-omp/XSbench_header.cuh:73-110` to expose plain C prototypes (no CUDA/Thrust includes) so the new kernel helpers can compile cleanly.
- Rebuilt `data/src/XSBench-omp/Simulation.cpp:10-870` with OpenMP helpers that allocate sample buffers, copy/resync arrays, execute `#pragma omp target teams loop` kernels, and perform the host-side sorts/counts; the baseline and optimizations now move `SimulationData` to the device, invoke the converted kernels, and copy the verification data back via `copy_from_device`.
- Updated `data/src/XSBench-omp/io.cpp:121-179` (and added `<omp.h>`) so the CLI prints “Programming Model: OpenMP Offload” and the default target ID instead of CUDA device info.
- Captured the run log in `data/src/XSBench-omp/gpu_output.txt` (baseline comparison diffed below), and left `Simulation.cpp.bak` as the requested backup.

- **Tests:** `make -f Makefile.nvc` and `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > gpu_output.txt 2>&1` (both succeed; run logs in `data/src/XSBench-omp/gpu_output.txt`).
- **Diff vs baseline:** `diff baseline_output.txt gpu_output.txt` shows the new output says “Programming Model: OpenMP Offload” with `Runtime=0.541s`, `Lookups/s=184,687`, and profile line `281.243086,130.403042,0.617981,1,1`, whereas the CUDA baseline recorded the CUDA build log, a faster runtime (0.232s), and different lookups/s (431,119). These differences are expected because the new harness uses OpenMP target offload semantics.

Natural next steps:
1. Re-run the other optimization kernels (1–6) to ensure their verification hashes still match and to capture their runtimes.
2. Measure whether scheduling clauses (e.g., `schedule(dynamic,64)`) or chunking can recover some of the lost throughput shown in `gpu_output.txt`.

OpenAI Codex v0.72.0 (research preview)
--------
workdir: /root/codex_baseline/cuda_omp_pareval_workdir
model: gpt-5.1-codex-mini
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: medium
reasoning summaries: auto
session id: 019b2c68-ecb9-7f92-bd32-a6523f3c8c50
--------
user
Directory: /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp

# CUDA to OpenMP Migration

**Directory:** `/root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/`  
**Files:** - Simulation.cu  
**Reference:** `/root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/analysis.md`

**Required:** 
- Use `OMP_TARGET_OFFLOAD=MANDATORY` for all runs
- DO NOT use `distribute parallel for`

** IMPORTANT ** YOU MAY MODIFY THE MAKEFILE TO ADD ANYTHING YOU NEED TO RUN THE CODE.

## Workflow

### 0. Backup
Save backup of - Simulation.cu.

### 1. Get Baseline
```bash
Baseline cuda outpuut is in baseline_output.txt in /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/
```

### 2. Choose Data Strategy
Walk through IN ORDER, stop at first match:

```
RULE 1: Type B (Sparse/CSR)?              → STRATEGY A/C
RULE 2: Type C1 (Iterative Solvers/Butterfly)?→ STRATEGY C
RULE 3: Type C2 (Multigrid)?              → STRATEGY A
RULE 4: Multiple independent kernels?     → STRATEGY B
RULE 5: Otherwise                         → STRATEGY A
```

### 2.5. Create Data Management Plan
MANDATORY: Create data_plan.md in /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp before implementation

**FIRST: Understand CUDA memory model and map to OMP:**
- cudaMalloc + device pointers → omp_target_alloc OR target data map(alloc)
- cudaMemcpy H→D → map(to) OR omp_target_memcpy OR update to
- cudaMemcpy D→H → map(from) OR omp_target_memcpy OR update from
- Kernel launches in loops → target teams loop with is_device_ptr

**CUDA Pattern Recognition:**
```
Pattern 1: cudaMalloc once → kernel loop → cudaFree
  → Strategy C: omp_target_alloc + is_device_ptr

Pattern 2: Single kernel launch with data transfer
  → Strategy A: target data region

Pattern 3: Multiple kernels with dependencies
  → Strategy B: nowait + depend clauses
```

Analyze ALL arrays and kernels in timed region:

```markdown
# Data Management Plan

## CUDA Memory Analysis
List ALL device allocations and transfers:

| Array/Pointer | CUDA Allocation | Size | Transfer Pattern |
|---------------|-----------------|------|------------------|
| d_[name] | cudaMalloc | [bytes] | H→D once/D→H once/both |
| [name] | host array | [bytes] | source/destination |

**CUDA Operations:**
- cudaMalloc calls: [list with sizes]
- cudaMemcpy H→D: [list with timing]
- cudaMemcpy D→H: [list with timing]
- Kernel launches: [list with frequency]

## Kernel Inventory
| Kernel Name | Launch Config | Frequency | Arrays Used |
|-------------|---------------|-----------|-------------|
| kernel_name<<<G,B>>> | grid=[X], block=[Y] | per-iteration/once | [list] |

**Kernel Launch Patterns:**
- In outer loop? → Multiple target teams loop
- Sequential kernels? → Multiple target regions OR nowait+depend
- Conditional launch? → target if clause

## OMP Data Movement Strategy

**Chosen Strategy:** [A/B/C]

**Rationale:** [Map CUDA pattern to strategy]

**Device Allocations (OMP equivalent):**
```
CUDA: cudaMalloc(&d_arr, size)
OMP Strategy C: d_arr = omp_target_alloc(size, 0)
OMP Strategy A: #pragma omp target data map(alloc:arr[0:n])
```

**Host→Device Transfers (OMP equivalent):**
```
CUDA: cudaMemcpy(d_arr, h_arr, size, cudaMemcpyHostToDevice)
OMP Strategy C: omp_target_memcpy(d_arr, h_arr, size, 0, 0, 0, omp_get_initial_device())
OMP Strategy A: map(to:arr[0:n]) OR #pragma omp target update to(arr[0:n])
```
- When: [before iterations/once at start]
- Arrays: [list with sizes]
- Total H→D: ~[X] MB

**Device→Host Transfers (OMP equivalent):**
```
CUDA: cudaMemcpy(h_arr, d_arr, size, cudaMemcpyDeviceToHost)
OMP Strategy C: omp_target_memcpy(h_arr, d_arr, size, 0, 0, omp_get_initial_device(), 0)
OMP Strategy A: map(from:arr[0:n]) OR #pragma omp target update from(arr[0:n])
```
- When: [after iterations/once at end]
- Arrays: [list with sizes]
- Total D→H: ~[Y] MB

**Transfers During Iterations:** [YES/NO]
- If YES: [which arrays and why - may indicate wrong strategy]

## Kernel to OMP Mapping (short)
- Replace each CUDA kernel launch with a `#pragma omp target teams loop` over the same *logical* work domain.
- Replace `blockIdx/threadIdx` indexing with the loop induction variable.
- Keep bounds checks; keep inner device loops as normal C loops inside the offloaded loop body.

## Critical Migration Issues

**From analysis.md "OMP Migration Issues":**
- [ ] __syncthreads() usage: [locations and resolution strategy]
- [ ] Shared memory: [convert to private/firstprivate]
- [ ] Atomics: [verify OMP atomic equivalents]
- [ ] Dynamic indexing: [verify OMP handles correctly]

**__syncthreads() Resolution:**
- Within single kernel → May need to split into multiple target regions
- At kernel boundaries → Natural OMP barrier between target regions
- Strategy: [describe approach]

**Shared memory / barriers:**
- No direct equivalent for CUDA `__shared__` + `__syncthreads()`; refactor and document your approach.

## Expected Performance
- CUDA kernel time: [X] ms (from profiling if available)
- OMP expected: [Y] ms (may be slower due to __syncthreads elimination)
- Red flag: If >3x slower → wrong strategy or missing parallelism

**Summary:** [num] kernels, [num] device arrays, Strategy [A/B/C]. 
CUDA pattern: [describe]. OMP approach: [describe].
Expected: ~[X] MB H→D, ~[Y] MB D→H.
```

### 2.6. Implement Data Plan

**Use data_plan.md as implementation guide**

### Step 1: Remove CUDA API Calls
From "CUDA Memory Analysis":
- Remove all cudaMalloc/cudaFree calls
- Remove all cudaMemcpy calls
- Remove kernel launch syntax <<<grid, block>>>
- Keep all kernel BODY code (will convert to functions)

### Step 2: Convert Kernels to Functions
From "Kernel Inventory":
```
CUDA:
  __global__ void kernel_name(double *arr, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) arr[idx] = ...;
  }

OMP:
  void kernel_name(double *arr, int n) {
    #pragma omp target teams loop is_device_ptr(arr)
    for (int idx = 0; idx < n; idx++) {  
      arr[idx] = ...;
    }
  }
```

### Step 3: Setup Data Structures
From "OMP Data Movement Strategy":
- Create OMP allocations based on chosen strategy
- For Strategy C: Add omp_target_alloc calls
- For Strategy A: Setup target data regions

### Step 4: Implement Transfers
From "Host→Device" and "Device→Host" sections:
- Implement transfers using method for chosen strategy
- Match timing from original CUDA code

### Step 5: Convert Thread Indexing
From "Thread Indexing Conversion":
- Replace blockIdx/threadIdx with loop iterator
- Remove if (idx < N) guards (loop bounds handle this)
- Convert grid-stride loops to simple loops

### Step 6: Handle Special CUDA Constructs
From "Critical Migration Issues":
- **atomicAdd** → `#pragma omp atomic update`
- **__syncthreads()** → Split kernel OR remove if not critical
- **Shared memory** → Per-thread private OR elimination
- **Reduction in kernel** → `reduction(op:var)` clause

### Step 7: Verify Implementation
Check ALL items in "Critical Migration Issues":
- [ ] All kernels converted to OMP functions
- [ ] Thread indexing removed
- [ ] Memory management matches strategy
- [ ] Special constructs handled

**Common errors:** 
- Forgot to remove <<<>>> syntax
- Left blockIdx/threadIdx in code
- Missed cudaMemcpy conversions
- Wrong is_device_ptr usage

**CRITICAL: OpenMP Clause Syntax Limitation**
OpenMP pragma clauses (`is_device_ptr`, `use_device_addr`, `map`) do NOT support struct member access.
You MUST extract struct members to local pointer variables first.

WRONG (will not compile):
```c
#pragma omp target teams loop is_device_ptr(data.arr1, data.arr2)
```

CORRECT:
```c
double *d_arr1 = data.arr1;
double *d_arr2 = data.arr2;
#pragma omp target teams loop is_device_ptr(d_arr1, d_arr2)
for (int i = 0; i < n; i++) {
    // use d_arr1[i], d_arr2[i] inside the loop
}
```

When converting CUDA code that passes structs to kernels, extract ALL device pointer members
to local variables BEFORE the pragma, then use those local variables in the clause AND loop body.

**Ready when:** Compiles and runs with OMP flags, no CUDA API calls remain

---

## Strategy / Pattern Notes (short)
- Strategy A: `target data map(...)` for simpler flows (few kernels).
- Strategy C: `omp_target_alloc` + `omp_target_memcpy` + `is_device_ptr` for persistent device pointers (CUDA-like).
- Device helpers: former `__device__` helpers typically need `#pragma omp declare target`.

## 5. Compile and Test
```bash
make -f Makefile.nvc clean
make -f Makefile.nvc
timeout 300 make -f Makefile.nvc run > gpu_output.txt 2>&1
```

If timeout/segfault: Check for unconverted CUDA constructs.
If core dumped/Aborted: run compute sanitizer.

## 6. Verify Correctness
```bash
diff baseline_output.txt gpu_output.txt
```

## 8. Profile
```bash
make -f Makefile.nvc clean
env FORCE_OMP_GPU=1 OMP_TARGET_OFFLOAD=MANDATORY nsys profile --stats=true --trace=cuda,osrt --force-overwrite=true -o nsys_profile make -f Makefile.nvc run > /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/profile.log 2>&1
# Fallback: env OMP_TARGET_OFFLOAD=MANDATORY nsys profile --stats=true --trace=cuda,osrt --force-overwrite=true -o nsys_profile make -f Makefile.nvc run > /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/profile.log 2>&1
# Check for kernel information (OpenMP kernels may appear in cuda_gpu_kern_sum or with different names)
grep -E "cuda_gpu_kern|CUDA GPU Kernel|GPU activities" /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/profile.log | head -10 || echo "No kernel information found - check if code is offloading to GPU"
```

## RULES - BREAKING A RULE = FAILURE
- NO GIT COMMANDS.
- DO NOT READ/WRITE OUTSIDE THE WORKING DIRECTORY.
- DO NOT EDIT MAKEFILES.
- ALWAYS CLEAN BEFORE BUILD.
- YOU MAY MODIFY THE MAKEFILE TO ADD ANYTHING YOU NEED TO RUN THE CODE.
- REMOVE ALL CUDA API CALLS (cudaMalloc, cudaMemcpy, cudaFree, kernel<<<>>>)
- CONVERT ALL __global__ FUNCTIONS TO REGULAR FUNCTIONS
- REMOVE ALL CUDA-SPECIFIC SYNTAX (blockIdx, threadIdx, __syncthreads, __shared__)


you might want to leave comments in the code to explain your changes.

the code might contain macros like GATE_CHECKSUM_* or GATE_STATS_*, you should not change them.

you need to check what hardware you are running on in `system_info.txt` and use the information to optimize your code.


mcp startup: no servers

thinking
**Assessing task scope and conflicting instructions**
exec
/bin/bash -lc pwd in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 7ms:
/root/codex_baseline/cuda_omp_pareval_workdir

thinking
**Noting need for list**
exec
/bin/bash -lc ls in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 7ms:
AGENTS.md
data
gate_sdk
golden_labels
localbin
system_info.txt
tmp_rand
tmp_rand.cpp

thinking
**Requesting system information**
exec
/bin/bash -lc 'cat system_info.txt' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
=== SYSTEM BASELINE ===
Linux ErelPC 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
ldd (Ubuntu GLIBC 2.35-0ubuntu3.11) 2.35

=== FIRMWARE & MICROCODE ===
# dmidecode 3.3
Scanning /dev/mem for entry point.
# No SMBIOS nor DMI entry point found, sorry.

thinking
**Inspecting XSBench-omp directory**
exec
/bin/bash -lc 'ls data/src' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
XSBench-omp
microXOR-omp
microXORh-omp
nano-XOR-omp

thinking
**Navigating to target directory**
exec
/bin/bash -lc ls in /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp succeeded in 8ms:
GridInit.cpp
Main.cpp
Makefile.nvc
Materials.cpp
Simulation.cpp
XSbench_header.cuh
XSbench_shared_header.h
XSutils.cpp
analysis.md
baseline_output.txt
io.cpp

thinking
**Checking file discrepancies**
exec
/bin/bash -lc "sed -n '1,200p' data/src/XSBench-omp/Simulation.cpp" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
#include "XSbench_header.cuh"

unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile)
{
	double start = get_time();
        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
	profile->host_to_device_time = get_time() - start;

        if( mype == 0)	printf("Running baseline event-based simulation...\n");

        int nthreads = 256;
        int nblocks = ceil( (double) in.lookups / (double) nthreads);

	int nwarmups = in.num_warmups;
	start = 0.0;
	for (int i = 0; i < in.num_iterations + nwarmups; i++) {
		if (i == nwarmups) {
			gpuErrchk( cudaDeviceSynchronize() );
			start = get_time();
		}

thinking
**Waiting for more content**
exec
/bin/bash -lc "sed -n '200,400p' data/src/XSBench-omp/Simulation.cpp" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
                examinationPoint = lowerLimit + ( length / 2 );

                if( A[examinationPoint] > quarry )
                        upperLimit = examinationPoint;
                else
                        lowerLimit = examinationPoint;

                length = upperLimit - lowerLimit;
        }

        return lowerLimit;
}

__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high)
{
        long lowerLimit = low;
        long upperLimit = high;
        long examinationPoint;
        long length = upperLimit - lowerLimit;


thinking
**Needing more content to continue**
exec
/bin/bash -lc "sed -n '400,800p' data/src/XSBench-omp/Simulation.cpp" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
                        max = macro_xs_vector[j];
                        max_idx = j;
                }
        }
        GSD.verification[i] = max_idx+1;
}

unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype)
{
        const char * optimization_name = "Optimization 2 - Material Lookup Kernels";

        if( mype == 0)	printf("Simulation Kernel:\"%s\"\n", optimization_name);

        if( mype == 0)	printf("Allocating additional device data required by kernel...\n");
        size_t sz;
        size_t total_sz = 0;

        sz = in.lookups * sizeof(double);
        gpuErrchk( cudaMalloc((void **) &GSD.p_energy_samples, sz) );
        total_sz += sz;

thinking
**Waiting for more details**
exec
/bin/bash -lc "sed -n '800,1200p' data/src/XSBench-omp/Simulation.cpp" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 9ms:
        size_t total_sz = 0;

        sz = in.lookups * sizeof(double);
        gpuErrchk( cudaMalloc((void **) &GSD.p_energy_samples, sz) );
        total_sz += sz;
        GSD.length_p_energy_samples = in.lookups;

        sz = in.lookups * sizeof(int);
        gpuErrchk( cudaMalloc((void **) &GSD.mat_samples, sz) );
        total_sz += sz;
        GSD.length_mat_samples = in.lookups;

        if( mype == 0)	printf("Allocated an additional %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0);

        if( mype == 0)	printf("Beginning optimized simulation...\n");

        int nthreads = 32;
        int nblocks = ceil( (double) in.lookups / 32.0);

        sampling_kernel<<<nblocks, nthreads>>>( in, GSD );

thinking
**Checking analysis.md for planning instructions**
exec
/bin/bash -lc 'cat data/src/XSBench-omp/analysis.md' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
# XSBench-omp CUDA Loop Analysis

## File Conversion Mapping
```
Main.cu   → Main.cpp
Simulation.cu   → Simulation.cpp
io.cu     → io.cpp
GridInit.cu   → GridInit.cpp
XSutils.cu   → XSutils.cpp
Materials.cu   → Materials.cpp
```
(Headers `XSbench_header.cuh` and `XSbench_shared_header.h` are copied verbatim so the new sources can include them without renaming.)

## Kernel/Loop Nesting Structure
- `run_event_based_simulation_baseline` (Simulation.cu:3)
  └── host loop `for (int i = 0; i < in.num_iterations + nwarmups; i++)` (Simulation.cu:16) that launches `xs_lookup_kernel_baseline<<<nblocks, nthreads>>>` (Simulation.cu:41) on every iteration → Type A
- `run_event_based_simulation_optimization_1` (Simulation.cu:304)
  ├── `sampling_kernel<<<nblocks, nthreads>>>` (Simulation.cu:348) to generate samples and materials → Type A (per-lookup)
  └── `xs_lookup_kernel_optimization_1<<<nblocks, nthreads>>>` (Simulation.cu:367) to compute macros per sample → Type A
- `run_event_based_simulation_optimization_2` (Simulation.cu:407)

thinking
**Preparing data plan and backup strategy**

I’m sorting out the plan before implementation, deciding to back up Simulation.cpp since Simulation.cu seems replaced. I’m setting up steps to analyze code and data, convert to OpenMP, and possibly compile/test while respecting the “do not edit makefiles” rule. Next, I’ll inspect related headers to understand data structures.
exec
/bin/bash -lc "sed -n '1,200p' data/src/XSBench-omp/XSbench_header.cuh" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 9ms:
#ifndef __XSBENCH_HEADER_H__
#define __XSBENCH_HEADER_H__

#include<stdio.h>
#include<stdlib.h>
#include<math.h>
#include<assert.h>
#include<cuda.h>
#include <thrust/reduce.h>
#include <thrust/partition.h>
#include<stdint.h>
#include <chrono>
#include "XSbench_shared_header.h"

// Grid types
#define UNIONIZED 0
#define NUCLIDE 1
#define HASH 2

// Simulation types

thinking
**Evaluating CUDA dependencies**
exec
/bin/bash -lc "sed -n '1,200p' data/src/XSBench-omp/GridInit.cpp" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 12ms:
#include "XSbench_header.cuh"

// Moves all required data structures to the GPU's memory space
SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
{
        if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");

        ////////////////////////////////////////////////////////////////////////////////
        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
        // Here we list all heap arrays (and lengths) in SD that would need to be
        // offloaded manually if using an accelerator with a seperate memory space
        ////////////////////////////////////////////////////////////////////////////////
        // int * num_nucs;                     // Length = length_num_nucs;
        // double * concs;                     // Length = length_concs
        // int * mats;                         // Length = length_mats
        // double * unionized_energy_array;    // Length = length_unionized_energy_array
        // int * index_grid;                   // Length = length_index_grid
        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
        //
        // Note: "unionized_energy_array" and "index_grid" can be of zero length

thinking
**Continuing with data plan preparation**
exec
/bin/bash -lc "sed -n '200,400p' data/src/XSBench-omp/GridInit.cpp" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 12ms:
                nbytes += SD.length_index_grid * sizeof(int);

                // Generates the double indexing grid
                int * idx_low = (int *) calloc( in.n_isotopes, sizeof(int));
                assert(idx_low != NULL );
                double * energy_high = (double *) malloc( in.n_isotopes * sizeof(double));
                assert(energy_high != NULL );

                for( int i = 0; i < in.n_isotopes; i++ )
                        energy_high[i] = SD.nuclide_grid[i * in.n_gridpoints + 1].energy;

                for( long e = 0; e < SD.length_unionized_energy_array; e++ )
                {
                        double unionized_energy = SD.unionized_energy_array[e];
                        for( long i = 0; i < in.n_isotopes; i++ )
                        {
                                if( unionized_energy < energy_high[i]  )
                                        SD.index_grid[e * in.n_isotopes + i] = idx_low[i];
                                        else if( idx_low[i] == in.n_gridpoints - 2 )
                                        SD.index_grid[e * in.n_isotopes + i] = idx_low[i];

thinking
**Assessing CUDA to OMP conversion requirements**
exec
/bin/bash -lc "sed -n '1,200p' data/src/XSBench-omp/Main.cpp" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
#include "XSbench_header.cuh"
#include "gate.h"

int main(int argc, char *argv[]) {
        // =====================================================================
        // Initialization & Command Line Read-In
        // =====================================================================
        int version = 20;
        int mype = 0;
        double omp_start, omp_end;
        int nprocs = 1;
        unsigned long long verification;

        // Process CLI Fields -- store in "Inputs" structure
        Inputs in = read_CLI(argc, argv);

        // Print-out of Input Summary
        if (mype == 0)
                print_inputs(in, nprocs, version);


thinking
**Analyzing data strategy and plan creation**

I’m figuring out the right data management approach for converting CUDA-based simulation code to OpenMP, carefully reviewing loop types to select an offloading strategy per the rules. I’m preparing to document a detailed data plan before removing CUDA references and rewriting kernels to use OpenMP target directives, making sure to capture kernel classification, memory usage, and conversion steps in the plan.
Plan update
  → Analyze existing CUDA Simulation files/headers to document data allocations and kernel flow, then create data_plan.md as required
  • Convert Simulation logic (kernels, run loops, helper functions) to OpenMP offload equivalents, removing CUDA APIs and thurst references
  • Validate conversion (ensure include adjustments, compile/test if feasible, note any outstanding issues)

thinking
**Analyzing CUDA usage for data plan**
exec
/bin/bash -lc "sed -n '1,200p' data/src/XSBench-omp/Materials.cpp" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 9ms:
// Material data is hard coded into the functions in this file.
// Note that there are 12 materials present in H-M (large or small)

#include "XSbench_header.cuh"

// num_nucs represents the number of nuclides that each material contains
int * load_num_nucs(long n_isotopes)
{
        int * num_nucs = (int*)malloc(12*sizeof(int));

        // Material 0 is a special case (fuel). The H-M small reactor uses
        // 34 nuclides, while H-M larges uses 300.
        if( n_isotopes == 68 )
                num_nucs[0]  = 34; // HM Small is 34, H-M Large is 321
        else
                num_nucs[0]  = 321; // HM Small is 34, H-M Large is 321

        num_nucs[1]  = 5;
        num_nucs[2]  = 4;
        num_nucs[3]  = 4;

thinking
**Preparing to remove CUDA dependencies**
exec
/bin/bash -lc 'rg -n "thrust" -n data/src/XSBench-omp' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 37ms:
data/src/XSBench-omp/Simulation.cpp:341:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp:445:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp:534:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp:619:                n_lookups_per_material[m] = thrust::count(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, m);
data/src/XSBench-omp/Simulation.cpp:621:        thrust::sort_by_key(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, GSD.p_energy_samples);
data/src/XSBench-omp/Simulation.cpp:636:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp:728:        int n_fuel_lookups = thrust::count(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp:730:        thrust::partition(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, GSD.p_energy_samples, is_mat_fuel());
data/src/XSBench-omp/Simulation.cpp:743:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp:825:                n_lookups_per_material[m] = thrust::count(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, m);
data/src/XSBench-omp/Simulation.cpp:827:        thrust::sort_by_key(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, GSD.p_energy_samples);
data/src/XSBench-omp/Simulation.cpp:832:                thrust::sort_by_key(thrust::device, GSD.p_energy_samples + offset, GSD.p_energy_samples + offset + n_lookups_per_material[m], GSD.mat_samples + offset);
data/src/XSBench-omp/Simulation.cpp:849:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/XSbench_header.cuh:9:#include <thrust/reduce.h>
data/src/XSBench-omp/XSbench_header.cuh:10:#include <thrust/partition.h>
data/src/XSBench-omp/analysis.md:24:  ├── `thrust::count` (Simulation.cu:617)
data/src/XSBench-omp/analysis.md:25:  ├── `thrust::sort_by_key` (Simulation.cu:621)
data/src/XSBench-omp/analysis.md:28:  ├── `thrust::count` and `thrust::partition` (Simulation.cu:728-731)
data/src/XSBench-omp/analysis.md:31:  ├── `thrust::count` + `thrust::sort_by_key` + per-range `thrust::sort_by_key` (Simulation.cu:823-834)
data/src/XSBench-omp/analysis.md:98:- **Dependencies:** per-material counts (`n_lookups_per_material`) plus sorted arrays from `thrust::sort`

thinking
**Checking for std equivalents in thrust operations**
exec
/bin/bash -lc "sed -n '1,200p' data/src/XSBench-omp/XSbench_shared_header.h" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 11ms:
#ifndef XSBENCH_SHARED_HEADER_H
#define XSBENCH_SHARED_HEADER_H

// Header for shared utilities across XSBench versions

typedef struct{
        int nthreads;
        long n_isotopes;
        long n_gridpoints;
        int lookups;
        char * HM;
        int grid_type; // 0: Unionized Grid (default)    1: Nuclide Grid
        int hash_bins;
        int particles;
        int simulation_method;
        int binary_mode;
        int kernel_id;
        int num_iterations;
        int num_warmups;
        char *filename;

thinking
**Investigating CUDA usage in project**
exec
/bin/bash -lc 'rg -n "cuda" -n data/src/XSBench-omp' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 16ms:
data/src/XSBench-omp/Simulation.cpp:18:			gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp:23:	gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp:24:	gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp:29:        gpuErrchk(cudaMemcpy(SD.verification, GSD.verification, in.lookups * sizeof(unsigned long), cudaMemcpyDeviceToHost) );
data/src/XSBench-omp/Simulation.cpp:315:        gpuErrchk( cudaMalloc((void **) &GSD.p_energy_samples, sz) );
data/src/XSBench-omp/Simulation.cpp:320:        gpuErrchk( cudaMalloc((void **) &GSD.mat_samples, sz) );
data/src/XSBench-omp/Simulation.cpp:332:        gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp:333:        gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp:336:        gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp:337:        gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp:342:        gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp:343:        gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp:418:        gpuErrchk( cudaMalloc((void **) &GSD.p_energy_samples, sz) );
data/src/XSBench-omp/Simulation.cpp:423:        gpuErrchk( cudaMalloc((void **) &GSD.mat_samples, sz) );
data/src/XSBench-omp/Simulation.cpp:435:        gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp:436:        gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp:440:        gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp:441:        gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp:446:        gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp:447:        gpuErrchk( cudaDeviceSynchronize() );

thinking
**Replacing CUDA calls with OpenMP**
exec
/bin/bash -lc "sed -n '1,200p' data/src/XSBench-omp/io.cpp" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 12ms:
#include "XSbench_header.cuh"

// Prints program logo
void logo(int version)
{
	border_print();
	printf(
	"                   __   __ ___________                 _                        \n"
	"                   \\ \\ / //  ___| ___ \\               | |                       \n"
	"                    \\ V / \\ `--.| |_/ / ___ _ __   ___| |__                     \n"
	"                    /   \\  `--. \\ ___ \\/ _ \\ '_ \\ / __| '_ \\                    \n"
	"                   / /^\\ \\/\\__/ / |_/ /  __/ | | | (__| | | |                   \n"
	"                   \\/   \\/\\____/\\____/ \\___|_| |_|\\___|_| |_|                   \n\n"
	       );
	border_print();
	center_print("Developed at Argonne National Laboratory", 79);
	char v[100];
	sprintf(v, "Version: %d", version);
	center_print(v, 79);
	border_print();

thinking
**Preparing detailed data management plan**

I'm putting together a comprehensive data management plan to guide the migration from CUDA to OpenMP Offload. This involves removing CUDA and Thrust dependencies, mapping device allocations to OpenMP equivalents, describing kernel launches and memory transfers, and choosing Strategy C based on kernel patterns and iteration structure. I'll include array inventories, kernel configs, transfer details, critical migration issues, and expected performance notes—all formatted in a markdown file as instructed.
exec
/bin/bash -lc "cat <<'EOF' > data/src/XSBench-omp/data_plan.md
# Data Management Plan

## CUDA Memory Analysis
List ALL device allocations and transfers:

| Array/Pointer | CUDA Allocation | Size | Transfer Pattern |
|---------------|-----------------|------|------------------|
| "'`GSD.num_nucs` | `cudaMalloc` | `SD.length_num_nucs * sizeof(int)` | `cudaMemcpyHostToDevice` once during initialization |
| `GSD.concs` | `cudaMalloc` | `SD.length_concs * sizeof(double)` | `cudaMemcpyHostToDevice` once |
| `GSD.mats` | `cudaMalloc` | `SD.length_mats * sizeof(int)` | `cudaMemcpyHostToDevice` once |
| `GSD.unionized_energy_array` | `cudaMalloc` (conditional on `in.grid_type`) | `SD.length_unionized_energy_array * sizeof(double)` | `cudaMemcpyHostToDevice` once when configured |
| `GSD.index_grid` | `cudaMalloc` (conditional) | `SD.length_index_grid * sizeof(int)` | `cudaMemcpyHostToDevice` once when configured |
| `GSD.nuclide_grid` | `cudaMalloc` | `SD.length_nuclide_grid * sizeof(NuclideGridPoint)` | `cudaMemcpyHostToDevice` once |
| `GSD.verification` | `cudaMalloc` | `in.lookups * sizeof(unsigned long)` | Allocated on device and copied back once after simulation |
| `GSD.p_energy_samples` | `cudaMalloc` (per optimized run) | `in.lookups * sizeof(double)` | Populated by sampling kernel on device (no explicit memcpy) |
| `GSD.mat_samples` | `cudaMalloc` (per optimized run) | `in.lookups * sizeof(int)` | Populated by sampling kernel on device |

**CUDA Operations:**
- cudaMalloc calls: move_simulation_data_to_device includes 7 allocations (num_nucs, concs, mats, unionized_energy_array, index_grid, nuclide_grid, verification); each optimization run allocates `p_energy_samples` and `mat_samples` via cudaMalloc.
- cudaMemcpy H→D: Each of the persistent geometry/material arrays is copied once in move_simulation_data_to_device; `GSD.verification` is populated on device so no copy until the end.
- cudaMemcpy D→H: the verification buffer is copied back to host once after the timed kernel(s).
- Kernel launches: Baseline kernel (`xs_lookup_kernel_baseline`) runs once per iteration; optimized flows launch 2+ kernels per iteration (sampling + lookup, material-specific kernels, etc.) plus host loops wrapping them.

## Kernel Inventory
| Kernel Name | Launch Config | Frequency | Arrays Used |
|-------------|---------------|-----------|-------------|
| `xs_lookup_kernel_baseline` | `ceil(lookups/256)` grid × 256 threads | per simulation iteration (num_iterations + num_warmups) | Reads: num_nucs, concs, unionized_energy_array, index_grid, nuclide_grid, mats. Writes: verification |
| `sampling_kernel` | `ceil(lookups/32)` grid × 32 threads | once per optimization run before lookups | Writes: `GSD.p_energy_samples`, `GSD.mat_samples` |
| `xs_lookup_kernel_optimization_1` | `ceil(lookups/32)` grid × 32 threads | once per optimization run | Reads: sample arrays + same data as baseline |
| `xs_lookup_kernel_optimization_2` | `ceil(lookups/32)` grid × 32 threads, launched once per material via host loop | 12 times per run | Reads: sample arrays, material-specific data |
| `xs_lookup_kernel_optimization_3` | same as above but 2 launches (fuel / non-fuel) |
| `xs_lookup_kernel_optimization_4` | `ceil(n_lookups_per_material/32)` grid × 32 threads per material | up to 12 launches with offsets | Reads: sorted sample arrays |
| `xs_lookup_kernel_optimization_5` | Two launches for fuel and non-fuel buckets | 2 kernels (per run) |
| `xs_lookup_kernel_optimization_6` | Same as 4 but with extra sorting | per material kernel after sorting |

**Kernel Launch Patterns:**
- Most kernels execute inside the timed region, and the baseline/optimized drivers sequentially coordinate `sampling` + one or more lookup kernels.
- Material partition loops (optimizations 2/4/5/6) are host-side loops that rely on partitioned ranges computed before each kernel launch.
- There are no `__syncthreads()` or `__shared__` usages that would constrain mapping.

## OMP Data Movement Strategy
**Chosen Strategy:** C (persistent device pointers via `omp_target_alloc` / `omp_target_memcpy` + `is_device_ptr`).

**Rationale:** The workload keeps large datasets resident across multiple kernels with complex launch sequences; Strategy C mirrors the CUDA memory model with explicit allocations and fine-grained control of when data is mapped. This also simplifies translating the `SimulationData` structure into device-resident pointers while keeping the host driver logic intact.

**Device Allocations (OMP equivalent):**
```
CUDA: cudaMalloc(&GSD.num_nucs, ...) --> OMP: #pragma omp target enter data map(to: GSD.num_nucs[:length]) or omp_target_memcpy / omp_target_alloc
```
We'"'ll prefer "'`#pragma omp target enter data` inside helper routines that mimic move_simulation_data_to_device, followed by `#pragma omp target exit data` in release.

**Host→Device Transfers (OMP equivalent):**
```
CUDA: cudaMemcpy(GSD.num_nucs, SD.num_nucs, ..., HostToDevice)
OMP: #pragma omp target enter data map(to: GSD.num_nucs[:length])
```
- When: Immediately after the CPU-side data structures are initialized and before timing begins (exactly where move_simulation_data_to_device currently runs).
- Arrays: all `SimulationData` host arrays that were memcpy'"'d to device ("'`num_nucs`, `concs`, `mats`, `unionized_energy_array`, `index_grid`, `nuclide_grid`, `verification`, plus temporary sample arrays if we keep them offloaded).
- Total H→D: roughly `(length_num_nucs*sizeof(int) + length_concs*sizeof(double) + length_mats*sizeof(int) + length_unionized_energy_array*sizeof(double) + length_index_grid*sizeof(int) + length_nuclide_grid*sizeof(NuclideGridPoint) + lookups*sizeof(unsigned long))` which is similar to CUDA version (~tens of MB depending on inputs).

**Device→Host Transfers (OMP equivalent):**
```
CUDA: cudaMemcpy(SD.verification, GSD.verification, lookups * sizeof(unsigned long), DeviceToHost)
OMP: #pragma omp target update from(GSD.verification[:in.lookups])
```
- When: once after the timed simulation loops finish.
- Arrays: `verification`. No intermediate transfers occur during loop iterations.
- Total D→H: `lookups * sizeof(unsigned long)` (~a few MB).

**Transfers During Iterations:** NO – all offload transfers happen before and after the timed loops; sampling kernels operate on device data directly.

## Kernel to OMP Mapping (short)
- Each CUDA kernel becomes a regular function whose body is wrapped in `#pragma omp target teams loop is_device_ptr(...)` covering the same one-dimensional index space (`lookups`, `n_lookups`, etc.).
- Device helper routines (`calculate_macro_xs`, `calculate_micro_xs`, `grid_search`, `grid_search_nuclide`, RNG helpers) will be marked with `#pragma omp declare target` (if needed) or simply executed inside target regions when called.
- Block-/thread-based indexing (`blockIdx`, `threadIdx`) is replaced with a single `for (int i = 0; i < N; ++i)` inside the `target teams loop`, removing explicit bounds checks since the loops already respect `N`.
- All sequential helper loops remain unchanged but will be executed on the device within the offloaded loop.

## Critical Migration Issues (from analysis.md "OMP Migration Issues")
- `__syncthreads()` is not present, so no special barrier handling is required.
- Shared memory is not used; all per-thread data sits on the stack or in `SimulationData`, so equivalence is to rely on default OpenMP private storage.
- Atomics: none. All writes to `GSD.verification` are distinct indices.
- Dynamic indexing (material lookups with `num_nucs[mat]` varying per thread): preserve as sequential loops but mark the outer `parallel for` with `schedule(dynamic, 64)` or similar to balance work.
- `thrust::reduce`, `thrust::count`, `thrust::sort_by_key`, `thrust::partition` will be replaced with `std::accumulate`, `std::count`, `std::sort`, and `std::stable_partition` (or manual counting) executed on the host arrays before the target loops.

**__syncthreads() Resolution:**
- Not present.

**Shared memory / barriers:**
- Not used.

## Expected Performance
- CUDA kernel time: Unknown (baseline_output only records nvcc commands), but the kernel is dense per-lookup. Expect OMP offload runtimes to be similar in magnitude once `OMP_TARGET_OFFLOAD=MANDATORY` is enabled, though initial data movement via `target enter data` adds overhead.
- OMP expected: comparable or moderately slower due to extra host-side sorts and absence of Thrust parallel reductions; we can mitigate by using optimized `std::sort` and `std::reduce`.
- Red flag: If runtime is >3× longer than CUDA baseline, reconsider the dynamic scheduling strategy or partition approach.

**Summary:**
- Kernels: 6+ kernels (baseline + 5 optimizations), all Type A (dense per-lookup) with C1 orchestration loops for material-specific stages.
- Device arrays: `num_nucs`, `concs`, `mats`, `unionized_energy_array`, `index_grid`, `nuclide_grid`, `verification`, `p_energy_samples`, `mat_samples`.
- Strategy: C (persistent offload with explicit `target enter data` / `is_device_ptr`).
- CUDA pattern: Host initializes data, copy to device, run many kernels with `cudaDeviceSynchronize`, reduce verification using Thrust.
- OMP approach: Maintain host arrays, `#pragma omp target enter data` for all buffers, wrap each kernel body in `#pragma omp target teams loop is_device_ptr(...)`, replace `cudaMemcpy` with `target update`, remove Thrust in favor of STL algorithms, and keep verification reduction on host via `std::accumulate`.
- Expected transfers: ~same as CUDA (~H→D once per array, D→H once for verification), no transfers during iteration.
EOF' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 11ms:


thinking
**Verifying data_plan file creation**
exec
/bin/bash -lc 'cat data/src/XSBench-omp/data_plan.md' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
# Data Management Plan

## CUDA Memory Analysis
List ALL device allocations and transfers:

| Array/Pointer | CUDA Allocation | Size | Transfer Pattern |
|---------------|-----------------|------|------------------|
| `GSD.num_nucs` | `cudaMalloc` | `SD.length_num_nucs * sizeof(int)` | `cudaMemcpyHostToDevice` once during initialization |
| `GSD.concs` | `cudaMalloc` | `SD.length_concs * sizeof(double)` | `cudaMemcpyHostToDevice` once |
| `GSD.mats` | `cudaMalloc` | `SD.length_mats * sizeof(int)` | `cudaMemcpyHostToDevice` once |
| `GSD.unionized_energy_array` | `cudaMalloc` (conditional on `in.grid_type`) | `SD.length_unionized_energy_array * sizeof(double)` | `cudaMemcpyHostToDevice` once when configured |
| `GSD.index_grid` | `cudaMalloc` (conditional) | `SD.length_index_grid * sizeof(int)` | `cudaMemcpyHostToDevice` once when configured |
| `GSD.nuclide_grid` | `cudaMalloc` | `SD.length_nuclide_grid * sizeof(NuclideGridPoint)` | `cudaMemcpyHostToDevice` once |
| `GSD.verification` | `cudaMalloc` | `in.lookups * sizeof(unsigned long)` | Allocated on device and copied back once after simulation |
| `GSD.p_energy_samples` | `cudaMalloc` (per optimized run) | `in.lookups * sizeof(double)` | Populated by sampling kernel on device (no explicit memcpy) |
| `GSD.mat_samples` | `cudaMalloc` (per optimized run) | `in.lookups * sizeof(int)` | Populated by sampling kernel on device |

**CUDA Operations:**
- cudaMalloc calls: move_simulation_data_to_device includes 7 allocations (num_nucs, concs, mats, unionized_energy_array, index_grid, nuclide_grid, verification); each optimization run allocates `p_energy_samples` and `mat_samples` via cudaMalloc.
- cudaMemcpy H→D: Each of the persistent geometry/material arrays is copied once in move_simulation_data_to_device; `GSD.verification` is populated on device so no copy until the end.

thinking
**Updating plan status**
Plan update
  ✓ Analyze existing CUDA Simulation files/headers to document data allocations and kernel flow, then create data_plan.md as required
  → Convert Simulation logic (kernels, run loops, helper functions) to OpenMP offload equivalents, removing CUDA APIs and thurst references
  • Validate conversion (ensure include adjustments, compile/test if feasible, note any outstanding issues)

thinking
**Backing up simulation source**
exec
/bin/bash -lc 'cp data/src/XSBench-omp/Simulation.cpp data/src/XSBench-omp/Simulation.cpp.bak' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 9ms:




thinking
**Outlining CUDA-to-OpenMP refactor plan**

I'm mapping out how to strip CUDA from Simulation.cpp and headers, swapping in OpenMP for device memory management and kernels. This involves removing CUDA includes and macros, refactoring memory allocation with `omp_target_alloc` and memcpy, shifting from Thrust to STL algorithms on host, and carefully managing host-device data copies for sample arrays. I'll also convert device functions using OpenMP directives and ensure proper includes like `<omp.h>` are added. This plan balances maintaining device offload for compute kernels with host-side sorting and data preparation.
thinking
**Detailing OpenMP kernel and memory handling changes**

I’m working through converting the run_event functions to OpenMP offload, rewriting device memory setup with `omp_target_alloc` and freeing with `omp_target_free`. The plan includes replacing CUDA kernels with `#pragma omp target teams loop` without `distribute parallel for`. I’ll manage device-host data transfers with `target update` pragmas, convert thrust reductions, sorts, and counts to host STL algorithms on vectors copied back from device, and build helper functions for device memory management to keep code clean and consistent across Simulation.cpp and GridInit.cpp.
thinking
**Planning shared device utility functions**

I’m considering how to best implement device memory helper functions without adding new files—leaning toward declaring prototypes in the existing header and defining them in GridInit.cpp for shared use. This keeps the code organized while matching build constraints. I’m also thinking through includes and linkage to avoid multiple definitions or build issues across Simulation.cpp and GridInit.cpp.
exec
/bin/bash -lc "sed -n '1,200p' data/src/XSBench-omp/Makefile.nvc" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
#===============================================================================
# User Options
#===============================================================================

CC        = nvc++
OPTIMIZE  = yes
DEBUG     = no
DEVICE    = gpu
SM        = 89
LAUNCHER  =
GATE_ROOT ?= /root/codex_baseline/cuda_omp_pareval_workdir
REF_DIR   ?= $(GATE_ROOT)/data/src/XSBench-omp
REF_BIN   ?= $(REF_DIR)/XSBench
REF_MAKE  ?= Makefile.nvc

RUN_ARGS ?= -m event -s small -l 100000

#===============================================================================
# Program name & source code list
#===============================================================================

thinking
**Analyzing CUDA to OpenMP migration constraints**
exec
/bin/bash -lc 'rg -n "cuda" data/src/XSBench-omp/XSutils.cpp' in /root/codex_baseline/cuda_omp_pareval_workdir exited 1 in 14ms:



thinking
**Preparing OpenMP migration plan**

I’m outlining an incremental refactor to replace CUDA/Thrust with OpenMP target offloading. This involves removing CUDA includes and macros, updating function prototypes to remove CUDA qualifiers, and restructuring device memory management using `omp_target_alloc` and related functions. I’m planning to shift kernel launches and helper functions to OpenMP target directives, ensuring synchronization and preserving profiling times. The plan also covers rewriting simulation run functions and handling device memory allocation and copying with OpenMP routines, aiming for a smooth migration with thorough header and source updates.
thinking
**Detailing simulation sorting and kernel calls**

I’m outlining how to replace CUDA thrust sorting and counting with host-side STL operations using `std::vector`, `std::sort`, `std::count`, and `std::stable_partition`. This involves copying device arrays to host vectors, sorting or partitioning them as needed for various optimization kernels, then copying sorted arrays back to device before launching per-material OpenMP target kernels. The plan includes strategies for computing per-material lookup counts, handling offsets, and ensuring proper updates to the verification data. I’m also considering whether to rewrite `Simulation.cpp` entirely due to the breadth of changes, aiming to maintain all kernel structures while removing CUDA dependencies.
file update
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -7,5 +7,2 @@
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
@@ -31,12 +28,2 @@
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
@@ -87,4 +74,4 @@
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
@@ -93,3 +80,3 @@
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
@@ -100,23 +87,23 @@
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 


thinking
**Planning CUDA to OpenMP target conversion**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

file update
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,2 +1,3 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
apply_patch(auto_approved=true) exited 0 in 5ms:
Success. Updated the following files:
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..01f2db07f1e073d21e33c77c0e6b518731166596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 


thinking
**Planning OpenMP device memory helpers**

I’m working through rewriting `move_simulation_data_to_device` to use OpenMP for device memory management, including templated helper functions for safe allocation, copying, and freeing device buffers, plus integrating them cleanly into the patched function to replace the existing CUDA-based implementation.
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..01f2db07f1e073d21e33c77c0e6b518731166596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

file update
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -4,2 +4,41 @@
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
@@ -8,71 +47,21 @@
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
-
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
@@ -81,3 +70,2 @@
         return GSD;
-
 }
@@ -86,8 +74,11 @@
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
apply_patch(auto_approved=true) exited 0 in 10ms:
Success. Updated the following files:
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 



thinking
**Preparing to rewrite Simulation.cpp with careful helper setup**

I’m gathering all the necessary includes, helper functions, and memory management utilities needed to carefully rewrite the entire Simulation.cpp file. Since the original file is large and complex, I’ll define inline helpers for device allocation, copying, and frees, consider pragma directives for OpenMP target functions, and plan host-side sorting and partitioning of sample data with zipped vectors to maintain consistent order. The goal is to recreate all original behaviors cleanly while adapting to pure OpenMP target usage and eliminating CUDA dependencies.
thinking
**Drafting comprehensive Simulation.cpp rewrite plan**

I’m organizing the full rewrite strategy for Simulation.cpp, covering host-device memory copies, sample buffer management, sorting algorithms using zipped vectors, OpenMP target helper functions with proper pragmas, and detailed run function implementations for baseline and optimizations 1 through 6. I’m ensuring consistent includes, helper declarations, and kernel launches to maintain original logic while adapting fully to the new OpenMP offload approach.
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' > data/src/XSBench-omp/Simulation.cpp
#include \"XSbench_header.cuh\"

#include <algorithm>
#include <numeric>
#include <omp.h>
#include <utility>
#include <vector>

namespace {
int target_device()
{
    return omp_get_default_device();
}

int host_device()
{
    return omp_get_initial_device();
}

void copy_from_device(void *host_ptr, const void *device_ptr, size_t bytes)
{
    omp_target_memcpy(host_ptr, device_ptr, bytes, 0, 0, host_device(), target_device());
}

void copy_to_device(void *device_ptr, const void *host_ptr, size_t bytes)
{
    omp_target_memcpy(device_ptr, host_ptr, bytes, 0, 0, target_device(), host_device());
}

void allocate_sample_buffers(Inputs in, SimulationData &GSD)
{
    size_t energy_bytes = in.lookups * sizeof(double);
    size_t mat_bytes = in.lookups * sizeof(int);
    GSD.p_energy_samples = static_cast<double *>(omp_target_alloc(energy_bytes, target_device()));
    assert(GSD.p_energy_samples "'!= nullptr);
    GSD.length_p_energy_samples = in.lookups;
    GSD.mat_samples = static_cast<int *>(omp_target_alloc(mat_bytes, target_device()));
    assert(GSD.mat_samples != nullptr);
    GSD.length_mat_samples = in.lookups;
}

void free_sample_buffers(SimulationData &GSD)
{
    if (GSD.p_energy_samples)
        omp_target_free(GSD.p_energy_samples, target_device());
    if (GSD.mat_samples)
        omp_target_free(GSD.mat_samples, target_device());
    GSD.p_energy_samples = nullptr;
    GSD.mat_samples = nullptr;
}

void copy_samples_to_host(SimulationData &GSD, std::vector<int> &host_mats, std::vector<double> &host_energies)
{
    size_t mat_bytes = host_mats.size() * sizeof(int);
    size_t energy_bytes = host_energies.size() * sizeof(double);
    copy_from_device(host_mats.data(), GSD.mat_samples, mat_bytes);
    copy_from_device(host_energies.data(), GSD.p_energy_samples, energy_bytes);
}

void copy_samples_to_device(SimulationData &GSD, const std::vector<int> &host_mats, const std::vector<double> &host_energies)
{
    size_t mat_bytes = host_mats.size() * sizeof(int);
    size_t energy_bytes = host_energies.size() * sizeof(double);
    copy_to_device(GSD.mat_samples, host_mats.data(), mat_bytes);
    copy_to_device(GSD.p_energy_samples, host_energies.data(), energy_bytes);
}

void sort_samples_by_material(std::vector<int> &host_mats, std::vector<double> &host_energies)
{
    size_t count = host_mats.size();
    std::vector<std::pair<int, double>> zipped(count);
    for (size_t i = 0; i < count; ++i)
        zipped[i] = {host_mats[i], host_energies[i]};
    std::sort(zipped.begin(), zipped.end(), [](const auto &a, const auto &b) {
        return a.first < b.first;
    });
    for (size_t i = 0; i < count; ++i) {
        host_mats[i] = zipped[i].first;
        host_energies[i] = zipped[i].second;
    }
}

void sort_samples_by_material_then_energy(std::vector<int> &host_mats, std::vector<double> &host_energies)
{
    size_t count = host_mats.size();
    std::vector<std::pair<int, double>> zipped(count);
    for (size_t i = 0; i < count; ++i)
        zipped[i] = {host_mats[i], host_energies[i]};
    std::sort(zipped.begin(), zipped.end(), [](const auto &a, const auto &b) {
        if (a.first != b.first)
            return a.first < b.first;
        return a.second < b.second;
    });
    for (size_t i = 0; i < count; ++i) {
        host_mats[i] = zipped[i].first;
        host_energies[i] = zipped[i].second;
    }
}

unsigned long reduce_verification(SimulationData &SD, int lookups)
{
    return std::accumulate(SD.verification, SD.verification + lookups, 0ull);
}
} // namespace

#pragma omp declare target
long grid_search( long n, double quarry, double * __restrict__ A)
{
        long lowerLimit = 0;
        long upperLimit = n-1;
        long examinationPoint;
        long length = upperLimit - lowerLimit;

        while( length > 1 )
        {
                examinationPoint = lowerLimit + ( length / 2 );

                if( A[examinationPoint] > quarry )
                        upperLimit = examinationPoint;
                else
                        lowerLimit = examinationPoint;

                length = upperLimit - lowerLimit;
        }

        return lowerLimit;
}

long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high)
{
        long lowerLimit = low;
        long upperLimit = high;
        long examinationPoint;
        long length = upperLimit - lowerLimit;

        while( length > 1 )
        {
                examinationPoint = lowerLimit + ( length / 2 );

                if( A[examinationPoint].energy > quarry )
                        upperLimit = examinationPoint;
                else
                        lowerLimit = examinationPoint;

                length = upperLimit - lowerLimit;
        }

        return lowerLimit;
}

int pick_mat( uint64_t * seed )
{
        double dist[12];
        dist[0]  = 0.140;
        dist[1]  = 0.052;
        dist[2]  = 0.275;
        dist[3]  = 0.134;
        dist[4]  = 0.154;
        dist[5]  = 0.064;
        dist[6]  = 0.066;
        dist[7]  = 0.055;
        dist[8]  = 0.008;
        dist[9]  = 0.015;
        dist[10] = 0.025;
        dist[11] = 0.013;

        double roll = LCG_random_double(seed);

        for( int i = 0; i < 12; i++ )
        {
                double running = 0;
                for( int j = i; j > 0; j-- )
                        running += dist[j];
                if( roll < running )
                        return i;
        }

        return 0;
}

double LCG_random_double(uint64_t * seed)
{
        const uint64_t m = 9223372036854775808ULL;
        const uint64_t a = 2806196910506780709ULL;
        const uint64_t c = 1ULL;
        *seed = (a * (*seed) + c) % m;
        return (double) (*seed) / (double) m;
}

uint64_t fast_forward_LCG(uint64_t seed, uint64_t n)
{
        const uint64_t m = 9223372036854775808ULL;
        uint64_t a = 2806196910506780709ULL;
        uint64_t c = 1ULL;

        n = n % m;

        uint64_t a_new = 1;
        uint64_t c_new = 0;

        while(n > 0)
        {
                if(n & 1)
                {
                        a_new *= a;
                        c_new = c_new * a + c;
                }
                c *= (a + 1);
                a *= a;

                n >>= 1;
        }

        return (a_new * seed + c_new) % m;
}

void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                   long n_gridpoints,
                                   double * __restrict__ egrid, int * __restrict__ index_data,
                                   NuclideGridPoint * __restrict__ nuclide_grids,
                                   long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins )
{
        double f;
        NuclideGridPoint * low, * high;

        if( grid_type == NUCLIDE )
        {
                idx = grid_search_nuclide( n_gridpoints, p_energy, &nuclide_grids[nuc*n_gridpoints], 0, n_gridpoints-1);

                if( idx == n_gridpoints - 1 )
                        low = &nuclide_grids[nuc*n_gridpoints + idx - 1];
                else
                        low = &nuclide_grids[nuc*n_gridpoints + idx];
        }
        else if( grid_type == UNIONIZED)
        {
                if( index_data[idx * n_isotopes + nuc] == n_gridpoints - 1 )
                        low = &nuclide_grids[nuc*n_gridpoints + index_data[idx * n_isotopes + nuc] - 1];
                else
                        low = &nuclide_grids[nuc*n_gridpoints + index_data[idx * n_isotopes + nuc]];
        }
        else
        {
                int u_low = index_data[idx * n_isotopes + nuc];

                int u_high;
                if( idx == hash_bins - 1 )
                        u_high = n_gridpoints - 1;
                else
                        u_high = index_data[(idx+1)*n_isotopes + nuc] + 1;

                double e_low  = nuclide_grids[nuc*n_gridpoints + u_low].energy;
                double e_high = nuclide_grids[nuc*n_gridpoints + u_high].energy;
                int lower;
                if( p_energy <= e_low )
                        lower = 0;
                else if( p_energy >= e_high )
                        lower = n_gridpoints - 1;
                else
                        lower = grid_search_nuclide( n_gridpoints, p_energy, &nuclide_grids[nuc*n_gridpoints], u_low, u_high);

                if( lower == n_gridpoints - 1 )
                        low = &nuclide_grids[nuc*n_gridpoints + lower - 1];
                else
                        low = &nuclide_grids[nuc*n_gridpoints + lower];
        }

        high = low + 1;

        f = (high->energy - p_energy) / (high->energy - low->energy);

        xs_vector[0] = high->total_xs - f * (high->total_xs - low->total_xs);
        xs_vector[1] = high->elastic_xs - f * (high->elastic_xs - low->elastic_xs);
        xs_vector[2] = high->absorbtion_xs - f * (high->absorbtion_xs - low->absorbtion_xs);
        xs_vector[3] = high->fission_xs - f * (high->fission_xs - low->fission_xs);
        xs_vector[4] = high->nu_fission_xs - f * (high->nu_fission_xs - low->nu_fission_xs);
}

void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                   long n_gridpoints, int * __restrict__ num_nucs,
                                   double * __restrict__ concs,
                                   double * __restrict__ egrid, int * __restrict__ index_data,
                                   NuclideGridPoint * __restrict__ nuclide_grids,
                                   int * __restrict__ mats,
                                   double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs )
{
        int p_nuc;
        long idx = -1;
        double conc;

        for( int k = 0; k < 5; k++ )
                macro_xs_vector[k] = 0;

        if( grid_type == UNIONIZED )
                idx = grid_search( n_isotopes * n_gridpoints, p_energy, egrid);
        else if( grid_type == HASH )
        {
                double du = 1.0 / hash_bins;
                idx = p_energy / du;
        }

        for( int j = 0; j < num_nucs[mat]; j++ )
        {
                double xs_vector[5];
                p_nuc = mats[mat*max_num_nucs + j];
                conc = concs[mat*max_num_nucs + j];
                calculate_micro_xs( p_energy, p_nuc, n_isotopes,
                                   n_gridpoints, egrid, index_data,
                                   nuclide_grids, idx, xs_vector, grid_type, hash_bins );
                for( int k = 0; k < 5; k++ )
                        macro_xs_vector[k] += xs_vector[k] * conc;
        }
}
#pragma omp end declare target

void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;

        #pragma omp target teams loop thread_limit(256) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_verification)
        for (int i = 0; i < in.lookups; ++i)
        {
                uint64_t seed = STARTING_SEED;
                seed = fast_forward_LCG(seed, 2*i);
                double p_energy = LCG_random_double(&seed);
                int mat         = pick_mat(&seed);
                double macro_xs_vector[5] = {0};

                calculate_macro_xs(
                        p_energy,
                        mat,
                        in.n_isotopes,
                        in.n_gridpoints,
                        d_num_nucs,
                        d_concs,
                        d_unionized_energy_array,
                        d_index_grid,
                        d_nuclide_grid,
                        d_mats,
                        macro_xs_vector,
                        in.grid_type,
                        in.hash_bins,
                        GSD.max_num_nucs
                );

                double max = -1.0;
                int max_idx = 0;
                for(int j = 0; j < 5; j++ )
                {
                        if( macro_xs_vector[j] > max )
                        {
                                max = macro_xs_vector[j];
                                max_idx = j;
                        }
                }
                d_verification[i] = max_idx+1;
        }
}

void sampling_kernel(Inputs in, SimulationData GSD )
{
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_p_energy_samples, d_mat_samples)
        for (int i = 0; i < in.lookups; ++i)
        {
                uint64_t seed = STARTING_SEED;
                seed = fast_forward_LCG(seed, 2*i);
                double p_energy = LCG_random_double(&seed);
                int mat = pick_mat(&seed);
                d_p_energy_samples[i] = p_energy;
                d_mat_samples[i] = mat;
        }
}

void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_p_energy_samples, d_mat_samples, d_verification)
        for (int i = 0; i < in.lookups; ++i)
        {
                double macro_xs_vector[5] = {0};
                calculate_macro_xs(
                        d_p_energy_samples[i],
                        d_mat_samples[i],
                        in.n_isotopes,
                        in.n_gridpoints,
                        d_num_nucs,
                        d_concs,
                        d_unionized_energy_array,
                        d_index_grid,
                        d_nuclide_grid,
                        d_mats,
                        macro_xs_vector,
                        in.grid_type,
                        in.hash_bins,
                        GSD.max_num_nucs
                );

                double max = -1.0;
                int max_idx = 0;
                for(int j = 0; j < 5; j++ )
                {
                        if( macro_xs_vector[j] > max )
                        {
                                max = macro_xs_vector[j];
                                max_idx = j;
                        }
                }
                d_verification[i] = max_idx+1;
        }
}

void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_p_energy_samples, d_mat_samples, d_verification)
        for (int i = 0; i < in.lookups; ++i)
        {
                if (d_mat_samples[i] != m)
                        continue;

                double macro_xs_vector[5] = {0};
                calculate_macro_xs(
                        d_p_energy_samples[i],
                        m,
                        in.n_isotopes,
                        in.n_gridpoints,
                        d_num_nucs,
                        d_concs,
                        d_unionized_energy_array,
                        d_index_grid,
                        d_nuclide_grid,
                        d_mats,
                        macro_xs_vector,
                        in.grid_type,
                        in.hash_bins,
                        GSD.max_num_nucs
                );

                double max = -1.0;
                int max_idx = 0;
                for(int j = 0; j < 5; j++ )
                {
                        if( macro_xs_vector[j] > max )
                        {
                                max = macro_xs_vector[j];
                                max_idx = j;
                        }
                }
                d_verification[i] = max_idx+1;
        }
}

void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int is_fuel )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_p_energy_samples, d_mat_samples, d_verification)
        for (int i = 0; i < in.lookups; ++i)
        {
                int mat = d_mat_samples[i];
                if( ((is_fuel == 1) && (mat == 0)) || ((is_fuel == 0) && (mat != 0 )) )
                {
                        double macro_xs_vector[5] = {0};
                        calculate_macro_xs(
                                d_p_energy_samples[i],
                                mat,
                                in.n_isotopes,
                                in.n_gridpoints,
                                d_num_nucs,
                                d_concs,
                                d_unionized_energy_array,
                                d_index_grid,
                                d_nuclide_grid,
                                d_mats,
                                macro_xs_vector,
                                in.grid_type,
                                in.hash_bins,
                                GSD.max_num_nucs
                        );

                        double max = -1.0;
                        int max_idx = 0;
                        for(int j = 0; j < 5; j++ )
                        {
                                if( macro_xs_vector[j] > max )
                                {
                                        max = macro_xs_vector[j];
                                        max_idx = j;
                                }
                        }
                        d_verification[i] = max_idx+1;
                }
        }
}

void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_p_energy_samples, d_mat_samples, d_verification)
        for (int i = 0; i < n_lookups; ++i)
        {
                int idx = offset + i;
                int mat = d_mat_samples[idx];
                if( mat != m )
                        continue;

                double macro_xs_vector[5] = {0};
                calculate_macro_xs(
                        d_p_energy_samples[idx],
                        mat,
                        in.n_isotopes,
                        in.n_gridpoints,
                        d_num_nucs,
                        d_concs,
                        d_unionized_energy_array,
                        d_index_grid,
                        d_nuclide_grid,
                        d_mats,
                        macro_xs_vector,
                        in.grid_type,
                        in.hash_bins,
                        GSD.max_num_nucs
                );

                double max = -1.0;
                int max_idx = 0;
                for(int j = 0; j < 5; j++ )
                {
                        if( macro_xs_vector[j] > max )
                        {
                                max = macro_xs_vector[j];
                                max_idx = j;
                        }
                }
                d_verification[idx] = max_idx+1;
        }
}

void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_p_energy_samples, d_mat_samples, d_verification)
        for (int i = 0; i < n_lookups; ++i)
        {
                int idx = offset + i;
                double macro_xs_vector[5] = {0};
                calculate_macro_xs(
                        d_p_energy_samples[idx],
                        d_mat_samples[idx],
                        in.n_isotopes,
                        in.n_gridpoints,
                        d_num_nucs,
                        d_concs,
                        d_unionized_energy_array,
                        d_index_grid,
                        d_nuclide_grid,
                        d_mats,
                        macro_xs_vector,
                        in.grid_type,
                        in.hash_bins,
                        GSD.max_num_nucs
                );

                double max = -1.0;
                int max_idx = 0;
                for(int j = 0; j < 5; j++ )
                {
                        if( macro_xs_vector[j] > max )
                        {
                                max = macro_xs_vector[j];
                                max_idx = j;
                        }
                }
                d_verification[idx] = max_idx+1;
        }
}

unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile)
{
        double start = get_time();
        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
        profile->host_to_device_time = get_time() - start;

        if( mype == 0) printf("Running baseline event-based simulation...'"\\n\");

        int nwarmups = in.num_warmups;
        start = 0.0;
        for (int i = 0; i < in.num_iterations + nwarmups; i++) {
                if (i == nwarmups)
                        start = get_time();
                xs_lookup_kernel_baseline(in, GSD);
        }
        profile->kernel_time = get_time() - start;

        start = get_time();
        copy_from_device(SD.verification, GSD.verification, in.lookups * sizeof(unsigned long));
        profile->device_to_host_time = get_time() - start;

        unsigned long verification_scalar = reduce_verification(SD, in.lookups);
        release_device_memory(GSD);

        return verification_scalar;
}

unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype)
{
        const char * optimization_name = \"Optimization 1 - basic sample/lookup kernel splitting\";

        if( mype == 0) printf(\"Simulation Kernel:\\\"%s\\\"\\n\", optimization_name);
        if( mype == 0) printf(\"Allocating additional device data required by kernel...\\n\");

        size_t sz = in.lookups * sizeof(double) + in.lookups * sizeof(int);
        allocate_sample_buffers(in, GSD);
        if( mype == 0) printf(\"Allocated an additional %.0lf MB of data on GPU.\\n\", sz/1024.0/1024.0);
        if( mype == 0) printf(\"Beginning optimized simulation...\\n\");

        sampling_kernel(in, GSD);
        xs_lookup_kernel_optimization_1(in, GSD);

        copy_from_device(GSD.verification, SD.verification, in.lookups * sizeof(unsigned long));
        unsigned long verification_scalar = reduce_verification(SD, in.lookups);

        free_sample_buffers(GSD);
        release_device_memory(GSD);

        return verification_scalar;
}
EOF" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 16ms:


thinking
**Analyzing simulation data flow and function signatures**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "sed -n '30,120p' data/src/XSBench-omp/Main.cpp" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 11ms:
        // all simulation data structures from file instead
        if (in.binary_mode == READ)
                SD = binary_read(in);
        else
                SD = grid_init_do_not_profile(in, mype);

        // If writing from file mode is selected, write all simulation data
        // structures to file
        if (in.binary_mode == WRITE && mype == 0)
                binary_write(in, SD);

	Profile profile;

        // =====================================================================
        // Cross Section (XS) Parallel Lookup Simulation
        // This is the section that should be profiled, as it reflects a
        // realistic continuous energy Monte Carlo macroscopic cross section
        // lookup kernel.
        // =====================================================================
        if (mype == 0) {

thinking
**Checking for data movement calls**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc 'rg -n "move_simulation_data_to_device" -n data/src/XSBench-omp' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 28ms:
data/src/XSBench-omp/Simulation.cpp:634:        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
data/src/XSBench-omp/XSbench_header.cuh:114:SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD );
data/src/XSBench-omp/data_plan.md:19:- cudaMalloc calls: move_simulation_data_to_device includes 7 allocations (num_nucs, concs, mats, unionized_energy_array, index_grid, nuclide_grid, verification); each optimization run allocates `p_energy_samples` and `mat_samples` via cudaMalloc.
data/src/XSBench-omp/data_plan.md:20:- cudaMemcpy H→D: Each of the persistent geometry/material arrays is copied once in move_simulation_data_to_device; `GSD.verification` is populated on device so no copy until the end.
data/src/XSBench-omp/data_plan.md:50:We'll prefer `#pragma omp target enter data` inside helper routines that mimic move_simulation_data_to_device, followed by `#pragma omp target exit data` in release.
data/src/XSBench-omp/data_plan.md:57:- When: Immediately after the CPU-side data structures are initialized and before timing begins (exactly where move_simulation_data_to_device currently runs).
data/src/XSBench-omp/GridInit.cpp:44:SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
data/src/XSBench-omp/Simulation.cpp.bak:6:        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);

thinking
**Investigating device data movement usage**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc 'rg -n "move_simulation_data_to_device" -n data/src/XSBench-omp/Simulation.cpp.bak' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 15ms:
6:        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);

thinking
**Planning comprehensive Simulation.cpp rewrite**

I'm mapping out rewriting the entire Simulation.cpp file from scratch, carefully including headers, helper functions in a namespace, OpenMP declare target sections, kernel definitions, and run_event functions for baseline and optimizations. I'll use `cat <<'EOF'` to start the file overwrite and append subsequent sections, ensuring consistent host/device data handling, memory moves, and proper resource release, while preserving and improving original logic. This will take careful stepwise assembly.
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' > data/src/XSBench-omp/Simulation.cpp
#include \"XSbench_header.cuh\"

#include <algorithm>
#include <numeric>
#include <omp.h>
#include <utility>
#include <vector>

namespace {
int target_device()
{
    return omp_get_default_device();
}

int host_device()
{
    return omp_get_initial_device();
}

void copy_from_device(void *host_ptr, const void *device_ptr, size_t bytes)
{
    omp_target_memcpy(host_ptr, device_ptr, bytes, 0, 0, host_device(), target_device());
}

void copy_to_device(void *device_ptr, const void *host_ptr, size_t bytes)
{
    omp_target_memcpy(device_ptr, host_ptr, bytes, 0, 0, target_device(), host_device());
}

void allocate_sample_buffers(Inputs in, SimulationData &GSD)
{
    size_t energy_bytes = in.lookups * sizeof(double);
    size_t mat_bytes = in.lookups * sizeof(int);
    GSD.p_energy_samples = static_cast<double *>(omp_target_alloc(energy_bytes, target_device()));
    assert(GSD.p_energy_samples "'!= nullptr);
    GSD.length_p_energy_samples = in.lookups;
    GSD.mat_samples = static_cast<int *>(omp_target_alloc(mat_bytes, target_device()));
    assert(GSD.mat_samples != nullptr);
    GSD.length_mat_samples = in.lookups;
}

void free_sample_buffers(SimulationData &GSD)
{
    if (GSD.p_energy_samples)
        omp_target_free(GSD.p_energy_samples, target_device());
    if (GSD.mat_samples)
        omp_target_free(GSD.mat_samples, target_device());
    GSD.p_energy_samples = nullptr;
    GSD.mat_samples = nullptr;
}

void copy_samples_to_host(SimulationData &GSD, std::vector<int> &host_mats, std::vector<double> &host_energies)
{
    size_t mat_bytes = host_mats.size() * sizeof(int);
    size_t energy_bytes = host_energies.size() * sizeof(double);
    copy_from_device(host_mats.data(), GSD.mat_samples, mat_bytes);
    copy_from_device(host_energies.data(), GSD.p_energy_samples, energy_bytes);
}

void copy_samples_to_device(SimulationData &GSD, const std::vector<int> &host_mats, const std::vector<double> &host_energies)
{
    size_t mat_bytes = host_mats.size() * sizeof(int);
    size_t energy_bytes = host_energies.size() * sizeof(double);
    copy_to_device(GSD.mat_samples, host_mats.data(), mat_bytes);
    copy_to_device(GSD.p_energy_samples, host_energies.data(), energy_bytes);
}

void sort_samples_by_material(std::vector<int> &host_mats, std::vector<double> &host_energies)
{
    size_t count = host_mats.size();
    std::vector<std::pair<int, double>> zipped(count);
    for (size_t i = 0; i < count; ++i)
        zipped[i] = {host_mats[i], host_energies[i]};
    std::sort(zipped.begin(), zipped.end(), [](const auto &a, const auto &b) {
        return a.first < b.first;
    });
    for (size_t i = 0; i < count; ++i) {
        host_mats[i] = zipped[i].first;
        host_energies[i] = zipped[i].second;
    }
}

void sort_samples_by_material_then_energy(std::vector<int> &host_mats, std::vector<double> &host_energies)
{
    size_t count = host_mats.size();
    std::vector<std::pair<int, double>> zipped(count);
    for (size_t i = 0; i < count; ++i)
        zipped[i] = {host_mats[i], host_energies[i]};
    std::sort(zipped.begin(), zipped.end(), [](const auto &a, const auto &b) {
        if (a.first != b.first)
            return a.first < b.first;
        return a.second < b.second;
    });
    for (size_t i = 0; i < count; ++i) {
        host_mats[i] = zipped[i].first;
        host_energies[i] = zipped[i].second;
    }
}

unsigned long reduce_verification(SimulationData &SD, int lookups)
{
    return std::accumulate(SD.verification, SD.verification + lookups, 0ull);
}
} // namespace
EOF' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 20ms:


thinking
**Appending OpenMP declare target block**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' >> data/src/XSBench-omp/Simulation.cpp

#pragma omp declare target
long grid_search( long n, double quarry, double * __restrict__ A)
{
        long lowerLimit = 0;
        long upperLimit = n-1;
        long examinationPoint;
        long length = upperLimit - lowerLimit;

        while( length > 1 )
        {
                examinationPoint = lowerLimit + ( length / 2 );

                if( A[examinationPoint] > quarry )
                        upperLimit = examinationPoint;
                else
                        lowerLimit = examinationPoint;

                length = upperLimit - lowerLimit;
        }

        return lowerLimit;
}

long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high)
{
        long lowerLimit = low;
        long upperLimit = high;
        long examinationPoint;
        long length = upperLimit - lowerLimit;

        while( length > 1 )
        {
                examinationPoint = lowerLimit + ( length / 2 );

                if( A[examinationPoint].energy > quarry )
                        upperLimit = examinationPoint;
                else
                        lowerLimit = examinationPoint;

                length = upperLimit - lowerLimit;
        }

        return lowerLimit;
}

int pick_mat( uint64_t * seed )
{
        double dist[12];
        dist[0]  = 0.140;
        dist[1]  = 0.052;
        dist[2]  = 0.275;
        dist[3]  = 0.134;
        dist[4]  = 0.154;
        dist[5]  = 0.064;
        dist[6]  = 0.066;
        dist[7]  = 0.055;
        dist[8]  = 0.008;
        dist[9]  = 0.015;
        dist[10] = 0.025;
        dist[11] = 0.013;

        double roll = LCG_random_double(seed);

        for( int i = 0; i < 12; i++ )
        {
                double running = 0;
                for( int j = i; j > 0; j-- )
                        running += dist[j];
                if( roll < running )
                        return i;
        }

        return 0;
}

double LCG_random_double(uint64_t * seed)
{
        const uint64_t m = 9223372036854775808ULL;
        const uint64_t a = 2806196910506780709ULL;
        const uint64_t c = 1ULL;
        *seed = (a * (*seed) + c) % m;
        return (double) (*seed) / (double) m;
}

uint64_t fast_forward_LCG(uint64_t seed, uint64_t n)
{
        const uint64_t m = 9223372036854775808ULL;
        uint64_t a = 2806196910506780709ULL;
        uint64_t c = 1ULL;

        n = n % m;

        uint64_t a_new = 1;
        uint64_t c_new = 0;

        while(n > 0)
        {
                if(n & 1)
                {
                        a_new *= a;
                        c_new = c_new * a + c;
                }
                c *= (a + 1);
                a *= a;

                n >>= 1;
        }

        return (a_new * seed + c_new) % m;
}

void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                   long n_gridpoints,
                                   double * __restrict__ egrid, int * __restrict__ index_data,
                                   NuclideGridPoint * __restrict__ nuclide_grids,
                                   long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins )
{
        double f;
        NuclideGridPoint * low, * high;

        if( grid_type == NUCLIDE )
        {
                idx = grid_search_nuclide( n_gridpoints, p_energy, &nuclide_grids[nuc*n_gridpoints], 0, n_gridpoints-1);

                if( idx == n_gridpoints - 1 )
                        low = &nuclide_grids[nuc*n_gridpoints + idx - 1];
                else
                        low = &nuclide_grids[nuc*n_gridpoints + idx];
        }
        else if( grid_type == UNIONIZED)
        {
                if( index_data[idx * n_isotopes + nuc] == n_gridpoints - 1 )
                        low = &nuclide_grids[nuc*n_gridpoints + index_data[idx * n_isotopes + nuc] - 1];
                else
                        low = &nuclide_grids[nuc*n_gridpoints + index_data[idx * n_isotopes + nuc]];
        }
        else
        {
                int u_low = index_data[idx * n_isotopes + nuc];

                int u_high;
                if( idx == hash_bins - 1 )
                        u_high = n_gridpoints - 1;
                else
                        u_high = index_data[(idx+1)*n_isotopes + nuc] + 1;

                double e_low  = nuclide_grids[nuc*n_gridpoints + u_low].energy;
                double e_high = nuclide_grids[nuc*n_gridpoints + u_high].energy;
                int lower;
                if( p_energy <= e_low )
                        lower = 0;
                else if( p_energy >= e_high )
                        lower = n_gridpoints - 1;
                else
                        lower = grid_search_nuclide( n_gridpoints, p_energy, &nuclide_grids[nuc*n_gridpoints], u_low, u_high);

                if( lower == n_gridpoints - 1 )
                        low = &nuclide_grids[nuc*n_gridpoints + lower - 1];
                else
                        low = &nuclide_grids[nuc*n_gridpoints + lower];
        }

        high = low + 1;

        f = (high->energy - p_energy) / (high->energy - low->energy);

        xs_vector[0] = high->total_xs - f * (high->total_xs - low->total_xs);
        xs_vector[1] = high->elastic_xs - f * (high->elastic_xs - low->elastic_xs);
        xs_vector[2] = high->absorbtion_xs - f * (high->absorbtion_xs - low->absorbtion_xs);
        xs_vector[3] = high->fission_xs - f * (high->fission_xs - low->fission_xs);
        xs_vector[4] = high->nu_fission_xs - f * (high->nu_fission_xs - low->nu_fission_xs);
}

void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                   long n_gridpoints, int * __restrict__ num_nucs,
                                   double * __restrict__ concs,
                                   double * __restrict__ egrid, int * __restrict__ index_data,
                                   NuclideGridPoint * __restrict__ nuclide_grids,
                                   int * __restrict__ mats,
                                   double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs )
{
        int p_nuc;
        long idx = -1;
        double conc;

        for( int k = 0; k < 5; k++ )
                macro_xs_vector[k] = 0;

        if( grid_type == UNIONIZED )
                idx = grid_search( n_isotopes * n_gridpoints, p_energy, egrid);
        else if( grid_type == HASH )
        {
                double du = 1.0 / hash_bins;
                idx = p_energy / du;
        }

        for( int j = 0; j < num_nucs[mat]; j++ )
        {
                double xs_vector[5];
                p_nuc = mats[mat*max_num_nucs + j];
                conc = concs[mat*max_num_nucs + j];
                calculate_micro_xs( p_energy, p_nuc, n_isotopes,
                                   n_gridpoints, egrid, index_data,
                                   nuclide_grids, idx, xs_vector, grid_type, hash_bins );
                for( int k = 0; k < 5; k++ )
                        macro_xs_vector[k] += xs_vector[k] * conc;
        }
}
#pragma omp end declare target
EOF" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 10ms:


thinking
**Appending kernel definitions**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' >> data/src/XSBench-omp/Simulation.cpp

void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;

        #pragma omp target teams loop thread_limit(256) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_verification)
        for (int i = 0; i < in.lookups; ++i)
        {
                uint64_t seed = STARTING_SEED;
                seed = fast_forward_LCG(seed, 2*i);
                double p_energy = LCG_random_double(&seed);
                int mat         = pick_mat(&seed);
                double macro_xs_vector[5] = {0};

                calculate_macro_xs(
                        p_energy,
                        mat,
                        in.n_isotopes,
                        in.n_gridpoints,
                        d_num_nucs,
                        d_concs,
                        d_unionized_energy_array,
                        d_index_grid,
                        d_nuclide_grid,
                        d_mats,
                        macro_xs_vector,
                        in.grid_type,
                        in.hash_bins,
                        GSD.max_num_nucs
                );

                double max = -1.0;
                int max_idx = 0;
                for(int j = 0; j < 5; j++ )
                {
                        if( macro_xs_vector[j] > max )
                        {
                                max = macro_xs_vector[j];
                                max_idx = j;
                        }
                }
                d_verification[i] = max_idx+1;
        }
}

void sampling_kernel(Inputs in, SimulationData GSD )
{
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_p_energy_samples, d_mat_samples)
        for (int i = 0; i < in.lookups; ++i)
        {
                uint64_t seed = STARTING_SEED;
                seed = fast_forward_LCG(seed, 2*i);
                double p_energy = LCG_random_double(&seed);
                int mat = pick_mat(&seed);
                d_p_energy_samples[i] = p_energy;
                d_mat_samples[i] = mat;
        }
}

void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_p_energy_samples, d_mat_samples, d_verification)
        for (int i = 0; i < in.lookups; ++i)
        {
                double macro_xs_vector[5] = {0};
                calculate_macro_xs(
                        d_p_energy_samples[i],
                        d_mat_samples[i],
                        in.n_isotopes,
                        in.n_gridpoints,
                        d_num_nucs,
                        d_concs,
                        d_unionized_energy_array,
                        d_index_grid,
                        d_nuclide_grid,
                        d_mats,
                        macro_xs_vector,
                        in.grid_type,
                        in.hash_bins,
                        GSD.max_num_nucs
                );

                double max = -1.0;
                int max_idx = 0;
                for(int j = 0; j < 5; j++ )
                {
                        if( macro_xs_vector[j] > max )
                        {
                                max = macro_xs_vector[j];
                                max_idx = j;
                        }
                }
                d_verification[i] = max_idx+1;
        }
}

void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_p_energy_samples, d_mat_samples, d_verification)
        for (int i = 0; i < in.lookups; ++i)
        {
                if (d_mat_samples[i] "'!= m)
                        continue;

                double macro_xs_vector[5] = {0};
                calculate_macro_xs(
                        d_p_energy_samples[i],
                        m,
                        in.n_isotopes,
                        in.n_gridpoints,
                        d_num_nucs,
                        d_concs,
                        d_unionized_energy_array,
                        d_index_grid,
                        d_nuclide_grid,
                        d_mats,
                        macro_xs_vector,
                        in.grid_type,
                        in.hash_bins,
                        GSD.max_num_nucs
                );

                double max = -1.0;
                int max_idx = 0;
                for(int j = 0; j < 5; j++ )
                {
                        if( macro_xs_vector[j] > max )
                        {
                                max = macro_xs_vector[j];
                                max_idx = j;
                        }
                }
                d_verification[i] = max_idx+1;
        }
}

void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int is_fuel )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_p_energy_samples, d_mat_samples, d_verification)
        for (int i = 0; i < in.lookups; ++i)
        {
                int mat = d_mat_samples[i];
                if( ((is_fuel == 1) && (mat == 0)) || ((is_fuel == 0) && (mat != 0 )) )
                {
                        double macro_xs_vector[5] = {0};
                        calculate_macro_xs(
                                d_p_energy_samples[i],
                                mat,
                                in.n_isotopes,
                                in.n_gridpoints,
                                d_num_nucs,
                                d_concs,
                                d_unionized_energy_array,
                                d_index_grid,
                                d_nuclide_grid,
                                d_mats,
                                macro_xs_vector,
                                in.grid_type,
                                in.hash_bins,
                                GSD.max_num_nucs
                        );

                        double max = -1.0;
                        int max_idx = 0;
                        for(int j = 0; j < 5; j++ )
                        {
                                if( macro_xs_vector[j] > max )
                                {
                                        max = macro_xs_vector[j];
                                        max_idx = j;
                                }
                        }
                        d_verification[i] = max_idx+1;
                }
        }
}

void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_p_energy_samples, d_mat_samples, d_verification)
        for (int i = 0; i < n_lookups; ++i)
        {
                int idx = offset + i;
                int mat = d_mat_samples[idx];
                if( mat != m )
                        continue;

                double macro_xs_vector[5] = {0};
                calculate_macro_xs(
                        d_p_energy_samples[idx],
                        mat,
                        in.n_isotopes,
                        in.n_gridpoints,
                        d_num_nucs,
                        d_concs,
                        d_unionized_energy_array,
                        d_index_grid,
                        d_nuclide_grid,
                        d_mats,
                        macro_xs_vector,
                        in.grid_type,
                        in.hash_bins,
                        GSD.max_num_nucs
                );

                double max = -1.0;
                int max_idx = 0;
                for(int j = 0; j < 5; j++ )
                {
                        if( macro_xs_vector[j] > max )
                        {
                                max = macro_xs_vector[j];
                                max_idx = j;
                        }
                }
                d_verification[idx] = max_idx+1;
        }
}

void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset )
{
        double *d_concs = GSD.concs;
        int *d_num_nucs = GSD.num_nucs;
        int *d_index_grid = GSD.index_grid;
        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
        double *d_unionized_energy_array = GSD.unionized_energy_array;
        int *d_mats = GSD.mats;
        unsigned long *d_verification = GSD.verification;
        double *d_p_energy_samples = GSD.p_energy_samples;
        int *d_mat_samples = GSD.mat_samples;

        #pragma omp target teams loop thread_limit(32) is_device_ptr(d_concs, d_num_nucs, d_index_grid, d_nuclide_grid, d_unionized_energy_array, d_mats, d_p_energy_samples, d_mat_samples, d_verification)
        for (int i = 0; i < n_lookups; ++i)
        {
                int idx = offset + i;
                double macro_xs_vector[5] = {0};
                calculate_macro_xs(
                        d_p_energy_samples[idx],
                        d_mat_samples[idx],
                        in.n_isotopes,
                        in.n_gridpoints,
                        d_num_nucs,
                        d_concs,
                        d_unionized_energy_array,
                        d_index_grid,
                        d_nuclide_grid,
                        d_mats,
                        macro_xs_vector,
                        in.grid_type,
                        in.hash_bins,
                        GSD.max_num_nucs
                );

                double max = -1.0;
                int max_idx = 0;
                for(int j = 0; j < 5; j++ )
                {
                        if( macro_xs_vector[j] > max )
                        {
                                max = macro_xs_vector[j];
                                max_idx = j;
                        }
                }
                d_verification[idx] = max_idx+1;
        }
}
EOF' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 9ms:


thinking
**Preparing run_event functions implementation**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' >> data/src/XSBench-omp/Simulation.cpp

unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile)
{
        double start = get_time();
        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
        profile->host_to_device_time = get_time() - start;

        if( mype == 0) printf(\"Running baseline event-based simulation...\\n\");

        int nwarmups = in.num_warmups;
        start = 0.0;
        for (int i = 0; i < in.num_iterations + nwarmups; i++) {
                if (i == nwarmups)
                        start = get_time();
                xs_lookup_kernel_baseline(in, GSD);
        }
        profile->kernel_time = get_time() - start;

        start = get_time();
        copy_from_device(SD.verification, GSD.verification, in.lookups * sizeof(unsigned long));
        profile->device_to_host_time = get_time() - start;

        unsigned long verification_scalar = reduce_verification(SD, in.lookups);

        release_device_memory(GSD);
        return verification_scalar;
}
EOF" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 10ms:


thinking
**Preparing event-based simulation optimization code**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' >> data/src/XSBench-omp/Simulation.cpp

unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData SD, int mype)
{
        const char * optimization_name = \"Optimization 1 - basic sample/lookup kernel splitting\";

        if( mype == 0) printf(\"Simulation Kernel:\\\"%s\\\"\\n\", optimization_name);
        if( mype == 0) printf(\"Allocating additional device data required by kernel...\\n\");

        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
        size_t sz = in.lookups * sizeof(double) + in.lookups * sizeof(int);
        allocate_sample_buffers(in, GSD);
        if( mype == 0) printf(\"Allocated an additional %.0lf MB of data on GPU.\\n\", sz/1024.0/1024.0);
        if( mype == 0) printf(\"Beginning optimized simulation...\\n\");

        sampling_kernel(in, GSD);
        xs_lookup_kernel_optimization_1(in, GSD);

        copy_from_device(SD.verification, GSD.verification, in.lookups * sizeof(unsigned long));
        unsigned long verification_scalar = reduce_verification(SD, in.lookups);

        free_sample_buffers(GSD);
        release_device_memory(GSD);

        return verification_scalar;
}
EOF" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:


thinking
**Implementing optimized event simulation with loops**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' >> data/src/XSBench-omp/Simulation.cpp

unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData SD, int mype)
{
        const char * optimization_name = \"Optimization 2 - Material Lookup Kernels\";

        if( mype == 0) printf(\"Simulation Kernel:\\\"%s\\\"\\n\", optimization_name);
        if( mype == 0) printf(\"Allocating additional device data required by kernel...\\n\");

        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
        size_t sz = in.lookups * sizeof(double) + in.lookups * sizeof(int);
        allocate_sample_buffers(in, GSD);
        if( mype == 0) printf(\"Allocated an additional %.0lf MB of data on GPU.\\n\", sz/1024.0/1024.0);
        if( mype == 0) printf(\"Beginning optimized simulation...\\n\");

        sampling_kernel(in, GSD);
        for( int m = 0; m < 12; m++ )
                xs_lookup_kernel_optimization_2(in, GSD, m);

        copy_from_device(SD.verification, GSD.verification, in.lookups * sizeof(unsigned long));
        unsigned long verification_scalar = reduce_verification(SD, in.lookups);

        free_sample_buffers(GSD);
        release_device_memory(GSD);

        return verification_scalar;
}
EOF" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 9ms:


thinking
**Implementing dual-kernel optimization**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' >> data/src/XSBench-omp/Simulation.cpp

unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData SD, int mype)
{
        const char * optimization_name = \"Optimization 3 - Fuel or Other Lookup Kernels\";

        if( mype == 0) printf(\"Simulation Kernel:\\\"%s\\\"\\n\", optimization_name);
        if( mype == 0) printf(\"Allocating additional device data required by kernel...\\n\");

        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
        size_t sz = in.lookups * sizeof(double) + in.lookups * sizeof(int);
        allocate_sample_buffers(in, GSD);
        if( mype == 0) printf(\"Allocated an additional %.0lf MB of data on GPU.\\n\", sz/1024.0/1024.0);
        if( mype == 0) printf(\"Beginning optimized simulation...\\n\");

        sampling_kernel(in, GSD);
        xs_lookup_kernel_optimization_3(in, GSD, 0);
        xs_lookup_kernel_optimization_3(in, GSD, 1);

        copy_from_device(SD.verification, GSD.verification, in.lookups * sizeof(unsigned long));
        unsigned long verification_scalar = reduce_verification(SD, in.lookups);

        free_sample_buffers(GSD);
        release_device_memory(GSD);

        return verification_scalar;
}
EOF" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 14ms:


thinking
**Describing optimization steps for host sorting**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' >> data/src/XSBench-omp/Simulation.cpp

unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData SD, int mype)
{
        const char * optimization_name = \"Optimization 4 - All Material Lookup Kernels + Material Sort\";

        if( mype == 0) printf(\"Simulation Kernel:\\\"%s\\\"\\n\", optimization_name);
        if( mype == 0) printf(\"Allocating additional device data required by kernel...\\n\");

        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
        size_t sz = in.lookups * sizeof(double) + in.lookups * sizeof(int);
        allocate_sample_buffers(in, GSD);
        if( mype == 0) printf(\"Allocated an additional %.0lf MB of data on GPU.\\n\", sz/1024.0/1024.0);
        if( mype == 0) printf(\"Beginning optimized simulation...\\n\");

        sampling_kernel(in, GSD);
        std::vector<int> host_mats(in.lookups);
        std::vector<double> host_energies(in.lookups);
        copy_samples_to_host(GSD, host_mats, host_energies);

        sort_samples_by_material(host_mats, host_energies);

        int n_lookups_per_material[12] = {0};
        for (int mat : host_mats)
                n_lookups_per_material[mat]++;

        copy_samples_to_device(GSD, host_mats, host_energies);

        int offset = 0;
        for( int m = 0; m < 12; m++ )
        {
                int count = n_lookups_per_material[m];
                if (count == 0)
                        continue;
                xs_lookup_kernel_optimization_4(in, GSD, m, count, offset);
                offset += count;
        }

        copy_from_device(SD.verification, GSD.verification, in.lookups * sizeof(unsigned long));
        unsigned long verification_scalar = reduce_verification(SD, in.lookups);

        free_sample_buffers(GSD);
        release_device_memory(GSD);

        return verification_scalar;
}
EOF" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:


thinking
**Handling partition with zipped vectors**

I’m thinking about implementing a stable partition on zipped vectors of materials and energies to separate fuel lookups (mat == 0) from non-fuel efficiently. Since helper functions for sorting exist, I’m considering reusing them or doing a manual `std::stable_partition` on the zipped data, then copying back to host arrays and device before launching two kernels separately for fuel and non-fuel ranges. I’ll include code snippets to show this approach clearly.
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' >> data/src/XSBench-omp/Simulation.cpp

unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData SD, int mype)
{
        const char * optimization_name = \"Optimization 5 - Fuel/No Fuel Lookup Kernels + Fuel/No Fuel Sort\";

        if( mype == 0) printf(\"Simulation Kernel:\\\"%s\\\"\\n\", optimization_name);
        if( mype == 0) printf(\"Allocating additional device data required by kernel...\\n\");

        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
        size_t sz = in.lookups * sizeof(double) + in.lookups * sizeof(int);
        allocate_sample_buffers(in, GSD);
        if( mype == 0) printf(\"Allocated an additional %.0lf MB of data on GPU.\\n\", sz/1024.0/1024.0);
        if( mype == 0) printf(\"Beginning optimized simulation...\\n\");

        sampling_kernel(in, GSD);
        std::vector<int> host_mats(in.lookups);
        std::vector<double> host_energies(in.lookups);
        copy_samples_to_host(GSD, host_mats, host_energies);

        std::vector<std::pair<int, double>> zipped(in.lookups);
        for (int i = 0; i < in.lookups; ++i)
                zipped[i] = {host_mats[i], host_energies[i]};

        auto mid = std::stable_partition(zipped.begin(), zipped.end(), [](const auto &entry) {
                return entry.first == 0;
        });

        int n_fuel_lookups = std::distance(zipped.begin(), mid);
        for (int i = 0; i < in.lookups; ++i) {
                host_mats[i] = zipped[i].first;
                host_energies[i] = zipped[i].second;
        }

        copy_samples_to_device(GSD, host_mats, host_energies);

        xs_lookup_kernel_optimization_5(in, GSD, n_fuel_lookups, 0);
        xs_lookup_kernel_optimization_5(in, GSD, in.lookups - n_fuel_lookups, n_fuel_lookups);

        copy_from_device(SD.verification, GSD.verification, in.lookups * sizeof(unsigned long));
        unsigned long verification_scalar = reduce_verification(SD, in.lookups);

        free_sample_buffers(GSD);
        release_device_memory(GSD);

        return verification_scalar;
}
EOF" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:


thinking
**Implementing optimization with two-level sorting**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc "cat <<'EOF' >> data/src/XSBench-omp/Simulation.cpp

unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData SD, int mype)
{
        const char * optimization_name = \"Optimization 6 - Material & Energy Sorts + Material-specific Kernels\";

        if( mype == 0) printf(\"Simulation Kernel:\\\"%s\\\"\\n\", optimization_name);
        if( mype == 0) printf(\"Allocating additional device data required by kernel...\\n\");

        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
        size_t sz = in.lookups * sizeof(double) + in.lookups * sizeof(int);
        allocate_sample_buffers(in, GSD);
        if( mype == 0) printf(\"Allocated an additional %.0lf MB of data on GPU.\\n\", sz/1024.0/1024.0);
        if( mype == 0) printf(\"Beginning optimized simulation...\\n\");

        sampling_kernel(in, GSD);
        std::vector<int> host_mats(in.lookups);
        std::vector<double> host_energies(in.lookups);
        copy_samples_to_host(GSD, host_mats, host_energies);

        sort_samples_by_material_then_energy(host_mats, host_energies);

        int n_lookups_per_material[12] = {0};
        for (int mat : host_mats)
                n_lookups_per_material[mat]++;

        copy_samples_to_device(GSD, host_mats, host_energies);

        int offset = 0;
        for( int m = 0; m < 12; m++ )
        {
                int count = n_lookups_per_material[m];
                if (count == 0)
                        continue;
                xs_lookup_kernel_optimization_4(in, GSD, m, count, offset);
                offset += count;
        }

        copy_from_device(SD.verification, GSD.verification, in.lookups * sizeof(unsigned long));
        unsigned long verification_scalar = reduce_verification(SD, in.lookups);

        free_sample_buffers(GSD);
        release_device_memory(GSD);

        return verification_scalar;
}
EOF" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 7ms:


thinking
**Updating includes for iterator support**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

file update
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -7,2 +7,3 @@
 #include <vector>
+#include <iterator>
 
apply_patch(auto_approved=true) exited 0 in 18ms:
Success. Updated the following files:
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 


thinking
**Verifying proper includes and cleanup**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

exec
/bin/bash -lc 'rg -n "cuda" data/src/XSBench-omp' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 31ms:
data/src/XSBench-omp/io.cpp:128:	cudaDeviceProp prop;
data/src/XSBench-omp/io.cpp:130:	cudaGetDevice(&device);
data/src/XSBench-omp/io.cpp:131:	cudaGetDeviceProperties ( &prop, device );
data/src/XSBench-omp/Simulation.cpp.bak:18:			gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp.bak:23:	gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp.bak:24:	gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp.bak:29:        gpuErrchk(cudaMemcpy(SD.verification, GSD.verification, in.lookups * sizeof(unsigned long), cudaMemcpyDeviceToHost) );
data/src/XSBench-omp/Simulation.cpp.bak:315:        gpuErrchk( cudaMalloc((void **) &GSD.p_energy_samples, sz) );
data/src/XSBench-omp/Simulation.cpp.bak:320:        gpuErrchk( cudaMalloc((void **) &GSD.mat_samples, sz) );
data/src/XSBench-omp/Simulation.cpp.bak:332:        gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp.bak:333:        gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp.bak:336:        gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp.bak:337:        gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp.bak:342:        gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp.bak:343:        gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp.bak:418:        gpuErrchk( cudaMalloc((void **) &GSD.p_energy_samples, sz) );
data/src/XSBench-omp/Simulation.cpp.bak:423:        gpuErrchk( cudaMalloc((void **) &GSD.mat_samples, sz) );
data/src/XSBench-omp/Simulation.cpp.bak:435:        gpuErrchk( cudaPeekAtLastError() );
data/src/XSBench-omp/Simulation.cpp.bak:436:        gpuErrchk( cudaDeviceSynchronize() );
data/src/XSBench-omp/Simulation.cpp.bak:440:        gpuErrchk( cudaPeekAtLastError() );

thinking
**Updating io.cpp for OpenMP offload**

I'll replace all CUDA-specific code in io.cpp with OpenMP offload calls, adding `#include <omp.h>` at the top and changing print statements to reflect the new programming model and target device ID accordingly.
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 

file update
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,2 +1,3 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
apply_patch(auto_approved=true) exited 0 in 20ms:
Success. Updated the following files:
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..d42af0a6151ff2e58ac7675929fcf36979782375
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)


thinking
**Updating print_inputs function for OpenMP**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..d42af0a6151ff2e58ac7675929fcf36979782375
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)

file update
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -127,8 +127,5 @@
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
apply_patch(auto_approved=true) exited 0 in 10ms:
Success. Updated the following files:
M /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else


thinking
**Verifying cuda references cleanup**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'rg -n "cuda" data/src/XSBench-omp/Simulation.cpp' in /root/codex_baseline/cuda_omp_pareval_workdir exited 1 in 10ms:


thinking
**Verifying removal of thrust references**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'rg -n "thrust" data/src/XSBench-omp' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 11ms:
data/src/XSBench-omp/Simulation.cpp.bak:341:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp.bak:445:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp.bak:534:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp.bak:619:                n_lookups_per_material[m] = thrust::count(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, m);
data/src/XSBench-omp/Simulation.cpp.bak:621:        thrust::sort_by_key(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, GSD.p_energy_samples);
data/src/XSBench-omp/Simulation.cpp.bak:636:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp.bak:728:        int n_fuel_lookups = thrust::count(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp.bak:730:        thrust::partition(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, GSD.p_energy_samples, is_mat_fuel());
data/src/XSBench-omp/Simulation.cpp.bak:743:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/Simulation.cpp.bak:825:                n_lookups_per_material[m] = thrust::count(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, m);
data/src/XSBench-omp/Simulation.cpp.bak:827:        thrust::sort_by_key(thrust::device, GSD.mat_samples, GSD.mat_samples + in.lookups, GSD.p_energy_samples);
data/src/XSBench-omp/Simulation.cpp.bak:832:                thrust::sort_by_key(thrust::device, GSD.p_energy_samples + offset, GSD.p_energy_samples + offset + n_lookups_per_material[m], GSD.mat_samples + offset);
data/src/XSBench-omp/Simulation.cpp.bak:849:        unsigned long verification_scalar = thrust::reduce(thrust::device, GSD.verification, GSD.verification + in.lookups, 0);
data/src/XSBench-omp/data_plan.md:83:- `thrust::reduce`, `thrust::count`, `thrust::sort_by_key`, `thrust::partition` will be replaced with `std::accumulate`, `std::count`, `std::sort`, and `std::stable_partition` (or manual counting) executed on the host arrays before the target loops.
data/src/XSBench-omp/analysis.md:24:  ├── `thrust::count` (Simulation.cu:617)
data/src/XSBench-omp/analysis.md:25:  ├── `thrust::sort_by_key` (Simulation.cu:621)
data/src/XSBench-omp/analysis.md:28:  ├── `thrust::count` and `thrust::partition` (Simulation.cu:728-731)
data/src/XSBench-omp/analysis.md:31:  ├── `thrust::count` + `thrust::sort_by_key` + per-range `thrust::sort_by_key` (Simulation.cu:823-834)
data/src/XSBench-omp/analysis.md:98:- **Dependencies:** per-material counts (`n_lookups_per_material`) plus sorted arrays from `thrust::sort`
data/src/XSBench-omp/analysis.md:111:- **Contains:** thrust calls (`count`, `sort_by_key`, `partition`) that gather metadata before launching kernels

thinking
**Checking for CUDA references**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'rg -n "cuda" data/src/XSBench-omp/io.cpp' in /root/codex_baseline/cuda_omp_pareval_workdir exited 1 in 11ms:


thinking
**Reviewing includes and preparing build**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

Plan update
  ✓ Analyze existing CUDA Simulation files/headers to document data allocations and kernel flow, then create data_plan.md as required
  ✓ Convert Simulation logic (kernels, run loops, helper functions) to OpenMP offload equivalents, removing CUDA APIs and thurst references
  → Validate conversion (ensure include adjustments, compile/test if feasible, note any outstanding issues)

thinking
**Preparing to build and test**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'make -f Makefile.nvc clean' in /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp succeeded in 11ms:
rm -rf XSBench Main.o io.o Simulation.o GridInit.o XSutils.o Materials.o

thinking
**Running make command for Makefile.nvc**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'make -f Makefile.nvc' in /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp succeeded in 5.78s:
nvc++  -std=c++14 -cuda -gpu=sm_89 -I/root/codex_baseline/cuda_omp_pareval_workdir/gate_sdk -O3 -c Main.cpp -o Main.o
nvc++  -std=c++14 -cuda -gpu=sm_89 -I/root/codex_baseline/cuda_omp_pareval_workdir/gate_sdk -O3 -c io.cpp -o io.o
nvc++  -std=c++14 -cuda -gpu=sm_89 -I/root/codex_baseline/cuda_omp_pareval_workdir/gate_sdk -O3 -c Simulation.cpp -o Simulation.o
nvc++  -std=c++14 -cuda -gpu=sm_89 -I/root/codex_baseline/cuda_omp_pareval_workdir/gate_sdk -O3 -c GridInit.cpp -o GridInit.o
nvc++  -std=c++14 -cuda -gpu=sm_89 -I/root/codex_baseline/cuda_omp_pareval_workdir/gate_sdk -O3 -c XSutils.cpp -o XSutils.o
nvc++  -std=c++14 -cuda -gpu=sm_89 -I/root/codex_baseline/cuda_omp_pareval_workdir/gate_sdk -O3 -c Materials.cpp -o Materials.o
nvc++  -std=c++14 -cuda -gpu=sm_89 -I/root/codex_baseline/cuda_omp_pareval_workdir/gate_sdk -O3 Main.o io.o Simulation.o GridInit.o XSutils.o Materials.o -o XSBench -lm

thinking
**Preparing to run the build with environment variable**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > gpu_output.txt 2>&1' in /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp succeeded in 840ms:


thinking
**Preparing output comparison**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'diff baseline_output.txt gpu_output.txt' in /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp exited 1 in 20ms:
1,89d0
< nvcc  -std=c++14 -arch=sm_89 -Xcompiler -Wall -I/root/codex_baseline/cuda_omp_pareval_workdir/gate_sdk -O3 -Xcompiler -O3 -c Main.cu -o Main.o
< nvcc  -std=c++14 -arch=sm_89 -Xcompiler -Wall -I/root/codex_baseline/cuda_omp_pareval_workdir/gate_sdk -O3 -Xcompiler -O3 -c io.cu -o io.o
< io.cu(523): warning #1650-D: result of call is not used
<    fread(&SD, sizeof(SimulationData), 1, fp);
<    ^
< 
< Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"
< 
< io.cu(534): warning #1650-D: result of call is not used
<    fread(SD.num_nucs, sizeof(int), SD.length_num_nucs, fp);
<    ^
< 
< io.cu(535): warning #1650-D: result of call is not used
<    fread(SD.concs, sizeof(double), SD.length_concs, fp);
<    ^
< 
< io.cu(536): warning #1650-D: result of call is not used
<    fread(SD.mats, sizeof(int), SD.length_mats, fp);
<    ^

thinking
**Updating plan with output differences**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

Plan update
  ✓ Analyze existing CUDA Simulation files/headers to document data allocations and kernel flow, then create data_plan.md as required
  ✓ Convert Simulation logic (kernels, run loops, helper functions) to OpenMP offload equivalents, removing CUDA APIs and thurst references
  ✓ Validate conversion (ensure include adjustments, compile/test if feasible, note any outstanding issues)

thinking
**Checking git status**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'git status -sb' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 57ms:
## main...origin/main
 M ../NPB-OMP5-Offload-Translated-and-Manual-Fixing/EP/EP/ep.c
 M ../NPB-OMP5-Offload-Translated-and-Manual-Fixing/FT/FT/ft.c
 D data/src/XSBench-omp/GridInit.c
 D data/src/XSBench-omp/Main.c
 D data/src/XSBench-omp/Makefile
 M data/src/XSBench-omp/Makefile.nvc
 D data/src/XSBench-omp/Materials.c
 D data/src/XSBench-omp/XSbench_header.h
 D data/src/XSBench-omp/XSutils.c
 D data/src/XSBench-omp/io.c
 D data/src/microXOR-omp/Makefile
 M data/src/microXOR-omp/Makefile.nvc
 D data/src/microXOR-omp/include/microXOR.hpp
 M data/src/microXOR-omp/main.cpp
 D data/src/microXOR-omp/main.o
 D data/src/microXOR-omp/microXOR.o
 D data/src/microXORh-omp/Makefile
 M data/src/microXORh-omp/Makefile.nvc
 D data/src/microXORh-omp/include/microXORh.hpp

thinking
**Preparing clean state by removing build artifacts**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'make -f Makefile.nvc clean' in /root/codex_baseline/cuda_omp_pareval_workdir/data/src/XSBench-omp succeeded in 9ms:
rm -rf XSBench Main.o io.o Simulation.o GridInit.o XSutils.o Materials.o

thinking
**Deciding on keeping backup and output files**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'git status -sb data/src/XSBench-omp' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 16ms:
## main...origin/main
 D data/src/XSBench-omp/GridInit.c
 D data/src/XSBench-omp/Main.c
 D data/src/XSBench-omp/Makefile
 M data/src/XSBench-omp/Makefile.nvc
 D data/src/XSBench-omp/Materials.c
 D data/src/XSBench-omp/XSbench_header.h
 D data/src/XSBench-omp/XSutils.c
 D data/src/XSBench-omp/io.c
?? data/src/XSBench-omp/GridInit.cpp
?? data/src/XSBench-omp/Main.cpp
?? data/src/XSBench-omp/Materials.cpp
?? data/src/XSBench-omp/Simulation.cpp
?? data/src/XSBench-omp/Simulation.cpp.bak
?? data/src/XSBench-omp/XSbench_header.cuh
?? data/src/XSBench-omp/XSutils.cpp
?? data/src/XSBench-omp/analysis.md
?? data/src/XSBench-omp/baseline_output.txt
?? data/src/XSBench-omp/data_plan.md
?? data/src/XSBench-omp/gpu_output.txt

thinking
**Summarizing git status and file changes**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'git ls-files data/src/XSBench-omp/baseline_output.txt' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 18ms:


thinking
**Preparing detailed file references for summary**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc "nl -ba data/src/XSBench-omp/Simulation.cpp | sed -n '1,80p'" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 10ms:
     1	#include "XSbench_header.cuh"
     2	
     3	#include <algorithm>
     4	#include <numeric>
     5	#include <omp.h>
     6	#include <utility>
     7	#include <vector>
     8	#include <iterator>
     9	
    10	namespace {
    11	int target_device()
    12	{
    13	    return omp_get_default_device();
    14	}
    15	
    16	int host_device()
    17	{
    18	    return omp_get_initial_device();
    19	}
    20	

thinking
**Identifying code section line ranges**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc "nl -ba data/src/XSBench-omp/Simulation.cpp | sed -n '120,260p'" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 11ms:
   120	                        upperLimit = examinationPoint;
   121	                else
   122	                        lowerLimit = examinationPoint;
   123	
   124	                length = upperLimit - lowerLimit;
   125	        }
   126	
   127	        return lowerLimit;
   128	}
   129	
   130	long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high)
   131	{
   132	        long lowerLimit = low;
   133	        long upperLimit = high;
   134	        long examinationPoint;
   135	        long length = upperLimit - lowerLimit;
   136	
   137	        while( length > 1 )
   138	        {
   139	                examinationPoint = lowerLimit + ( length / 2 );

thinking
**Locating function line numbers**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc "nl -ba data/src/XSBench-omp/Simulation.cpp | sed -n '260,420p'" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 9ms:
   260	                else
   261	                        lower = grid_search_nuclide( n_gridpoints, p_energy, &nuclide_grids[nuc*n_gridpoints], u_low, u_high);
   262	
   263	                if( lower == n_gridpoints - 1 )
   264	                        low = &nuclide_grids[nuc*n_gridpoints + lower - 1];
   265	                else
   266	                        low = &nuclide_grids[nuc*n_gridpoints + lower];
   267	        }
   268	
   269	        high = low + 1;
   270	
   271	        f = (high->energy - p_energy) / (high->energy - low->energy);
   272	
   273	        xs_vector[0] = high->total_xs - f * (high->total_xs - low->total_xs);
   274	        xs_vector[1] = high->elastic_xs - f * (high->elastic_xs - low->elastic_xs);
   275	        xs_vector[2] = high->absorbtion_xs - f * (high->absorbtion_xs - low->absorbtion_xs);
   276	        xs_vector[3] = high->fission_xs - f * (high->fission_xs - low->fission_xs);
   277	        xs_vector[4] = high->nu_fission_xs - f * (high->nu_fission_xs - low->nu_fission_xs);
   278	}
   279	

thinking
**Collecting run_event function lines**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc "nl -ba data/src/XSBench-omp/Simulation.cpp | sed -n '520,780p'" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 10ms:
   520	                        {
   521	                                if( macro_xs_vector[j] > max )
   522	                                {
   523	                                        max = macro_xs_vector[j];
   524	                                        max_idx = j;
   525	                                }
   526	                        }
   527	                        d_verification[i] = max_idx+1;
   528	                }
   529	        }
   530	}
   531	
   532	void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset )
   533	{
   534	        double *d_concs = GSD.concs;
   535	        int *d_num_nucs = GSD.num_nucs;
   536	        int *d_index_grid = GSD.index_grid;
   537	        NuclideGridPoint *d_nuclide_grid = GSD.nuclide_grid;
   538	        double *d_unionized_energy_array = GSD.unionized_energy_array;
   539	        int *d_mats = GSD.mats;

thinking
**Continuing code review for optimization**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc "nl -ba data/src/XSBench-omp/Simulation.cpp | sed -n '780,1040p'" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
   780	
   781	unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData SD, int mype)
   782	{
   783	        const char * optimization_name = "Optimization 5 - Fuel/No Fuel Lookup Kernels + Fuel/No Fuel Sort";
   784	
   785	        if( mype == 0) printf("Simulation Kernel:\"%s\"\n", optimization_name);
   786	        if( mype == 0) printf("Allocating additional device data required by kernel...\n");
   787	
   788	        SimulationData GSD = move_simulation_data_to_device(in, mype, SD);
   789	        size_t sz = in.lookups * sizeof(double) + in.lookups * sizeof(int);
   790	        allocate_sample_buffers(in, GSD);
   791	        if( mype == 0) printf("Allocated an additional %.0lf MB of data on GPU.\n", sz/1024.0/1024.0);
   792	        if( mype == 0) printf("Beginning optimized simulation...\n");
   793	
   794	        sampling_kernel(in, GSD);
   795	        std::vector<int> host_mats(in.lookups);
   796	        std::vector<double> host_energies(in.lookups);
   797	        copy_samples_to_host(GSD, host_mats, host_energies);
   798	
   799	        std::vector<std::pair<int, double>> zipped(in.lookups);

thinking
**Preparing line references for summary**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc "nl -ba data/src/XSBench-omp/GridInit.cpp | sed -n '1,200p'" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 11ms:
     1	#include "XSbench_header.cuh"
     2	#include <omp.h>
     3	
     4	// Moves all required data structures to the GPU's memory space
     5	namespace {
     6	int get_target_device()
     7	{
     8	        return omp_get_default_device();
     9	}
    10	
    11	int get_host_device()
    12	{
    13	        return omp_get_initial_device();
    14	}
    15	
    16	template<typename T>
    17	static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
    18	{
    19	        if (count == 0 || src == nullptr) {
    20	                dst = nullptr;

thinking
**Preparing summary details about memory allocation changes**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc "nl -ba data/src/XSBench-omp/XSbench_header.cuh | sed -n '1,200p'" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 10ms:
     1	#ifndef __XSBENCH_HEADER_H__
     2	#define __XSBENCH_HEADER_H__
     3	
     4	#include<stdio.h>
     5	#include<stdlib.h>
     6	#include<math.h>
     7	#include<assert.h>
     8	#include<stdint.h>
     9	#include <chrono>
    10	#include "XSbench_shared_header.h"
    11	
    12	// Grid types
    13	#define UNIONIZED 0
    14	#define NUCLIDE 1
    15	#define HASH 2
    16	
    17	// Simulation types
    18	#define HISTORY_BASED 1
    19	#define EVENT_BASED 2
    20	

thinking
**Noting header and io.cpp changes**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc "nl -ba data/src/XSBench-omp/io.cpp | sed -n '100,200p'" in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 15ms:
   100			if( vhash == large )
   101				is_invalid_result = 0;
   102		}
   103		else if( strcmp(in.HM, "small") == 0 )
   104		{
   105			if( vhash == small )
   106				is_invalid_result = 0;
   107		}
   108	
   109		if(mype == 0 )
   110		{
   111			if( is_invalid_result )
   112				printf("Verification checksum: %llu (WARNING - INAVALID CHECKSUM!)\n", vhash);
   113			else
   114				printf("Verification checksum: %llu (Valid)\n", vhash);
   115			border_print();
   116		}
   117	
   118		return is_invalid_result;
   119	}

thinking
**Summarizing output and data plan changes**
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

exec
/bin/bash -lc 'cat data/src/XSBench-omp/gpu_output.txt' in /root/codex_baseline/cuda_omp_pareval_workdir succeeded in 8ms:
./XSBench -m event -s small -l 100000
================================================================================
                   __   __ ___________                 _                        
                   \ \ / //  ___| ___ \               | |                       
                    \ V / \ `--.| |_/ / ___ _ __   ___| |__                     
                    /   \  `--. \ ___ \/ _ \ '_ \ / __| '_ \                    
                   / /^\ \/\__/ / |_/ /  __/ | | | (__| | | |                   
                   \/   \/\____/\____/ \___|_| |_|\___|_| |_|                   

================================================================================
                    Developed at Argonne National Laboratory
                                   Version: 20
================================================================================
                                  INPUT SUMMARY
================================================================================
Programming Model:            OpenMP Offload
Target device ID:             0
Simulation Method:            Event Based
Grid Type:                    Unionized Grid
Materials:                    12

thinking
**Preparing detailed final report**
codex
Converted the XSBench CUDA kernels into OpenMP offload versions, documented the new data management plan, and kept the requested `Simulation.cpp.bak` snapshot so you can compare before/after behaviour.

- Added `data/src/XSBench-omp/data_plan.md` detailing the CUDA → OpenMP Strategy C mapping, array transfers, kernel inventory, and key migration issues before touching the code.
- Reworked device setup and teardown inside `data/src/XSBench-omp/GridInit.cpp:1-84` to use `omp_target_alloc`/`omp_target_memcpy` with helper templates instead of `cudaMalloc/cudaMemcpy`.
- Simplified `data/src/XSBench-omp/XSbench_header.cuh:73-110` to expose plain C prototypes (no CUDA/Thrust includes) so the new kernel helpers can compile cleanly.
- Rebuilt `data/src/XSBench-omp/Simulation.cpp:10-870` with OpenMP helpers that allocate sample buffers, copy/resync arrays, execute `#pragma omp target teams loop` kernels, and perform the host-side sorts/counts; the baseline and optimizations now move `SimulationData` to the device, invoke the converted kernels, and copy the verification data back via `copy_from_device`.
- Updated `data/src/XSBench-omp/io.cpp:121-179` (and added `<omp.h>`) so the CLI prints “Programming Model: OpenMP Offload” and the default target ID instead of CUDA device info.
- Captured the run log in `data/src/XSBench-omp/gpu_output.txt` (baseline comparison diffed below), and left `Simulation.cpp.bak` as the requested backup.

- **Tests:** `make -f Makefile.nvc` and `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > gpu_output.txt 2>&1` (both succeed; run logs in `data/src/XSBench-omp/gpu_output.txt`).
- **Diff vs baseline:** `diff baseline_output.txt gpu_output.txt` shows the new output says “Programming Model: OpenMP Offload” with `Runtime=0.541s`, `Lookups/s=184,687`, and profile line `281.243086,130.403042,0.617981,1,1`, whereas the CUDA baseline recorded the CUDA build log, a faster runtime (0.232s), and different lookups/s (431,119). These differences are expected because the new harness uses OpenMP target offload semantics.

Natural next steps:
1. Re-run the other optimization kernels (1–6) to ensure their verification hashes still match and to capture their runtimes.
2. Measure whether scheduling clauses (e.g., `schedule(dynamic,64)`) or chunking can recover some of the lost throughput shown in `gpu_output.txt`.
file update:
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
index 8ea865ef7c6d3759c7af4f4a91ff84cc8a55d150..dfac6c2f4e70c143544fca3c44f5a9aed2e42fdc
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/GridInit.cpp
@@ -1,94 +1,86 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Moves all required data structures to the GPU's memory space
+namespace {
+int get_target_device()
+{
+        return omp_get_default_device();
+}
+
+int get_host_device()
+{
+        return omp_get_initial_device();
+}
+
+template<typename T>
+static void copy_to_target(T *&dst, T *src, size_t count, int device, int host_device, size_t &total_sz)
+{
+        if (count == 0 || src == nullptr) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        omp_target_memcpy(dst, src, bytes, 0, 0, device, host_device);
+        total_sz += bytes;
+}
+
+template<typename T>
+static void allocate_on_target(T *&dst, size_t count, int device, size_t &total_sz)
+{
+        if (count == 0) {
+                dst = nullptr;
+                return;
+        }
+        size_t bytes = count * sizeof(T);
+        dst = static_cast<T *>(omp_target_alloc(bytes, device));
+        assert(dst != nullptr);
+        total_sz += bytes;
+}
+} // anonymous namespace
+
 SimulationData move_simulation_data_to_device( Inputs in, int mype, SimulationData SD )
 {
         if(mype == 0) printf("Allocating and moving simulation data to GPU memory space...\n");
 
-        ////////////////////////////////////////////////////////////////////////////////
-        // SUMMARY: Simulation Data Structure Manifest for "SD" Object
-        // Here we list all heap arrays (and lengths) in SD that would need to be
-        // offloaded manually if using an accelerator with a seperate memory space
-        ////////////////////////////////////////////////////////////////////////////////
-        // int * num_nucs;                     // Length = length_num_nucs;
-        // double * concs;                     // Length = length_concs
-        // int * mats;                         // Length = length_mats
-        // double * unionized_energy_array;    // Length = length_unionized_energy_array
-        // int * index_grid;                   // Length = length_index_grid
-        // NuclideGridPoint * nuclide_grid;    // Length = length_nuclide_grid
-        //
-        // Note: "unionized_energy_array" and "index_grid" can be of zero length
-        //        depending on lookup method.
-        //
-        // Note: "Lengths" are given as the number of objects in the array, not the
-        //       number of bytes.
-        ////////////////////////////////////////////////////////////////////////////////
-        size_t sz;
         size_t total_sz = 0;
-
-        // Shallow copy of CPU simulation data to GPU simulation data
         SimulationData GSD = SD;
+        int device = get_target_device();
+        int host_device = get_host_device();
 
-        // Move data to GPU memory space
-        sz = GSD.length_num_nucs * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.num_nucs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.num_nucs, SD.num_nucs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        copy_to_target(GSD.num_nucs, SD.num_nucs, GSD.length_num_nucs, device, host_device, total_sz);
+        copy_to_target(GSD.concs, SD.concs, GSD.length_concs, device, host_device, total_sz);
+        copy_to_target(GSD.mats, SD.mats, GSD.length_mats, device, host_device, total_sz);
 
-        sz = GSD.length_concs * sizeof(double);
-        gpuErrchk( cudaMalloc((void **) &GSD.concs, sz) );
-        gpuErrchk( cudaMemcpy(GSD.concs, SD.concs, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_unionized_energy_array != 0)
+                copy_to_target(GSD.unionized_energy_array, SD.unionized_energy_array, GSD.length_unionized_energy_array, device, host_device, total_sz);
 
-        sz = GSD.length_mats * sizeof(int);
-        gpuErrchk( cudaMalloc((void **) &GSD.mats, sz) );
-        gpuErrchk( cudaMemcpy(GSD.mats, SD.mats, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
+        if (SD.length_index_grid != 0)
+                copy_to_target(GSD.index_grid, SD.index_grid, GSD.length_index_grid, device, host_device, total_sz);
 
-        if (SD.length_unionized_energy_array != 0) {
-                sz = GSD.length_unionized_energy_array * sizeof(double);
-                gpuErrchk( cudaMalloc((void **) &GSD.unionized_energy_array, sz) );
-                gpuErrchk( cudaMemcpy(GSD.unionized_energy_array, SD.unionized_energy_array, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
+        copy_to_target(GSD.nuclide_grid, SD.nuclide_grid, GSD.length_nuclide_grid, device, host_device, total_sz);
 
-        if (SD.length_index_grid != 0) {
-                sz = GSD.length_index_grid * sizeof(int);
-                gpuErrchk( cudaMalloc((void **) &GSD.index_grid, sz) );
-                gpuErrchk( cudaMemcpy(GSD.index_grid, SD.index_grid, sz, cudaMemcpyHostToDevice) );
-                total_sz += sz;
-        }
-
-        sz = GSD.length_nuclide_grid * sizeof(NuclideGridPoint);
-        gpuErrchk( cudaMalloc((void **) &GSD.nuclide_grid, sz) );
-        gpuErrchk( cudaMemcpy(GSD.nuclide_grid, SD.nuclide_grid, sz, cudaMemcpyHostToDevice) );
-        total_sz += sz;
-
-        // Allocate verification array on device. This structure is not needed on CPU, so we don't
-        // have to copy anything over.
-        sz = in.lookups * sizeof(unsigned long);
-        gpuErrchk( cudaMalloc((void **) &GSD.verification, sz) );
-        total_sz += sz;
+        allocate_on_target(GSD.verification, in.lookups, device, total_sz);
         GSD.length_verification = in.lookups;
-
-        // Synchronize
-        gpuErrchk( cudaPeekAtLastError() );
-        gpuErrchk( cudaDeviceSynchronize() );
 
         if(mype == 0 ) printf("GPU Intialization complete. Allocated %.0lf MB of data on GPU.\n", total_sz/1024.0/1024.0 );
 
         return GSD;
-
 }
 
 // Release device memory
 void release_device_memory(SimulationData GSD) {
-        cudaFree(GSD.num_nucs);
-        cudaFree(GSD.concs);
-        cudaFree(GSD.mats);
-        if (GSD.length_unionized_energy_array > 0) cudaFree(GSD.unionized_energy_array);
-        cudaFree(GSD.nuclide_grid);
-        cudaFree(GSD.verification);
+        int device = get_target_device();
+        if (GSD.num_nucs) omp_target_free(GSD.num_nucs, device);
+        if (GSD.concs) omp_target_free(GSD.concs, device);
+        if (GSD.mats) omp_target_free(GSD.mats, device);
+        if (GSD.length_unionized_energy_array > 0 && GSD.unionized_energy_array)
+                omp_target_free(GSD.unionized_energy_array, device);
+        if (GSD.index_grid) omp_target_free(GSD.index_grid, device);
+        if (GSD.nuclide_grid) omp_target_free(GSD.nuclide_grid, device);
+        if (GSD.verification) omp_target_free(GSD.verification, device);
 }
 
 void release_memory(SimulationData SD) {
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
index 07b395fe81ab4856c7101337494babe164e07296..9a72c93613c1d98592d6e96c527d6e4116d9e596
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/Simulation.cpp
@@ -5,6 +5,7 @@
 #include <omp.h>
 #include <utility>
 #include <vector>
+#include <iterator>
 
 namespace {
 int target_device()
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
index ca346eb77f18795502acc01025e004860583e5bf..3567de6f7f37d91df95169ce50ac8aee72e1b38f
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/XSbench_header.cuh
@@ -5,9 +5,6 @@
 #include<stdlib.h>
 #include<math.h>
 #include<assert.h>
-#include<cuda.h>
-#include <thrust/reduce.h>
-#include <thrust/partition.h>
 #include<stdint.h>
 #include <chrono>
 #include "XSbench_shared_header.h"
@@ -29,16 +26,6 @@
 // Starting Seed
 #define STARTING_SEED 1070
 
-#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
-inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
-{
-        if (code != cudaSuccess)
-        {
-                fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
-                if (abort) exit(code);
-        }
-}
-
 // Structures
 typedef struct{
         double energy;
@@ -85,40 +72,40 @@
 
 // Simulation.cu
 unsigned long long run_event_based_simulation_baseline(Inputs in, SimulationData SD, int mype, Profile* profile);
-__global__ void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
-__device__ void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
+void xs_lookup_kernel_baseline(Inputs in, SimulationData GSD );
+void calculate_micro_xs(   double p_energy, int nuc, long n_isotopes,
                                    long n_gridpoints,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    long idx, double * __restrict__ xs_vector, int grid_type, int hash_bins );
-__device__ void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
+void calculate_macro_xs( double p_energy, int mat, long n_isotopes,
                                    long n_gridpoints, int * __restrict__ num_nucs,
                                    double * __restrict__ concs,
                                    double * __restrict__ egrid, int * __restrict__ index_data,
                                    NuclideGridPoint * __restrict__ nuclide_grids,
                                    int * __restrict__ mats,
                                    double * __restrict__ macro_xs_vector, int grid_type, int hash_bins, int max_num_nucs );
-__device__ long grid_search( long n, double quarry, double * __restrict__ A);
-__host__ __device__ long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
-__device__ int pick_mat( uint64_t * seed );
-__host__ __device__ double LCG_random_double(uint64_t * seed);
-__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
+long grid_search( long n, double quarry, double * __restrict__ A);
+long grid_search_nuclide( long n, double quarry, NuclideGridPoint * A, long low, long high);
+int pick_mat( uint64_t * seed );
+double LCG_random_double(uint64_t * seed);
+uint64_t fast_forward_LCG(uint64_t seed, uint64_t n);
 
 unsigned long long run_event_based_simulation_optimization_1(Inputs in, SimulationData GSD, int mype);
-__global__ void sampling_kernel(Inputs in, SimulationData GSD );
-__global__ void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
+void sampling_kernel(Inputs in, SimulationData GSD );
+void xs_lookup_kernel_optimization_1(Inputs in, SimulationData GSD );
 
 unsigned long long run_event_based_simulation_optimization_2(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_2(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_3(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
+void xs_lookup_kernel_optimization_3(Inputs in, SimulationData GSD, int m );
 
 unsigned long long run_event_based_simulation_optimization_4(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_4(Inputs in, SimulationData GSD, int m, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_5(Inputs in, SimulationData GSD, int mype);
-__global__ void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
+void xs_lookup_kernel_optimization_5(Inputs in, SimulationData GSD, int n_lookups, int offset );
 
 unsigned long long run_event_based_simulation_optimization_6(Inputs in, SimulationData GSD, int mype);
 
diff --git a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
index 6cf134abc984378266b340e0b3fd975a93755d22..687c645668b2ed268ec2252a110f4d18d6921d1b
--- a/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
+++ b/cuda_omp_pareval_workdir/data/src/XSBench-omp/io.cpp
@@ -1,4 +1,5 @@
 #include "XSbench_header.cuh"
+#include <omp.h>
 
 // Prints program logo
 void logo(int version)
@@ -124,12 +125,9 @@
 	logo(version);
 	center_print("INPUT SUMMARY", 79);
 	border_print();
-	printf("Programming Model:            CUDA\n");
-	cudaDeviceProp prop;
-	int device;
-	cudaGetDevice(&device);
-	cudaGetDeviceProperties ( &prop, device );
-		printf("CUDA Device:                  %s\n", prop.name);
+	printf("Programming Model:            OpenMP Offload\n");
+	int device = omp_get_default_device();
+	printf("Target device ID:             %d\n", device);
 	if( in.simulation_method == EVENT_BASED )
 		printf("Simulation Method:            Event Based\n");
 	else

tokens used
470,572