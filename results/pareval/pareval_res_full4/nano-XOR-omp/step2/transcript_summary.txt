**Update**
- Strengthened the hot `cellsXOR` loop (`nanoXOR.cpp:21`) with `__restrict__` pointers, cached row offsets, and boundary constants so the target loop does minimal address math while still using the original `map` strategy.
- Captured the profiling summary, checklist, and final migration narrative in `optimization_plan.md:19` and `optimization_plan.md:29`, documenting the before/after runtimes (0.25s â†’ 0.23s), the checklist status, and the micro-optimization gains.
- Tests: `make -f Makefile.nvc clean`; `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run` (the diff against `baseline_output.txt` still shows different compile commands/GATE sums because the CUDA baseline used `nanoXOR.cu` and nondeterministic input, but validation lines remain identical); `env OMP_TARGET_OFFLOAD=MANDATORY time ./nanoXOR.exe 1024 32` captured the 0.25s pre-change run and the 0.23s post-change run for comparison.

Next steps:
1. Investigate parallelizing or overlapping the host-side initialization/validation work that currently dominates the 0.23s runtime.
2. Explore persistent device buffers or explicit `omp_target_memcpy` usage if reducing the ~3.7ms transfer window becomes necessary for larger problem sizes.