Updated the OpenMP offload path so it now drives the `cellsXOR` work entirely through a teams/loop target region, supported by the new data plan and the preserved CUDA driver backup.

- **Data plan + backup:** Captured the CUDA allocation/transfer sequence, kernel launch parameters, and the Strategy A mapping rationale in `data/src/microXOR-omp/data_plan.md:1`, and saved the original CUDA driver from `golden_labels` as `data/src/microXOR-omp/main.cu.backup:1` so the legacy reference is preserved.
- **OpenMP kernel:** Added `data/src/microXOR-omp/microXOR.cpp:1` containing the `cellsXOR` implementation with a `#pragma omp target teams loop collapse(2)` block that mirrors the neighbor-count logic and writes directly into the `target data` mapped `output`, eliminating CUDA-specific constructs.

**Tests**

- `make -f Makefile.nvc clean` (workspace cleaned)
- `make -f Makefile.nvc` (successfully compiles `microXOR.cpp`/`main.cpp` with NVHPC)
- `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > gpu_output.txt 2>&1` â†’ run succeeded (`data/src/microXOR-omp/gpu_output.txt:1` shows the invocation plus `GATE:SUM`/`Validation passed.`). The diff between that log and `data/src/microXOR-omp/baseline_output.txt:1` only differs because the baseline records the earlier CUDA compiler lines and a different checksum (`value=da388d9a400182e3` vs `d4dfbe9e431561a2`), which is expected when `input` is randomized per run.