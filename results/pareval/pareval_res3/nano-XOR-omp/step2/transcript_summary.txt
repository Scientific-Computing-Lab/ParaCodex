- Added a fuller plan and outcome record in `data/src/nano-XOR-omp/optimization_plan.md:1`, capturing the starting metrics, chosen bottlenecks, implemented actions, and a final performance summary that explains why the runtime is still dominated by one-time offload setup.
- Restructured `cellsXOR` at `data/src/nano-XOR-omp/nanoXOR.cpp:22` so the kernel now uses `#pragma omp target teams distribute parallel for collapse(2)` with `__restrict__` pointers and one `idx = i * N + j`, which mirrors the CUDA grid, removes the division/mod math, and gives the compiler clearer aliasing.
- Validation continues to pass (new gate checksum reported per run), and the data strategy/target data mappings remain unchanged so the transfers still encapsulate the two arrays exactly as before.

- Tests:
  - `make -f Makefile.nvc clean`
  - `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > current_output.txt 2>&1`
  - `diff baseline_output.txt current_output.txt | grep -E "Verification|SUCCESSFUL|FAILED"` (returns exit code 1 because the diff only reflects compiler/runtime metadata and the random gate checksum; the program itself still validates)

1. Re-run the profiling workflow (e.g., regenerate `profile.log` with `nsys`) so the final summary can reference post-optimization kernel/transfer timings rather than the pre-change numbers.
2. Investigate caching or reusing the OpenMP target module (libomptarget tensor) to reduce the one-time `wait` cost that still dominates the ~0.7â€¯s runtime even after the kernel compute path matches CUDA.