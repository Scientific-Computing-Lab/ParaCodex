**OpenMP NanoXOR**
- Replaced the CUDA kernel with a host `cellsXOR` that now offloads the flattened `NÂ²` workset via `#pragma omp target teams loop`, and wrapped the launch in a `#pragma omp target data map(to: input[0:totalCells]) map(from: output[0:totalCells])` region so the `input`/`output` buffers stay resident on-device before/after the computation (`nanoXOR.cpp:7`, `nanoXOR.cpp:22`, `nanoXOR.cpp:72`).
- Captured the needed data-management reasoning (allocations, transfers, launch patterns, and Strategy A justification) in `data/src/nano-XOR-omp/data_plan.md:1` so the OpenMP version mirrors the CUDA workflow and network requirements.
- Simplified cleanup/validation to only touch host memory (`cleanup` now takes two arrays) and kept the existing validation structure while relying on the offloaded output copy-back before the checksum (`nanoXOR.cpp:36-105`).
- Tests: `make -f Makefile.nvc clean` and `make -f Makefile.nvc` (build succeeds with the OpenMP offload variant).

- Next steps: 1) Run `env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > gpu_output.txt 2>&1` and compare `gpu_output.txt` against `baseline_output.txt` to ensure bitwise agreement; 2) If validation passes, profile with `env OMP_TARGET_OFFLOAD=MANDATORY nsys profile --stats=true --trace=cuda,osrt --force-overwrite=true -o nsys_profile make -f Makefile.nvc run` as suggested in the workflow instructions.