Row-pointer caching plus `__restrict__` hints in `cellsXOR` now avoid repeated `i * N` arithmetic and expose contiguous rows to the compiler, leaving the collapse(2) teams loop untouched while keeping the offload path intact (`microXOR.cpp:5-21`).

Documented the runtime, GPU instrumentation gaps, hypothesis, and final takeaways in `optimization_plan.md`, including the action plan and recovery/micro-opt summaries that justify why only micro-tweaks were available (`optimization_plan.md:1-51`).

`diff baseline_output.txt current_output.txt` still shows only the compile/invocation differences (NVCC vs nvc++ and blockEdge 32 vs 16), but `Validation passed.` remains in the current run.

Tests:
- `make -f Makefile.nvc clean`
- `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > current_output.txt 2>&1`
- `env OMP_TARGET_OFFLOAD=MANDATORY /usr/bin/time -f 'real %e\nuser %U\nsys %S' ./microXOR.exe 1024 16`

Next steps:
1. Re-run the benchmark with a larger `N` (and/or repeat `nsys` profiling) so that GPU kernels and transfer metrics appear in `cuda_gpu_kern_sum`/`cuda_gpu_mem_time_sum`, enabling a more accurate “expected optimal” assessment.
2. Once richer profiling data exists, revisit the plan for possible collapse tuning or data-mapping refinements if the offload still underperforms against the CUDA baseline.