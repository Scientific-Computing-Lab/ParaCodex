Converted the CUDA driver in `main.cpp` to an OpenMP offload so the host still seeds the buffers, uses a single `target data` region, and runs the neighbor-count loop via `#pragma omp target teams loop collapse(2)` (see `main.cpp:13-59`), while `cleanup` now only releases host memory and `std::stoul` replaces the previous `cuda`-dependent argument parsing (`main.cpp:8-35`, `main.cpp:68-94`).

- **Optimizations documented:** `optimization_plan.md:1-18` records the measured runtime (≈0.02s with the provided arguments), the RTX 4060/cc8.9 hardware, the lack of `cuda_*` profiling entries in `profile.log`, and a two-action plan that keeps the collapsed teams loop plus the single `target data` mapping as the primary levers before adding any micro-optimizations.
- **Profiling note:** `profile.log` still only captures the compile + run steps because the grep for `cuda_gpu_kern_sum`, `cuda_api_sum`, and `cuda_gpu_mem_time_sum` returned nothing, so detailed kernel timings remain unavailable even after rebuilding with `OMP_TARGET_OFFLOAD=MANDATORY`.
- **Tests:** `make -f Makefile.nvc clean` followed by `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc CFLAGS="-std=c++17 -cuda -gpu=cc89 -O3" run` produced `Validation passed.`; `/usr/bin/time -f "%e" env OMP_TARGET_OFFLOAD=MANDATORY ./microXORh.exe 1024 32` measured ≈0.02s; `nvidia-smi --query-gpu=name,compute_cap --format=csv` confirmed the RTX 4060 compute capability 8.9 target.

Next steps you might take:
1. Capture an nsys/nsight profile of the OpenMP target region so you can compare actual GPU kernel time vs. the CUDA baseline and identify whether the collapse is saturating the device.
2. If the loop turns out to be the bottleneck, revisit micro-optimizations (e.g., annotate inputs as `const`/`restrict`, hoist stride calculations, or cache neighbor offsets) without altering the existing data-mapping strategy.