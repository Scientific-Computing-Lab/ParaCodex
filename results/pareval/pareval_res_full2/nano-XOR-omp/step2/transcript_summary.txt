- `nanoXOR.cpp:22` now passes `__restrict` pointers into the `cellsXOR` loop and caches each row/index so the target teams loop keeps its 2D collapse while reusing the same arithmetic for the four neighbor checks; validation still passes even though the profile log only reports the host `wait` region (no explicit kernel timings are exposed).
- `nanoXOR.cpp:65` hoists `totalCells` for allocation, mapping, and the checksum so the `target data` clause has a concrete span, the checksum reuses that size, and the unused-variable warning disappears without touching the original transfer strategy.
- Logged the runtime numbers, hypotheses, and final-performance summary (CUDA baseline 0.64s, OMP before 2.16s, OMP after 2.70s, and the recovery/micro-optimization checklist) inside `optimization_plan.md:1` and `optimization_plan.md:19` so the migration plan meets the required template.
- Tests: `/usr/bin/time -p env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run` (pre-change 2.16s, post-change 2.70s, including build time); `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > current_output.txt 2>&1` (validation output loggedâ€”diff vs `baseline_output.txt` only shows the compiler invocation and RNG-driven checksum difference); `nvcc -std=c++17 -arch=sm_89 -I../../../gate_sdk -O3 -x cu backups/nanoXOR.cpp.bak -o cuda_nanoXOR.exe -lcudart` and `/usr/bin/time -p ./cuda_nanoXOR.exe 1024 32` to capture the CUDA baseline (0.64s).

Next steps:
1. Capture a fresh `nsys` report or enable OpenMP-target tracing (e.g., via `libomptarget` callbacks) so the actual GPU kernel/mem times are visible instead of just the host `wait` region.
2. Once fine-grained profiling is available, reconsider the `target teams distribute parallel for collapse(2)` variant with tuned team/thread counts to better mirror the CUDA grid without regressing overall runtime.