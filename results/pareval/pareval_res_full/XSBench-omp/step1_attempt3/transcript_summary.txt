- Documented the current OpenMP-target workflow in `data/src/XSBench-omp/data_plan.md:1` so the plan now explains the `omp_target_alloc`/`omp_target_memcpy` lifecycle, the `#pragma omp target teams loop` kernels, and the host-side sort/partition helpers instead of the previous CUDA-centric wording.
- Built the benchmark via `make -f Makefile.nvc NVCC='nvc++ -cuda -std=c++14 -O3'` (override required because `nvc++` rejects `-arch=sm_89`), ran `OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc run`, and captured the mandated OpenMP-target log in `gpu_output.txt` (backup preserved at `data/src/XSBench-omp/Simulation.cpp.backup`).
- `diff baseline_output.txt gpu_output.txt` shows the expected narrative changes: the CUDA baseline (e.g., `data/src/XSBench-omp/baseline_output.txt:104`) logs “Programming Model: CUDA”, faster runtime/lookups (`0.147s`, 680k lookups/s) and a different summary line than the OpenMP run (`gpu_output.txt:16` reports “Programming Model: OpenMP target”, `0.432s`, 231k lookups/s, and a new verification tuple `159.013987,124.481916,0.542879,1,1`).
- **Testing:** `make -f Makefile.nvc clean`, `make -f Makefile.nvc NVCC='nvc++ -cuda -std=c++14 -O3'`, `OMP_TARGET_OFFLOAD=MANDATORY timeout 300 make -f Makefile.nvc run > gpu_output.txt 2>&1`.

Next steps (if desired):
1. Capture `nsys`/profiling output (with `OMP_TARGET_OFFLOAD=MANDATORY`) to confirm the OpenMP target kernels run on the GPU and to trace the observed throughput gap.
2. Double-check that the final verification scalar in `gpu_output.txt` matches the CUDA baseline; if there’s a mismatch, rerun with added logging around `std::accumulate`/verification to ensure data movement stayed coherent.