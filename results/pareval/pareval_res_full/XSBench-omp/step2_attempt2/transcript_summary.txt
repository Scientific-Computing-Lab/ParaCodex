Baseline performance now sits at 0.400 s (139 ms host→device, 129 ms kernel) and the main kernel’s inner loop now reuses cached pointers so Ada cc86 registers spend less time on repeated index math; the plan reflects the current metrics and ongoing transfer concern.

- `Simulation.cpp:75` now takes a per-nuclide `nuclide_grid` base pointer in `calculate_micro_xs` and always walks that base rather than recomputing `nuclide_grids[nuc * n_gridpoints]`, which reduces integer multiplies inside the declared-target helper.
- `Simulation.cpp:159` caches each material’s pointer rows and passes a `nuclide_grid_base` to `calculate_micro_xs`, so the hot `calculate_macro_xs` loop avoids repeated `mat*max_num_nucs`/`p_nuc*n_gridpoints` arithmetic while leaving the data strategy unchanged.
- `optimization_plan.md:3` now records the 0.400 s runtime plus 139 ms transfer trace, highlights the RTX 4060 (Ada/cc86) target, and lists the pointer-caching micro-optimization plus future transfer-overlap monitoring.

Testing:
```bash
cd data/src/XSBench-omp && timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc NVCC="nvc++ -cuda -std=c++14 -gpu=cc86 -O3" run
```

Next steps:
1. Keep observing the 139 ms host→device window (e.g., with more detailed profiling) and consider overlapping or pinning transfers if it stays above ~30% of runtime.
2. If additional kernel gains are needed, revisit grid-search nesting (collapse or precomputed lookups) now that the index math has been steadied.