Reworked `cellsXOR_device` so the OpenMP target now exposes a canonical 2D iteration space with `collapse(2)` and `thread_limit(256)` while the pointers are marked `__restrict__`, letting the compiler mirror the CUDA grid/block mapping without changing the data-transfer strategy (`nanoXOR.cpp:1-27`).

Documented the new metrics and confirmed the chosen bottleneck hypotheses in `optimization_plan.md:3-16`; the plan notes the runtime drop to 0.276s and the successful application of the collapse + micro-optimization steps.

- **Tests**: `make -f Makefile.nvc clean` + `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run` (validation passes again). `nsys profile --output /tmp/nanoXOR ./nanoXOR.exe 1024 32` followed by `nsys stats /tmp/nanoXOR.nsys-rep --report ...` series (cuda/osrt reports) regenerated `profile.log`.
- **Profile**: `profile.log:1-35` shows the OS runtime sum (~276ms dominated by poll/ioctl), `profile.log:44-70` reports the 45,184 ns `cellsXOR_device` kernel, and `profile.log:79-98` captures 83% D→H (1.8 ms) vs 17% H→D (0.37 ms) memory time.

Next steps for you to consider:
1. Run the same `nsys` workflow for other `N`/`blockEdge` combinations to ensure the collapse mapping scales and doesn’t expose regressions.
2. If you need tighter latency, try tuning team/thread limits or tiling heuristics in the `target teams distribute` loop and capture another profile to see whether host-side wait time drops further.