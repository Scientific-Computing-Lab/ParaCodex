Cached each row’s base offset and boundary flags plus marked the buffers as `__restrict__` so the `cellsXOR` target loop reuses precomputed indices instead of recomputing `i*N`/`j` on every neighbor read, which dropped the runtime from ~0.03 s to ~0.01 s (see `data/src/microXORh-omp/main.cpp:23`).  
Documented the baseline/after metrics, CUDA comparison, micro-opt plan, and final summary in `data/src/microXORh-omp/optimization_plan.md:1` and the final report section starting at `optimization_plan.md:29`, capturing the runtime improvements and the checklist recovery status.

Tests & validation:
- `make -f Makefile.nvc clean`
- `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > current_output.txt 2>&1` (current output still differs from `baseline_output.txt` only by the compiler command lines; `Validation passed.` matches).
- `env OMP_TARGET_OFFLOAD=MANDATORY /usr/bin/time -f "real %e" ./microXORh.exe 1024 32` (post-optimization runtime: 0.01s).
- `nvcc -std=c++17 -arch=sm_89 -O3 -x cu main.cpp.bak -o microXORh_cuda` + `./microXORh_cuda 1024 32` (CUDA baseline runtime: 0.29s for comparison).

Next steps:
1. Collect a full `nsys` report (larger `N`/`blockEdge`) to capture kernel and transfer times beyond the current short-run `wait` summary and validate where future tuning should focus.
2. Experiment with larger problem sizes and `blockEdge` heuristics to see if persistent data reuse, tiling, or streaming strategies uncover new bottlenecks once the kernel runtime grows.