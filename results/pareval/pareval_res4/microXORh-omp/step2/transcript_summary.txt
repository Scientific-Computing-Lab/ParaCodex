**Summary**
- Replaced the CUDA runtime plumbing with a single OpenMP target offload and kept the host validation untouched: `cellsXOR` now maps the full grids (via `target data`) and uses `#pragma omp target teams distribute parallel for collapse(2)` plus `__restrict__` pointers so the stencil mirrors the original CUDA work while eliminating `cudaMalloc`/`cudaMemcpy` (`main.cpp:11`); the host still builds the random grid, calls `cellsXOR`, and checks every cell exactly as before (`main.cpp:58`–`main.cpp:89`).
- Logged the measured 0.16 s runtime, the fact that only one `cellsXOR` offload exists, and the pending micro-optimization work in `optimization_plan.md:3`–`optimization_plan.md:17`, noting that `profile.log` only reports that the `cuda_gpu_*` stats were generated but no numeric breakdown, so we will need to dig into `nsys_profile.sqlite` for actual kernel/transfer times.

**Testing**
- `make -f Makefile.nvc clean`
- `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > current_output.txt 2>&1`
- `env OMP_TARGET_OFFLOAD=MANDATORY /usr/bin/time -f 'TIME_REAL=%e' ./microXORh.exe 1024 32` (`TIME_REAL=0.16`)
- `nvidia-smi --query-gpu=name,compute_cap --format=csv`

**Next Steps**
1. Run `nsys stats` (or open `nsys_profile.sqlite`) to extract `cuda_gpu_kern_sum`/`cuda_gpu_mem_*` numbers for the current binary so we can confirm how much of the 0.16 s is the kernel versus the H↔D copies.
2. If the kernel still dominates, revisit the OpenMP offload parameters (e.g., `thread_limit`, further alias hints, or explicit `simd`) to tune the remaining device-side compute before considering data-transfer refactors, as noted in `optimization_plan.md:15`.