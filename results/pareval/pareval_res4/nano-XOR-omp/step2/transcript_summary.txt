**Summary**
- Logged the runtime/NSYS metrics, bottleneck hypothesis, and planned micro-optimizations in `optimization_plan.md`, capturing the transfer-heavy 0.22 s run and the two actions I applied.
- Reworked `cellsXOR` (`nanoXOR.cpp:23`) to take `__restrict__` pointers, cache each thread’s linear index, and launch via `#pragma omp target teams distribute parallel for collapse(2) thread_limit(1024)` so the OpenMP team/thread hierarchy mirrors the original 32×32 CUDA block.
- Added post-optimization notes to `optimization_plan.md` so the still-single 32 µs kernel and the dominant host/transfer overhead are visible for future tuning.

**Testing**
- `make -f Makefile.nvc clean`
- `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run > current_output.txt 2>&1`
- `/usr/bin/time -p env OMP_TARGET_OFFLOAD=MANDATORY ./nanoXOR.exe 1024 32`

**Next Steps**
1. Explore parallelizing or pre-seeding the host RNG/validation loops so the ~0.22 s runtime isn’t dominated by serial host work.
2. If the kernel is ever executed repeatedly, consider persistent device storage (e.g., `omp_target_alloc` + `target enter data`/`is_device_ptr`) or pinned host buffers to cut the two 4 MB transfers per run.