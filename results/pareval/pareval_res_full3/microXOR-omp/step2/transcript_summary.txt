Cached the flattened index arithmetic inside the `cellsXOR` OpenMP target region, added `__restrict__`/`const` hints, and recorded the full analysis + outcome in the new plan document so the migration story (metrics, bottlenecks, checklist) is explicit.

- Reworked `cellsXOR` to store the row base, reuse `idx ± width`, and only do the minimal boundary tests inside the loop so each iteration avoids repeated `i*N`/`j` math; this gave the measured runtime drop from 0.02s to 0.01s while keeping the same single OpenMP target region (`data/src/microXOR-omp/microXOR.cpp:20-41`).
- Added `__restrict__`/`const size_t N` across the declaration and header to make aliasing and immutability explicit for the compiler, aligning with the micro-optimization strategy (`data/src/microXOR-omp/include/microXOR.cuh:8`).
- Documented current metrics, action plan, baseline comparison, final summary, and checklist in `optimization_plan.md` so the CUDA→OMP performance story is traceable (`data/src/microXOR-omp/optimization_plan.md:1-60`); the plan also notes that the profile log only reports OS `osrt_sum` and no GPU kernel stats.

Tests:
- `cd data/src/microXOR-omp && make -f Makefile.nvc clean` then `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run` (confirmed validation and new checksum).
- `/usr/bin/time -f "real %e" env OMP_TARGET_OFFLOAD=MANDATORY ./microXOR.exe 1024 32` (0.01s runtime after optimizations).
- `nvidia-smi --query-gpu=name,compute_cap --format=csv` (retrieved NVIDIA GeForce RTX 4060 Laptop GPU with compute capability 8.9).
- `nsys stats -r cuda_gpu_kern_sum golden_labels/src/microXOR-cuda/nsys_profile.nsys-rep` (validated the CUDA kernel cost ~39 µs for comparison).

Next steps:
1. Capture OpenMP offload activity directly (e.g., run `nsys`/`libomptarget` tracing again or enable OMPT hooks) to confirm whether the target loop actually reaches the GPU and to get accurate kernel timings.
2. If GPU execution is confirmed, revisit the host-side RNG/validation loops (N² work) for similar micro-optimizations or offloading/parallelization to keep them from becoming the bottleneck.