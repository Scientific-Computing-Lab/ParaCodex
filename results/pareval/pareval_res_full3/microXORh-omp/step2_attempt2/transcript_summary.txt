Cached row pointers and `__restrict__` qualifiers in `cellsXOR` so the single OpenMP offload now mirrors the CUDA stencil’s access pattern without touching the existing data mapping or validation logic.

- `main.cpp:30-57`: qualify `input`/`output` as non-aliasing and hoist row/neighbor pointer math out of the inner loop, leaving the inner loop with only neighbor dereferences and a single write (`rowStart + j`).
- `optimization_plan.md:3-20`: refreshed the metrics/logged runtime (0.01s), noted the missing kernel stats for the early-exit check, and documented the two micro-optimization actions that were applied.
- `profile.log:1-4`: regenerated the log with the updated build/run, which still only contains the compiler/run stdout and therefore no `cuda_gpu_kern_sum` data to confirm the 5% rule; the diff against `baseline_output.txt` via `diff … | grep -E "Verification|SUCCESSFUL|FAILED"` produced no matches, so outputs remain effectively identical.
- Tests: `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make -f Makefile.nvc run CFLAGS="-std=c++17 -cuda -gpu=cc89 -Iinclude -I/root/codex_baseline/cuda_omp_pareval_workdir/gate_sdk -O3"` (build & run), `env OMP_TARGET_OFFLOAD=MANDATORY /usr/bin/time -f "TIME:%e" ./microXORh.exe 1024 32` (0.01s), plus the verification diff command above.

Next steps:
1. Capture GPU timing via `nsys`/`nvprof` or another profiling tool so we can log `cuda_gpu_kern_sum`/`cuda_api_sum` and judge whether the current runtime is within 5% of the CUDA baseline.
2. If more headroom is needed after profiling, revisit the target loop bounds to ensure the collapse/team/limit tuning matches the CUDA grid-stride schedule and consider `simd` hints or register-level caching for neighbors.