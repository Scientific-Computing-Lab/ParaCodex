- **OpenMP data/loop offload** `cg.c:12-20` now includes `<omp.h>`, and the post-`makea` initialization/benchmark sequence is wrapped in `#pragma omp target data` so that `colidx`, `rowstr`, `a`, and the vector buffers stay on the GPU while `target teams loop` kernels (e.g., `cg.c:182-258`) reset `x/p/q/r`, compute norms, and scale `x` between iterations without repeated host-device transfers.  
- **conj_grad offload** All vector and sparse-matrix operations in `conj_grad` use `#pragma omp target teams loop` (with reductions for dot products) so that multi-iteration updates of `q`, `d`, `z`, `r`, and `p` execute on the device; the final residual and norm calculations are likewise offloaded (`cg.c:323-423`).  
- **Testing** Per instructions, ran `make clean` before building, then `make CC=nvc++ CLASS=B` and `./cg.B.x`; the benchmark completes (2.98 s, 18 328 Mops, verification successful) with the standard `deprecated_string_conv` warnings from the legacy print routines.  
- **Warnings** The only remaining compiler diagnostics are the pre-existing `char *` versus string-literal warnings emitted by `nvc++` (lines 117‑300), which match the original serial sources.

- **Next steps**  
  1. Run other classes (e.g., `CLASS=A` or `CLASS=C`) if you need smaller/larger problem sizes for further regression testing.  
  2. Profile the OpenMP kernels (nvprof/ncu or `nvc++` InfoQualifiers) to tune gang/worker counts and assess memory bandwidth use.