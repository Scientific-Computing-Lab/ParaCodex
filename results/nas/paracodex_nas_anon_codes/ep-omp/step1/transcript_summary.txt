Simplified the EP timed region by mapping all samples through a single `target data` region and handling RNG/hist locally on the device, eliminating the large `v60/v61` scratch buffers and chunk loops.

- **Work:** Documented the chosen Strategy A plan before coding in `data/src/ep-omp/data_plan.md:1`, listing the arrays, transfers, and histogram merge approach that guided the rewrite.  
- **ep.c changes:** Replaced the chunked RNG/hist logic with a single per-sample block inside `data/src/ep-omp/ep.c:96-235`, offloading via `#pragma omp target data`/`target teams loop`, generating Gaussian pairs on-the-fly, accumulating per-thread histograms, atomically merging into `v59`, and reducing `v27`/`v28`/`v32`. A copy of the original source was preserved at `data/src/ep-omp/ep.c.bak:1`.  
- **Cleanup:** Removed the dynamic scratch arrays and freed the host-to-device shuttle code, relying instead on local buffers and atomic bin updates (see the `target teams loop` block around `data/src/ep-omp/ep.c:123`).  
- **Outputs:** Kept both `baseline_output.txt:1` (pre-change verification) and the new `gpu_output.txt:1` (post-change run) for reference.

**Testing**
- `timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make CC=nvc++ CLASS=S run` (in `data/src/ep-omp`, output in `gpu_output.txt`) â€” `Verification    =               SUCCESSFUL` and the prior baseline file also reports success.

**Next Steps**
1. Profile the GPU implementation for higher classes (B/C) via `nsys`/profilers to ensure the new per-sample offload keeps performance scaling and to spot any remaining bottlenecks.  
2. If more speed is needed, consider refining the atomic histogram merge (e.g., shared-memory staging) and verifying no further reductions require chunking.