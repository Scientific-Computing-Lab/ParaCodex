# Data Management Plan

## Arrays Inventory
List ALL arrays used in timed region:

| Array Name | Size | Type | Init | Access |
|------------|------|------|------|--------|
| `v1` | `NZ = NA*(NONZER+1)*(NONZER+1)` | index | host (generated by `f2`) | R |
| `v2` | `NA+1` | index | host (set by `f2`) | R |
| `v7` | `NZ` | working (matrix values) | host (set by `f2`) | R |
| `v8` | `NA+2` | working (q vector) | host (initialized to ones) | R/W |
| `v9` | `NA+2` | working (r vector) | host (zeroed before CG) | R/W |
| `v10` | `NA+2` | working (p vector) | host | R/W |
| `v11` | `NA+2` | working (Ap vector) | host | R/W |
| `v12` | `NA+2` | working (z vector) | host | R/W |

**Types:** working (main CG vectors/matrix), index (CSR structure). All arrays are prepared on the host before the timed `f1` loop and then kept on the device for the entire benchmark.

## Functions in Timed Region
| Function | Arrays Accessed | Frequency | Must Run On |
|----------|------------------|-----------|-------------|
| `f1` | `v1, v2, v7, v8, v9, v10, v11, v12` | per iteration (NITER times) | device |

## Data Movement Strategy

**Chosen Strategy:** C (global device state + `omp_target_alloc`)

**Device Allocations (once):**
- `d_v1` (`int`, `NZ` elements) via `omp_target_alloc`
- `d_v2` (`int`, `NA+1` elements) via `omp_target_alloc`
- `d_v7` (`double`, `NZ` elements) via `omp_target_alloc`
- `d_v8`, `d_v9`, `d_v10`, `d_v11`, `d_v12` (`double`, `NA+2` each) via `omp_target_alloc`

**Host→Device Transfers:**
- When: after `f2` finishes initializing the CSR matrix and before the timed loop starts.
- Arrays: `v1`, `v2` (indices), `v7` (matrix values), `v8`-`v12` (vectors) → device copies.
- Total H→D: ~(NZ × (sizeof(int)+sizeof(double))) + (NA+1)×sizeof(int) + 5×(NA+2)×sizeof(double)
  (Class S example: ~0.5 MB for CSR, ~0.5 MB for vectors, so < 2 MB total).

**Device→Host Transfers:**
- When: none for large arrays; scalars (`v80`, `v81`, `v78`) produced by reductions inside device kernels are implicitly mapped back via OpenMP reductions.
- Arrays: none after initialization.
- Total D→H: scalars only (negligible).

**Transfers During Iterations:** NO – all CG vectors remain on device, only scalars need host visibility via reductions.

## Critical Checks (for chosen strategy)

**Strategy C:**
- [ ] ALL functions in iteration loop use `is_device_ptr` (device pointers for CSR/vector arrays).
- [ ] Scratch arrays (none beyond per-thread scalars) are allocated on device if needed.
- [ ] No `map()` clauses are introduced for the hot-path vectors/matrix (only device pointers).

**Common Mistakes:**
- Some loops still run on host (will be moved into `target teams loop`).
- Forgetting to update CRC scalars on device before each iteration.
- Using host pointers inside device kernels (will use device pointers only).

## Expected Transfer Volume
- Total: ~1 MB (H→D) for Class S initialization; D→H only scalars per iteration.
- **Red flag:** transferring full `v8`/`v9` every iteration – data will stay resident, dot products handled on device.

## Additional Parallelization Notes
- **RNG Replicable?** NO – no RNG inside timed region.
- **Outer Saturation?** NITER (15 for Class S) – each `f1` iteration offloaded to saturate GPU teams.
- **Sparse Matrix NONZER?** `NONZER` (class dependent; e.g., 7 for Class S).
- **Histogram Strategy?** N/A – no histogram updates in CG.

**Summary:** 8 arrays (0 scratch beyond scalars), 1 function (`f1`). Strategy C keeps ~1 MB of CSR/vectors on device via `omp_target_alloc`, reads them inside `f1` with `is_device_ptr`, and only moves scalars back to host via reductions.
