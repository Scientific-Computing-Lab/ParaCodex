Fused the repeated reductions and the final SpMV/residual pass so each `conj_grad` iteration issues fewer `target teams` launches, and logged the performance strategy + new profile data for future tuning.

- `cg.c:233-413`: Combined the `norm_temp1`/`norm_temp2` reductions into a single `target teams loop`, kept the subsequent `x` update, and now compute `r[j]` plus the residual sum in one kernel that reuses the freshly computed row data while caching the leftover difference (`diff`).
- `optimization_plan.md:1-48`: Captured the verified Class S runtime, the latest Class C `nsys` profile data, fusion ideas, bottleneck checklist, and strategy/micro-op targets so the next round of tuning has a clear checklist.
- `profile.log:254-288`: Re-profiled with `CLASS=C` and confirmed that the conj_grad kernel still dominates (94.3% total GPU time, ~1900 instances), `cuLaunchKernel` is ~9883, and the transfers cost ~47.9ms/461MB, so todayâ€™s fusions are the remaining lever for runtime wins.

**Testing**
- `make clean && timeout 300 env OMP_TARGET_OFFLOAD=MANDATORY make CC=nvc++ CLASS=S run > current_output.txt 2>&1` (`current_output.txt:73-109` shows `VERIFICATION SUCCESSFUL`, 0.11s runtime, 612 Mop/s while keeping the expected residual sequence)
- `make clean && env OMP_TARGET_OFFLOAD=MANDATORY nsys profile --stats=true --trace=cuda,osrt --force-overwrite=true -o nsys_profile make CC=nvc++ CLASS=C run > profile.log 2>&1` (see `profile.log:254-288` for the new kernel/memory breakdown; the run also produced `nsys_profile.nsys-rep` and `.sqlite` for deeper analysis)

**Next Steps**
1. Inspect `nsys_profile.nsys-rep`/`.sqlite` to verify whether the fused loops actually reduced the `cuLaunchKernel` count and to check occupancy across the remaining heavy kernels.
2. If more launch reduction is needed, fuse the initialization/zeroing loop (`cg.c:186-199`) and consider merging adjacent vector updates (e.g., the z/r update loop at `cg.c:376-394`) into wider kernels.